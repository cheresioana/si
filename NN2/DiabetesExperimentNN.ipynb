{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnVLqgJPKZYn"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, from_numpy, optim\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XoINPLfdmvGM"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"diabetes.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fGjK2mEym5gP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.294118 ,  0.487437 ,  0.180328 , ..., -0.53117  , -0.0333333,\n",
       "         0.       ],\n",
       "       [-0.882353 , -0.145729 ,  0.0819672, ..., -0.766866 , -0.666667 ,\n",
       "         1.       ],\n",
       "       [-0.0588235,  0.839196 ,  0.0491803, ..., -0.492741 , -0.633333 ,\n",
       "         0.       ],\n",
       "       ...,\n",
       "       [-0.411765 ,  0.21608  ,  0.180328 , ..., -0.857387 , -0.7      ,\n",
       "         1.       ],\n",
       "       [-0.882353 ,  0.266332 , -0.0163934, ..., -0.768574 , -0.133333 ,\n",
       "         0.       ],\n",
       "       [-0.882353 , -0.0653266,  0.147541 , ..., -0.797609 , -0.933333 ,\n",
       "         1.       ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentare cu diferiti parametrii\n",
    "\n",
    "In toate experimentele am tinut fixe toti parametrii in afara de cei testati. Configuratia de baza este\n",
    "\n",
    "Batch size 16\n",
    "\n",
    "Epochs 200\n",
    "\n",
    "lr 0.08\n",
    "\n",
    "Layers: self.sequential=nn.Sequential(\n",
    "            nn.Linear(8,12),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(12,8),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(8,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "## 1. Batch size\n",
    "\n",
    "a. 16 -> loss Mean 6.8248950988054276 -> mean loss/input 0.4275\n",
    "\n",
    "b. 32 -> loss Mean 13.75011432170868 -> mean loss/input  0.4281\n",
    "\n",
    "c. 128 -> loss Mean 58.93515205383301 -> mean loss/input 0.4603\n",
    "\n",
    "Concluzie: Cu cat batch size-ul este mai mare cu atat modelul de comporta mai rau si mai incet. Asta se intampla deoarece nu invata din fiecare exemplu si incearca sa invete din mai multe o data. Batch-size mare trebuie folosit cand sunt foarte multe date pentru a creste viteza de procesare. Cu toate actestea, cu cat batch-ul este mai mare cu atat modelul se comporta mai slab\n",
    "\n",
    "\n",
    "## 2. Numar de neuroni / layer\n",
    "\n",
    "Am folosit urmatoarea structura:\n",
    "\n",
    "self.sequential=nn.Sequential(\n",
    "            nn.Linear(8,120),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(120,180),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(180,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "Mean 7.409046034018199 -> mean loss/input 0.4625\n",
    "\n",
    "Concluzie: Rezultatul este mai rau din cauza ca functia este foarte complicata si nu reuseste sa generalizeze bine sau nu este potrivita pentru setul de date.\n",
    "\n",
    "## 3. Numar de layere\n",
    "\n",
    "Am folosit urmatoarea structura:\n",
    "\n",
    "self.sequential=nn.Sequential(\n",
    "            nn.Linear(8,12),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(12,8),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(8,12),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(12,8),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(8, 12),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(12,8),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(8,12),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(12,8),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(8,12),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(12,8),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(8,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "Mean 10.310399025678635 -> mean loss/input 0.6444\n",
    "\n",
    "Concluzie: Algoritmul nu converge sau converge foarte greu. Cu cat este mai lunga reteaua cu atat ii este mai greu sa modifice bine weight-urile deoarece fiecare weight are un impact mic, si atunci derivatele sunt mici si modificarea este mica. \n",
    "\n",
    "## 4. Learning rate\n",
    "\n",
    "a. learning rate 0.15\n",
    "\n",
    "Mean 6.653563092152278 -> mean loss/input 0.4158125\n",
    "\n",
    "b. learning rate 0.6\n",
    "\n",
    "Mean 9.322209392984709 -> mean loss/input 0.5825 + faptul ca algoritmul nu converge\n",
    "\n",
    "c. learning rate 0.08\n",
    "\n",
    "Mean 6.8248950988054276 -> mean loss/input 0.4275\n",
    "\n",
    "Concluzie: learning rate-ul face ca algoritmul sa convearga si cat de repede sa convearga. Chiar daca in urma rezultatealor de mai sus learning rate-ul potrivit pare a fii 0.15, in cazul in care se creste numarul de epochs, algoritmul nu mai converge de la un punct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QVMR_ijqKj-r"
   },
   "outputs": [],
   "source": [
    "#Dataset - o clasă din PyTorch foarte utilă gestionării seturilor de date\n",
    "class DiabetesDataset(Dataset):\n",
    "    \"\"\" Diabetes dataset.\"\"\"\n",
    "    # Initialize your data, download, etc.\n",
    "    def __init__(self):\n",
    "        #Citim setul de date\n",
    "        df=pd.read_csv(\"diabetes.csv\",header=None, dtype=np.float32)\n",
    "        xy = torch.from_numpy(df.values)\n",
    "        self.len = xy.shape[0]\n",
    "        #Vom folosi ca input toate valorile mai puțin ultima coloană\n",
    "        self.x_data = xy[:, 0:-1]\n",
    "        #Vom folosi ca output ultima coloană\n",
    "        self.y_data = xy[:, [-1]]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hVWrVEUMKkBU"
   },
   "outputs": [],
   "source": [
    "dataset = DiabetesDataset()\n",
    "#DataLoader - un utilitar ce ne ajută să împărțim setul de date pe batch-uri și astfel să facem antrenare în mod Mini-Batch\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=16,\n",
    "                          shuffle=True,\n",
    "                          num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "44aFZMd_KkC_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "759"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]\n",
    "dataset.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6k7bD66psdcT"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        torch.manual_seed(0)\n",
    "        #Practic toate functiile liniare și non-liniare realizează o secvență a flowlui de date\n",
    "        self.sequential=nn.Sequential(\n",
    "            nn.Linear(8,12),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(12,8),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(8,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLat(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ModelLat, self).__init__()\n",
    "        torch.manual_seed(0)\n",
    "        #Practic toate functiile liniare și non-liniare realizează o secvență a flowlui de date\n",
    "        self.sequential=nn.Sequential(\n",
    "            nn.Linear(8,120),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(120,180),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(180,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ModelLat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLung(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Dat fiind faptul că \n",
    "        \"\"\"\n",
    "        super(ModelLung, self).__init__()\n",
    "        torch.manual_seed(0)\n",
    "        #Practic toate functiile liniare și non-liniare realizează o secvență a flowlui de date\n",
    "        self.sequential=nn.Sequential(\n",
    "            nn.Linear(8,12),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(12,8),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(8,12),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(12,8),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(8, 12),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(12,8),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(8,12),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(12,8),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(8,12),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(12,8),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(8,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ModelLung()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cWeFa6ZzKkH3"
   },
   "outputs": [],
   "source": [
    "model=Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QHWknpAvKkKj"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss(reduction='mean')\n",
    "criterion2 = nn.BCELoss(reduction='sum')\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "THRIbzCKKkNI",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Batch: 1 | Loss: 10.9848\n",
      "Epoch 1 | Batch: 2 | Loss: 12.9700\n",
      "Epoch 1 | Batch: 3 | Loss: 33.0138\n",
      "Epoch 1 | Batch: 4 | Loss: 18.4174\n",
      "Epoch 1 | Batch: 5 | Loss: 20.5828\n",
      "Epoch 1 | Batch: 6 | Loss: 11.6737\n",
      "Epoch 1 | Batch: 7 | Loss: 10.0719\n",
      "Epoch 1 | Batch: 8 | Loss: 8.9911\n",
      "Epoch 1 | Batch: 9 | Loss: 10.0674\n",
      "Epoch 1 | Batch: 10 | Loss: 10.0695\n",
      "Epoch 1 | Batch: 11 | Loss: 7.8711\n",
      "Epoch 1 | Batch: 12 | Loss: 6.0832\n",
      "Epoch 1 | Batch: 13 | Loss: 15.9492\n",
      "Epoch 1 | Batch: 14 | Loss: 13.2772\n",
      "Epoch 1 | Batch: 15 | Loss: 7.7508\n",
      "Epoch 1 | Batch: 16 | Loss: 13.9231\n",
      "Epoch 1 | Batch: 17 | Loss: 15.9064\n",
      "Epoch 1 | Batch: 18 | Loss: 18.9490\n",
      "Epoch 1 | Batch: 19 | Loss: 11.8343\n",
      "Epoch 1 | Batch: 20 | Loss: 19.9313\n",
      "Epoch 1 | Batch: 21 | Loss: 11.1460\n",
      "Epoch 1 | Batch: 22 | Loss: 7.7663\n",
      "Epoch 1 | Batch: 23 | Loss: 18.8790\n",
      "Epoch 1 | Batch: 24 | Loss: 34.6083\n",
      "Epoch 1 | Batch: 25 | Loss: 8.0873\n",
      "Epoch 1 | Batch: 26 | Loss: 12.3708\n",
      "Epoch 1 | Batch: 27 | Loss: 10.9619\n",
      "Epoch 1 | Batch: 28 | Loss: 11.0670\n",
      "Epoch 1 | Batch: 29 | Loss: 11.3516\n",
      "Epoch 1 | Batch: 30 | Loss: 10.4171\n",
      "Epoch 1 | Batch: 31 | Loss: 12.5356\n",
      "Epoch 1 | Batch: 32 | Loss: 11.2562\n",
      "Epoch 1 | Batch: 33 | Loss: 12.3120\n",
      "Epoch 1 | Batch: 34 | Loss: 10.0360\n",
      "Epoch 1 | Batch: 35 | Loss: 9.4554\n",
      "Epoch 1 | Batch: 36 | Loss: 9.4165\n",
      "Epoch 1 | Batch: 37 | Loss: 9.1366\n",
      "Epoch 1 | Batch: 38 | Loss: 16.0398\n",
      "Epoch 1 | Batch: 39 | Loss: 29.2413\n",
      "Epoch 1 | Batch: 40 | Loss: 18.8551\n",
      "Epoch 1 | Batch: 41 | Loss: 10.8768\n",
      "Epoch 1 | Batch: 42 | Loss: 9.1253\n",
      "Epoch 1 | Batch: 43 | Loss: 13.2535\n",
      "Epoch 1 | Batch: 44 | Loss: 15.3051\n",
      "Epoch 1 | Batch: 45 | Loss: 12.5385\n",
      "Epoch 1 | Batch: 46 | Loss: 11.0160\n",
      "Epoch 1 | Batch: 47 | Loss: 11.2110\n",
      "Epoch 1 | Batch: 48 | Loss: 5.4837\n",
      "Mean 13.376408527294794\n",
      "Epoch 2 | Batch: 1 | Loss: 15.6282\n",
      "Epoch 2 | Batch: 2 | Loss: 20.4453\n",
      "Epoch 2 | Batch: 3 | Loss: 24.3196\n",
      "Epoch 2 | Batch: 4 | Loss: 13.5889\n",
      "Epoch 2 | Batch: 5 | Loss: 25.4193\n",
      "Epoch 2 | Batch: 6 | Loss: 15.5726\n",
      "Epoch 2 | Batch: 7 | Loss: 6.0259\n",
      "Epoch 2 | Batch: 8 | Loss: 4.0787\n",
      "Epoch 2 | Batch: 9 | Loss: 16.4678\n",
      "Epoch 2 | Batch: 10 | Loss: 12.0803\n",
      "Epoch 2 | Batch: 11 | Loss: 9.4785\n",
      "Epoch 2 | Batch: 12 | Loss: 9.1375\n",
      "Epoch 2 | Batch: 13 | Loss: 10.4921\n",
      "Epoch 2 | Batch: 14 | Loss: 10.3554\n",
      "Epoch 2 | Batch: 15 | Loss: 10.4710\n",
      "Epoch 2 | Batch: 16 | Loss: 10.6163\n",
      "Epoch 2 | Batch: 17 | Loss: 11.4581\n",
      "Epoch 2 | Batch: 18 | Loss: 10.9535\n",
      "Epoch 2 | Batch: 19 | Loss: 11.1118\n",
      "Epoch 2 | Batch: 20 | Loss: 12.7107\n",
      "Epoch 2 | Batch: 21 | Loss: 14.8472\n",
      "Epoch 2 | Batch: 22 | Loss: 29.4771\n",
      "Epoch 2 | Batch: 23 | Loss: 30.0556\n",
      "Epoch 2 | Batch: 24 | Loss: 23.9511\n",
      "Epoch 2 | Batch: 25 | Loss: 13.7377\n",
      "Epoch 2 | Batch: 26 | Loss: 19.4209\n",
      "Epoch 2 | Batch: 27 | Loss: 11.0110\n",
      "Epoch 2 | Batch: 28 | Loss: 10.3678\n",
      "Epoch 2 | Batch: 29 | Loss: 14.9452\n",
      "Epoch 2 | Batch: 30 | Loss: 10.2509\n",
      "Epoch 2 | Batch: 31 | Loss: 10.3674\n",
      "Epoch 2 | Batch: 32 | Loss: 10.6262\n",
      "Epoch 2 | Batch: 33 | Loss: 9.0707\n",
      "Epoch 2 | Batch: 34 | Loss: 10.5683\n",
      "Epoch 2 | Batch: 35 | Loss: 10.3690\n",
      "Epoch 2 | Batch: 36 | Loss: 10.4831\n",
      "Epoch 2 | Batch: 37 | Loss: 10.3317\n",
      "Epoch 2 | Batch: 38 | Loss: 13.2929\n",
      "Epoch 2 | Batch: 39 | Loss: 18.2624\n",
      "Epoch 2 | Batch: 40 | Loss: 33.6482\n",
      "Epoch 2 | Batch: 41 | Loss: 11.1999\n",
      "Epoch 2 | Batch: 42 | Loss: 11.7442\n",
      "Epoch 2 | Batch: 43 | Loss: 4.4630\n",
      "Epoch 2 | Batch: 44 | Loss: 16.7090\n",
      "Epoch 2 | Batch: 45 | Loss: 12.4016\n",
      "Epoch 2 | Batch: 46 | Loss: 17.7219\n",
      "Epoch 2 | Batch: 47 | Loss: 16.2321\n",
      "Epoch 2 | Batch: 48 | Loss: 15.6937\n",
      "Mean 14.201277196407318\n",
      "Epoch 3 | Batch: 1 | Loss: 10.1537\n",
      "Epoch 3 | Batch: 2 | Loss: 16.1473\n",
      "Epoch 3 | Batch: 3 | Loss: 15.2559\n",
      "Epoch 3 | Batch: 4 | Loss: 26.0365\n",
      "Epoch 3 | Batch: 5 | Loss: 23.6489\n",
      "Epoch 3 | Batch: 6 | Loss: 9.1528\n",
      "Epoch 3 | Batch: 7 | Loss: 9.0064\n",
      "Epoch 3 | Batch: 8 | Loss: 8.9116\n",
      "Epoch 3 | Batch: 9 | Loss: 14.2484\n",
      "Epoch 3 | Batch: 10 | Loss: 30.1017\n",
      "Epoch 3 | Batch: 11 | Loss: 33.1932\n",
      "Epoch 3 | Batch: 12 | Loss: 12.5426\n",
      "Epoch 3 | Batch: 13 | Loss: 7.7916\n",
      "Epoch 3 | Batch: 14 | Loss: 10.3196\n",
      "Epoch 3 | Batch: 15 | Loss: 10.5154\n",
      "Epoch 3 | Batch: 16 | Loss: 10.2621\n",
      "Epoch 3 | Batch: 17 | Loss: 10.4492\n",
      "Epoch 3 | Batch: 18 | Loss: 8.8636\n",
      "Epoch 3 | Batch: 19 | Loss: 11.0924\n",
      "Epoch 3 | Batch: 20 | Loss: 11.1284\n",
      "Epoch 3 | Batch: 21 | Loss: 7.7500\n",
      "Epoch 3 | Batch: 22 | Loss: 12.9839\n",
      "Epoch 3 | Batch: 23 | Loss: 11.1791\n",
      "Epoch 3 | Batch: 24 | Loss: 10.7129\n",
      "Epoch 3 | Batch: 25 | Loss: 10.6643\n",
      "Epoch 3 | Batch: 26 | Loss: 29.2304\n",
      "Epoch 3 | Batch: 27 | Loss: 16.7111\n",
      "Epoch 3 | Batch: 28 | Loss: 20.7785\n",
      "Epoch 3 | Batch: 29 | Loss: 10.3405\n",
      "Epoch 3 | Batch: 30 | Loss: 16.7251\n",
      "Epoch 3 | Batch: 31 | Loss: 9.5450\n",
      "Epoch 3 | Batch: 32 | Loss: 10.2030\n",
      "Epoch 3 | Batch: 33 | Loss: 10.4371\n",
      "Epoch 3 | Batch: 34 | Loss: 13.1582\n",
      "Epoch 3 | Batch: 35 | Loss: 15.8065\n",
      "Epoch 3 | Batch: 36 | Loss: 10.0202\n",
      "Epoch 3 | Batch: 37 | Loss: 14.3459\n",
      "Epoch 3 | Batch: 38 | Loss: 15.2455\n",
      "Epoch 3 | Batch: 39 | Loss: 8.2538\n",
      "Epoch 3 | Batch: 40 | Loss: 10.8721\n",
      "Epoch 3 | Batch: 41 | Loss: 10.6260\n",
      "Epoch 3 | Batch: 42 | Loss: 10.5542\n",
      "Epoch 3 | Batch: 43 | Loss: 9.3924\n",
      "Epoch 3 | Batch: 44 | Loss: 17.6614\n",
      "Epoch 3 | Batch: 45 | Loss: 19.1487\n",
      "Epoch 3 | Batch: 46 | Loss: 12.1521\n",
      "Epoch 3 | Batch: 47 | Loss: 8.0038\n",
      "Epoch 3 | Batch: 48 | Loss: 7.1051\n",
      "Mean 13.508916437625885\n",
      "Epoch 4 | Batch: 1 | Loss: 11.2589\n",
      "Epoch 4 | Batch: 2 | Loss: 12.2004\n",
      "Epoch 4 | Batch: 3 | Loss: 11.4025\n",
      "Epoch 4 | Batch: 4 | Loss: 12.2963\n",
      "Epoch 4 | Batch: 5 | Loss: 14.0287\n",
      "Epoch 4 | Batch: 6 | Loss: 35.2858\n",
      "Epoch 4 | Batch: 7 | Loss: 23.5766\n",
      "Epoch 4 | Batch: 8 | Loss: 21.4091\n",
      "Epoch 4 | Batch: 9 | Loss: 30.3105\n",
      "Epoch 4 | Batch: 10 | Loss: 26.3029\n",
      "Epoch 4 | Batch: 11 | Loss: 14.1657\n",
      "Epoch 4 | Batch: 12 | Loss: 8.2680\n",
      "Epoch 4 | Batch: 13 | Loss: 14.7342\n",
      "Epoch 4 | Batch: 14 | Loss: 12.7252\n",
      "Epoch 4 | Batch: 15 | Loss: 12.0212\n",
      "Epoch 4 | Batch: 16 | Loss: 12.5390\n",
      "Epoch 4 | Batch: 17 | Loss: 25.3455\n",
      "Epoch 4 | Batch: 18 | Loss: 12.3473\n",
      "Epoch 4 | Batch: 19 | Loss: 24.3321\n",
      "Epoch 4 | Batch: 20 | Loss: 8.6975\n",
      "Epoch 4 | Batch: 21 | Loss: 9.3447\n",
      "Epoch 4 | Batch: 22 | Loss: 10.5200\n",
      "Epoch 4 | Batch: 23 | Loss: 6.2377\n",
      "Epoch 4 | Batch: 24 | Loss: 6.0567\n",
      "Epoch 4 | Batch: 25 | Loss: 13.1114\n",
      "Epoch 4 | Batch: 26 | Loss: 9.2972\n",
      "Epoch 4 | Batch: 27 | Loss: 10.5152\n",
      "Epoch 4 | Batch: 28 | Loss: 17.6945\n",
      "Epoch 4 | Batch: 29 | Loss: 19.5106\n",
      "Epoch 4 | Batch: 30 | Loss: 11.9045\n",
      "Epoch 4 | Batch: 31 | Loss: 8.4709\n",
      "Epoch 4 | Batch: 32 | Loss: 22.3657\n",
      "Epoch 4 | Batch: 33 | Loss: 9.1698\n",
      "Epoch 4 | Batch: 34 | Loss: 8.6069\n",
      "Epoch 4 | Batch: 35 | Loss: 9.7601\n",
      "Epoch 4 | Batch: 36 | Loss: 9.2778\n",
      "Epoch 4 | Batch: 37 | Loss: 6.6811\n",
      "Epoch 4 | Batch: 38 | Loss: 12.1872\n",
      "Epoch 4 | Batch: 39 | Loss: 15.7444\n",
      "Epoch 4 | Batch: 40 | Loss: 6.8218\n",
      "Epoch 4 | Batch: 41 | Loss: 7.0613\n",
      "Epoch 4 | Batch: 42 | Loss: 8.0733\n",
      "Epoch 4 | Batch: 43 | Loss: 14.6176\n",
      "Epoch 4 | Batch: 44 | Loss: 29.9313\n",
      "Epoch 4 | Batch: 45 | Loss: 6.7519\n",
      "Epoch 4 | Batch: 46 | Loss: 7.0479\n",
      "Epoch 4 | Batch: 47 | Loss: 19.2651\n",
      "Epoch 4 | Batch: 48 | Loss: 3.9281\n",
      "Mean 13.816714410980543\n",
      "Epoch 5 | Batch: 1 | Loss: 17.4738\n",
      "Epoch 5 | Batch: 2 | Loss: 26.5901\n",
      "Epoch 5 | Batch: 3 | Loss: 7.5209\n",
      "Epoch 5 | Batch: 4 | Loss: 7.6414\n",
      "Epoch 5 | Batch: 5 | Loss: 5.9000\n",
      "Epoch 5 | Batch: 6 | Loss: 7.3596\n",
      "Epoch 5 | Batch: 7 | Loss: 8.8543\n",
      "Epoch 5 | Batch: 8 | Loss: 16.8719\n",
      "Epoch 5 | Batch: 9 | Loss: 6.6362\n",
      "Epoch 5 | Batch: 10 | Loss: 7.5073\n",
      "Epoch 5 | Batch: 11 | Loss: 7.2053\n",
      "Epoch 5 | Batch: 12 | Loss: 21.6602\n",
      "Epoch 5 | Batch: 13 | Loss: 13.3430\n",
      "Epoch 5 | Batch: 14 | Loss: 13.0624\n",
      "Epoch 5 | Batch: 15 | Loss: 7.8733\n",
      "Epoch 5 | Batch: 16 | Loss: 10.3963\n",
      "Epoch 5 | Batch: 17 | Loss: 19.3377\n",
      "Epoch 5 | Batch: 18 | Loss: 5.4709\n",
      "Epoch 5 | Batch: 19 | Loss: 14.5318\n",
      "Epoch 5 | Batch: 20 | Loss: 12.1644\n",
      "Epoch 5 | Batch: 21 | Loss: 12.2505\n",
      "Epoch 5 | Batch: 22 | Loss: 9.6169\n",
      "Epoch 5 | Batch: 23 | Loss: 24.2047\n",
      "Epoch 5 | Batch: 24 | Loss: 23.1245\n",
      "Epoch 5 | Batch: 25 | Loss: 10.7814\n",
      "Epoch 5 | Batch: 26 | Loss: 12.0169\n",
      "Epoch 5 | Batch: 27 | Loss: 12.4114\n",
      "Epoch 5 | Batch: 28 | Loss: 9.8694\n",
      "Epoch 5 | Batch: 29 | Loss: 14.0731\n",
      "Epoch 5 | Batch: 30 | Loss: 9.4632\n",
      "Epoch 5 | Batch: 31 | Loss: 14.4702\n",
      "Epoch 5 | Batch: 32 | Loss: 8.6411\n",
      "Epoch 5 | Batch: 33 | Loss: 8.3877\n",
      "Epoch 5 | Batch: 34 | Loss: 14.8292\n",
      "Epoch 5 | Batch: 35 | Loss: 13.5154\n",
      "Epoch 5 | Batch: 36 | Loss: 21.3898\n",
      "Epoch 5 | Batch: 37 | Loss: 31.3796\n",
      "Epoch 5 | Batch: 38 | Loss: 21.2487\n",
      "Epoch 5 | Batch: 39 | Loss: 11.3003\n",
      "Epoch 5 | Batch: 40 | Loss: 17.4380\n",
      "Epoch 5 | Batch: 41 | Loss: 12.2536\n",
      "Epoch 5 | Batch: 42 | Loss: 8.2457\n",
      "Epoch 5 | Batch: 43 | Loss: 10.3619\n",
      "Epoch 5 | Batch: 44 | Loss: 15.0222\n",
      "Epoch 5 | Batch: 45 | Loss: 25.8514\n",
      "Epoch 5 | Batch: 46 | Loss: 23.1705\n",
      "Epoch 5 | Batch: 47 | Loss: 6.5918\n",
      "Epoch 5 | Batch: 48 | Loss: 4.8679\n",
      "Mean 13.420374910036722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Batch: 1 | Loss: 9.5398\n",
      "Epoch 6 | Batch: 2 | Loss: 15.4542\n",
      "Epoch 6 | Batch: 3 | Loss: 16.9669\n",
      "Epoch 6 | Batch: 4 | Loss: 26.8135\n",
      "Epoch 6 | Batch: 5 | Loss: 11.3798\n",
      "Epoch 6 | Batch: 6 | Loss: 7.6738\n",
      "Epoch 6 | Batch: 7 | Loss: 12.8157\n",
      "Epoch 6 | Batch: 8 | Loss: 18.3528\n",
      "Epoch 6 | Batch: 9 | Loss: 17.7010\n",
      "Epoch 6 | Batch: 10 | Loss: 9.4209\n",
      "Epoch 6 | Batch: 11 | Loss: 10.2367\n",
      "Epoch 6 | Batch: 12 | Loss: 10.7108\n",
      "Epoch 6 | Batch: 13 | Loss: 7.7410\n",
      "Epoch 6 | Batch: 14 | Loss: 5.1658\n",
      "Epoch 6 | Batch: 15 | Loss: 6.3044\n",
      "Epoch 6 | Batch: 16 | Loss: 7.8694\n",
      "Epoch 6 | Batch: 17 | Loss: 9.5561\n",
      "Epoch 6 | Batch: 18 | Loss: 17.8523\n",
      "Epoch 6 | Batch: 19 | Loss: 12.2103\n",
      "Epoch 6 | Batch: 20 | Loss: 7.3212\n",
      "Epoch 6 | Batch: 21 | Loss: 4.8653\n",
      "Epoch 6 | Batch: 22 | Loss: 6.6032\n",
      "Epoch 6 | Batch: 23 | Loss: 8.9278\n",
      "Epoch 6 | Batch: 24 | Loss: 11.7168\n",
      "Epoch 6 | Batch: 25 | Loss: 4.5434\n",
      "Epoch 6 | Batch: 26 | Loss: 6.7761\n",
      "Epoch 6 | Batch: 27 | Loss: 11.4792\n",
      "Epoch 6 | Batch: 28 | Loss: 6.3922\n",
      "Epoch 6 | Batch: 29 | Loss: 8.1049\n",
      "Epoch 6 | Batch: 30 | Loss: 12.5754\n",
      "Epoch 6 | Batch: 31 | Loss: 9.2270\n",
      "Epoch 6 | Batch: 32 | Loss: 16.5554\n",
      "Epoch 6 | Batch: 33 | Loss: 8.8120\n",
      "Epoch 6 | Batch: 34 | Loss: 23.6057\n",
      "Epoch 6 | Batch: 35 | Loss: 33.8616\n",
      "Epoch 6 | Batch: 36 | Loss: 59.8911\n",
      "Epoch 6 | Batch: 37 | Loss: 9.9071\n",
      "Epoch 6 | Batch: 38 | Loss: 10.9093\n",
      "Epoch 6 | Batch: 39 | Loss: 11.2726\n",
      "Epoch 6 | Batch: 40 | Loss: 12.2796\n",
      "Epoch 6 | Batch: 41 | Loss: 11.4046\n",
      "Epoch 6 | Batch: 42 | Loss: 9.3839\n",
      "Epoch 6 | Batch: 43 | Loss: 10.1775\n",
      "Epoch 6 | Batch: 44 | Loss: 15.2977\n",
      "Epoch 6 | Batch: 45 | Loss: 14.1976\n",
      "Epoch 6 | Batch: 46 | Loss: 8.9572\n",
      "Epoch 6 | Batch: 47 | Loss: 7.0913\n",
      "Epoch 6 | Batch: 48 | Loss: 2.4217\n",
      "Mean 12.465072085460028\n",
      "Epoch 7 | Batch: 1 | Loss: 7.5600\n",
      "Epoch 7 | Batch: 2 | Loss: 7.8960\n",
      "Epoch 7 | Batch: 3 | Loss: 5.6173\n",
      "Epoch 7 | Batch: 4 | Loss: 13.5093\n",
      "Epoch 7 | Batch: 5 | Loss: 9.1263\n",
      "Epoch 7 | Batch: 6 | Loss: 7.1277\n",
      "Epoch 7 | Batch: 7 | Loss: 4.6196\n",
      "Epoch 7 | Batch: 8 | Loss: 10.1424\n",
      "Epoch 7 | Batch: 9 | Loss: 16.9443\n",
      "Epoch 7 | Batch: 10 | Loss: 26.7445\n",
      "Epoch 7 | Batch: 11 | Loss: 19.5326\n",
      "Epoch 7 | Batch: 12 | Loss: 15.3694\n",
      "Epoch 7 | Batch: 13 | Loss: 7.2528\n",
      "Epoch 7 | Batch: 14 | Loss: 10.6804\n",
      "Epoch 7 | Batch: 15 | Loss: 7.2799\n",
      "Epoch 7 | Batch: 16 | Loss: 10.7311\n",
      "Epoch 7 | Batch: 17 | Loss: 10.1591\n",
      "Epoch 7 | Batch: 18 | Loss: 10.5687\n",
      "Epoch 7 | Batch: 19 | Loss: 9.6636\n",
      "Epoch 7 | Batch: 20 | Loss: 7.8232\n",
      "Epoch 7 | Batch: 21 | Loss: 6.4674\n",
      "Epoch 7 | Batch: 22 | Loss: 9.2703\n",
      "Epoch 7 | Batch: 23 | Loss: 13.4445\n",
      "Epoch 7 | Batch: 24 | Loss: 8.5742\n",
      "Epoch 7 | Batch: 25 | Loss: 5.7852\n",
      "Epoch 7 | Batch: 26 | Loss: 10.1495\n",
      "Epoch 7 | Batch: 27 | Loss: 10.4863\n",
      "Epoch 7 | Batch: 28 | Loss: 9.4328\n",
      "Epoch 7 | Batch: 29 | Loss: 8.3117\n",
      "Epoch 7 | Batch: 30 | Loss: 6.9254\n",
      "Epoch 7 | Batch: 31 | Loss: 8.0004\n",
      "Epoch 7 | Batch: 32 | Loss: 10.7094\n",
      "Epoch 7 | Batch: 33 | Loss: 9.9338\n",
      "Epoch 7 | Batch: 34 | Loss: 10.4616\n",
      "Epoch 7 | Batch: 35 | Loss: 12.1963\n",
      "Epoch 7 | Batch: 36 | Loss: 16.7984\n",
      "Epoch 7 | Batch: 37 | Loss: 13.8598\n",
      "Epoch 7 | Batch: 38 | Loss: 21.2112\n",
      "Epoch 7 | Batch: 39 | Loss: 22.1302\n",
      "Epoch 7 | Batch: 40 | Loss: 22.0427\n",
      "Epoch 7 | Batch: 41 | Loss: 11.7403\n",
      "Epoch 7 | Batch: 42 | Loss: 19.2888\n",
      "Epoch 7 | Batch: 43 | Loss: 10.0754\n",
      "Epoch 7 | Batch: 44 | Loss: 9.9059\n",
      "Epoch 7 | Batch: 45 | Loss: 10.5298\n",
      "Epoch 7 | Batch: 46 | Loss: 9.4827\n",
      "Epoch 7 | Batch: 47 | Loss: 9.8378\n",
      "Epoch 7 | Batch: 48 | Loss: 4.4864\n",
      "Mean 11.24762632449468\n",
      "Epoch 8 | Batch: 1 | Loss: 5.7637\n",
      "Epoch 8 | Batch: 2 | Loss: 7.8028\n",
      "Epoch 8 | Batch: 3 | Loss: 7.2220\n",
      "Epoch 8 | Batch: 4 | Loss: 14.0337\n",
      "Epoch 8 | Batch: 5 | Loss: 7.1420\n",
      "Epoch 8 | Batch: 6 | Loss: 10.1700\n",
      "Epoch 8 | Batch: 7 | Loss: 7.8169\n",
      "Epoch 8 | Batch: 8 | Loss: 6.7945\n",
      "Epoch 8 | Batch: 9 | Loss: 8.8667\n",
      "Epoch 8 | Batch: 10 | Loss: 14.1580\n",
      "Epoch 8 | Batch: 11 | Loss: 14.6291\n",
      "Epoch 8 | Batch: 12 | Loss: 21.9197\n",
      "Epoch 8 | Batch: 13 | Loss: 17.6322\n",
      "Epoch 8 | Batch: 14 | Loss: 9.6539\n",
      "Epoch 8 | Batch: 15 | Loss: 11.2737\n",
      "Epoch 8 | Batch: 16 | Loss: 17.9216\n",
      "Epoch 8 | Batch: 17 | Loss: 11.1957\n",
      "Epoch 8 | Batch: 18 | Loss: 14.3006\n",
      "Epoch 8 | Batch: 19 | Loss: 9.5974\n",
      "Epoch 8 | Batch: 20 | Loss: 19.7094\n",
      "Epoch 8 | Batch: 21 | Loss: 22.6274\n",
      "Epoch 8 | Batch: 22 | Loss: 8.0334\n",
      "Epoch 8 | Batch: 23 | Loss: 3.4239\n",
      "Epoch 8 | Batch: 24 | Loss: 8.8024\n",
      "Epoch 8 | Batch: 25 | Loss: 10.4687\n",
      "Epoch 8 | Batch: 26 | Loss: 12.7467\n",
      "Epoch 8 | Batch: 27 | Loss: 18.6329\n",
      "Epoch 8 | Batch: 28 | Loss: 23.5922\n",
      "Epoch 8 | Batch: 29 | Loss: 7.2418\n",
      "Epoch 8 | Batch: 30 | Loss: 12.4448\n",
      "Epoch 8 | Batch: 31 | Loss: 12.9685\n",
      "Epoch 8 | Batch: 32 | Loss: 19.4570\n",
      "Epoch 8 | Batch: 33 | Loss: 19.8326\n",
      "Epoch 8 | Batch: 34 | Loss: 11.5076\n",
      "Epoch 8 | Batch: 35 | Loss: 13.3039\n",
      "Epoch 8 | Batch: 36 | Loss: 15.1369\n",
      "Epoch 8 | Batch: 37 | Loss: 6.2380\n",
      "Epoch 8 | Batch: 38 | Loss: 13.6693\n",
      "Epoch 8 | Batch: 39 | Loss: 12.9342\n",
      "Epoch 8 | Batch: 40 | Loss: 11.5643\n",
      "Epoch 8 | Batch: 41 | Loss: 14.5880\n",
      "Epoch 8 | Batch: 42 | Loss: 20.5144\n",
      "Epoch 8 | Batch: 43 | Loss: 10.5932\n",
      "Epoch 8 | Batch: 44 | Loss: 8.1078\n",
      "Epoch 8 | Batch: 45 | Loss: 8.7307\n",
      "Epoch 8 | Batch: 46 | Loss: 10.1776\n",
      "Epoch 8 | Batch: 47 | Loss: 9.6380\n",
      "Epoch 8 | Batch: 48 | Loss: 7.3481\n",
      "Mean 12.331834052999815\n",
      "Epoch 9 | Batch: 1 | Loss: 7.4963\n",
      "Epoch 9 | Batch: 2 | Loss: 6.0773\n",
      "Epoch 9 | Batch: 3 | Loss: 9.5976\n",
      "Epoch 9 | Batch: 4 | Loss: 4.0940\n",
      "Epoch 9 | Batch: 5 | Loss: 10.7970\n",
      "Epoch 9 | Batch: 6 | Loss: 6.4987\n",
      "Epoch 9 | Batch: 7 | Loss: 9.1539\n",
      "Epoch 9 | Batch: 8 | Loss: 13.8897\n",
      "Epoch 9 | Batch: 9 | Loss: 10.8901\n",
      "Epoch 9 | Batch: 10 | Loss: 7.5325\n",
      "Epoch 9 | Batch: 11 | Loss: 6.1340\n",
      "Epoch 9 | Batch: 12 | Loss: 8.1746\n",
      "Epoch 9 | Batch: 13 | Loss: 18.2968\n",
      "Epoch 9 | Batch: 14 | Loss: 15.0494\n",
      "Epoch 9 | Batch: 15 | Loss: 8.1070\n",
      "Epoch 9 | Batch: 16 | Loss: 11.4543\n",
      "Epoch 9 | Batch: 17 | Loss: 7.7029\n",
      "Epoch 9 | Batch: 18 | Loss: 6.5908\n",
      "Epoch 9 | Batch: 19 | Loss: 8.3094\n",
      "Epoch 9 | Batch: 20 | Loss: 10.5907\n",
      "Epoch 9 | Batch: 21 | Loss: 10.5220\n",
      "Epoch 9 | Batch: 22 | Loss: 18.2744\n",
      "Epoch 9 | Batch: 23 | Loss: 16.5271\n",
      "Epoch 9 | Batch: 24 | Loss: 22.7663\n",
      "Epoch 9 | Batch: 25 | Loss: 13.8573\n",
      "Epoch 9 | Batch: 26 | Loss: 8.9247\n",
      "Epoch 9 | Batch: 27 | Loss: 10.7033\n",
      "Epoch 9 | Batch: 28 | Loss: 8.1544\n",
      "Epoch 9 | Batch: 29 | Loss: 7.4180\n",
      "Epoch 9 | Batch: 30 | Loss: 5.3960\n",
      "Epoch 9 | Batch: 31 | Loss: 11.4428\n",
      "Epoch 9 | Batch: 32 | Loss: 7.8046\n",
      "Epoch 9 | Batch: 33 | Loss: 13.1264\n",
      "Epoch 9 | Batch: 34 | Loss: 8.8709\n",
      "Epoch 9 | Batch: 35 | Loss: 10.4497\n",
      "Epoch 9 | Batch: 36 | Loss: 15.3691\n",
      "Epoch 9 | Batch: 37 | Loss: 10.2392\n",
      "Epoch 9 | Batch: 38 | Loss: 13.0452\n",
      "Epoch 9 | Batch: 39 | Loss: 6.9027\n",
      "Epoch 9 | Batch: 40 | Loss: 6.8105\n",
      "Epoch 9 | Batch: 41 | Loss: 5.9005\n",
      "Epoch 9 | Batch: 42 | Loss: 11.5531\n",
      "Epoch 9 | Batch: 43 | Loss: 11.4371\n",
      "Epoch 9 | Batch: 44 | Loss: 26.1219\n",
      "Epoch 9 | Batch: 45 | Loss: 15.4403\n",
      "Epoch 9 | Batch: 46 | Loss: 9.2810\n",
      "Epoch 9 | Batch: 47 | Loss: 9.5567\n",
      "Epoch 9 | Batch: 48 | Loss: 6.1871\n",
      "Mean 10.594150404135386\n",
      "Epoch 10 | Batch: 1 | Loss: 6.4475\n",
      "Epoch 10 | Batch: 2 | Loss: 13.4493\n",
      "Epoch 10 | Batch: 3 | Loss: 10.6468\n",
      "Epoch 10 | Batch: 4 | Loss: 9.2510\n",
      "Epoch 10 | Batch: 5 | Loss: 7.9628\n",
      "Epoch 10 | Batch: 6 | Loss: 6.5838\n",
      "Epoch 10 | Batch: 7 | Loss: 11.0185\n",
      "Epoch 10 | Batch: 8 | Loss: 22.2068\n",
      "Epoch 10 | Batch: 9 | Loss: 27.4067\n",
      "Epoch 10 | Batch: 10 | Loss: 13.0275\n",
      "Epoch 10 | Batch: 11 | Loss: 19.0837\n",
      "Epoch 10 | Batch: 12 | Loss: 8.7169\n",
      "Epoch 10 | Batch: 13 | Loss: 11.1589\n",
      "Epoch 10 | Batch: 14 | Loss: 13.0805\n",
      "Epoch 10 | Batch: 15 | Loss: 7.8235\n",
      "Epoch 10 | Batch: 16 | Loss: 14.4864\n",
      "Epoch 10 | Batch: 17 | Loss: 11.9789\n",
      "Epoch 10 | Batch: 18 | Loss: 25.5288\n",
      "Epoch 10 | Batch: 19 | Loss: 26.2389\n",
      "Epoch 10 | Batch: 20 | Loss: 9.6593\n",
      "Epoch 10 | Batch: 21 | Loss: 10.8096\n",
      "Epoch 10 | Batch: 22 | Loss: 11.1622\n",
      "Epoch 10 | Batch: 23 | Loss: 12.0717\n",
      "Epoch 10 | Batch: 24 | Loss: 12.7780\n",
      "Epoch 10 | Batch: 25 | Loss: 11.8037\n",
      "Epoch 10 | Batch: 26 | Loss: 10.5102\n",
      "Epoch 10 | Batch: 27 | Loss: 4.4678\n",
      "Epoch 10 | Batch: 28 | Loss: 5.2020\n",
      "Epoch 10 | Batch: 29 | Loss: 13.7555\n",
      "Epoch 10 | Batch: 30 | Loss: 8.6048\n",
      "Epoch 10 | Batch: 31 | Loss: 10.5025\n",
      "Epoch 10 | Batch: 32 | Loss: 6.0802\n",
      "Epoch 10 | Batch: 33 | Loss: 8.0323\n",
      "Epoch 10 | Batch: 34 | Loss: 8.1629\n",
      "Epoch 10 | Batch: 35 | Loss: 16.7069\n",
      "Epoch 10 | Batch: 36 | Loss: 12.7247\n",
      "Epoch 10 | Batch: 37 | Loss: 7.5440\n",
      "Epoch 10 | Batch: 38 | Loss: 6.0558\n",
      "Epoch 10 | Batch: 39 | Loss: 7.3032\n",
      "Epoch 10 | Batch: 40 | Loss: 3.9749\n",
      "Epoch 10 | Batch: 41 | Loss: 4.5718\n",
      "Epoch 10 | Batch: 42 | Loss: 8.2693\n",
      "Epoch 10 | Batch: 43 | Loss: 5.4282\n",
      "Epoch 10 | Batch: 44 | Loss: 8.9506\n",
      "Epoch 10 | Batch: 45 | Loss: 10.8710\n",
      "Epoch 10 | Batch: 46 | Loss: 11.3374\n",
      "Epoch 10 | Batch: 47 | Loss: 9.5515\n",
      "Epoch 10 | Batch: 48 | Loss: 5.5882\n",
      "Mean 11.012027690807978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Batch: 1 | Loss: 6.6072\n",
      "Epoch 11 | Batch: 2 | Loss: 10.9744\n",
      "Epoch 11 | Batch: 3 | Loss: 11.9749\n",
      "Epoch 11 | Batch: 4 | Loss: 9.5216\n",
      "Epoch 11 | Batch: 5 | Loss: 11.6638\n",
      "Epoch 11 | Batch: 6 | Loss: 9.5180\n",
      "Epoch 11 | Batch: 7 | Loss: 8.5855\n",
      "Epoch 11 | Batch: 8 | Loss: 16.0211\n",
      "Epoch 11 | Batch: 9 | Loss: 9.8917\n",
      "Epoch 11 | Batch: 10 | Loss: 17.6865\n",
      "Epoch 11 | Batch: 11 | Loss: 13.4545\n",
      "Epoch 11 | Batch: 12 | Loss: 15.0469\n",
      "Epoch 11 | Batch: 13 | Loss: 15.9126\n",
      "Epoch 11 | Batch: 14 | Loss: 3.6043\n",
      "Epoch 11 | Batch: 15 | Loss: 12.2523\n",
      "Epoch 11 | Batch: 16 | Loss: 10.5818\n",
      "Epoch 11 | Batch: 17 | Loss: 5.4113\n",
      "Epoch 11 | Batch: 18 | Loss: 13.7526\n",
      "Epoch 11 | Batch: 19 | Loss: 8.2559\n",
      "Epoch 11 | Batch: 20 | Loss: 12.1638\n",
      "Epoch 11 | Batch: 21 | Loss: 17.4502\n",
      "Epoch 11 | Batch: 22 | Loss: 21.7202\n",
      "Epoch 11 | Batch: 23 | Loss: 10.0723\n",
      "Epoch 11 | Batch: 24 | Loss: 8.7377\n",
      "Epoch 11 | Batch: 25 | Loss: 8.2770\n",
      "Epoch 11 | Batch: 26 | Loss: 5.5737\n",
      "Epoch 11 | Batch: 27 | Loss: 11.8854\n",
      "Epoch 11 | Batch: 28 | Loss: 8.4540\n",
      "Epoch 11 | Batch: 29 | Loss: 7.7633\n",
      "Epoch 11 | Batch: 30 | Loss: 5.0651\n",
      "Epoch 11 | Batch: 31 | Loss: 10.3191\n",
      "Epoch 11 | Batch: 32 | Loss: 5.1148\n",
      "Epoch 11 | Batch: 33 | Loss: 4.6355\n",
      "Epoch 11 | Batch: 34 | Loss: 5.4910\n",
      "Epoch 11 | Batch: 35 | Loss: 7.3041\n",
      "Epoch 11 | Batch: 36 | Loss: 10.6827\n",
      "Epoch 11 | Batch: 37 | Loss: 10.6238\n",
      "Epoch 11 | Batch: 38 | Loss: 11.7739\n",
      "Epoch 11 | Batch: 39 | Loss: 10.1150\n",
      "Epoch 11 | Batch: 40 | Loss: 7.1159\n",
      "Epoch 11 | Batch: 41 | Loss: 8.4271\n",
      "Epoch 11 | Batch: 42 | Loss: 23.7251\n",
      "Epoch 11 | Batch: 43 | Loss: 9.3069\n",
      "Epoch 11 | Batch: 44 | Loss: 10.6433\n",
      "Epoch 11 | Batch: 45 | Loss: 15.7435\n",
      "Epoch 11 | Batch: 46 | Loss: 10.5288\n",
      "Epoch 11 | Batch: 47 | Loss: 8.1653\n",
      "Epoch 11 | Batch: 48 | Loss: 3.5375\n",
      "Mean 10.440267716844877\n",
      "Epoch 12 | Batch: 1 | Loss: 9.5655\n",
      "Epoch 12 | Batch: 2 | Loss: 18.7619\n",
      "Epoch 12 | Batch: 3 | Loss: 26.8954\n",
      "Epoch 12 | Batch: 4 | Loss: 13.2660\n",
      "Epoch 12 | Batch: 5 | Loss: 6.0825\n",
      "Epoch 12 | Batch: 6 | Loss: 6.3266\n",
      "Epoch 12 | Batch: 7 | Loss: 4.8037\n",
      "Epoch 12 | Batch: 8 | Loss: 7.6592\n",
      "Epoch 12 | Batch: 9 | Loss: 5.3426\n",
      "Epoch 12 | Batch: 10 | Loss: 4.6562\n",
      "Epoch 12 | Batch: 11 | Loss: 7.2677\n",
      "Epoch 12 | Batch: 12 | Loss: 8.3622\n",
      "Epoch 12 | Batch: 13 | Loss: 9.3465\n",
      "Epoch 12 | Batch: 14 | Loss: 15.0954\n",
      "Epoch 12 | Batch: 15 | Loss: 13.7119\n",
      "Epoch 12 | Batch: 16 | Loss: 15.5088\n",
      "Epoch 12 | Batch: 17 | Loss: 14.4270\n",
      "Epoch 12 | Batch: 18 | Loss: 10.1951\n",
      "Epoch 12 | Batch: 19 | Loss: 7.4737\n",
      "Epoch 12 | Batch: 20 | Loss: 13.0987\n",
      "Epoch 12 | Batch: 21 | Loss: 6.6342\n",
      "Epoch 12 | Batch: 22 | Loss: 5.9885\n",
      "Epoch 12 | Batch: 23 | Loss: 7.0201\n",
      "Epoch 12 | Batch: 24 | Loss: 9.8857\n",
      "Epoch 12 | Batch: 25 | Loss: 8.5768\n",
      "Epoch 12 | Batch: 26 | Loss: 4.7759\n",
      "Epoch 12 | Batch: 27 | Loss: 15.7712\n",
      "Epoch 12 | Batch: 28 | Loss: 9.3572\n",
      "Epoch 12 | Batch: 29 | Loss: 13.8179\n",
      "Epoch 12 | Batch: 30 | Loss: 14.5763\n",
      "Epoch 12 | Batch: 31 | Loss: 19.5443\n",
      "Epoch 12 | Batch: 32 | Loss: 4.2672\n",
      "Epoch 12 | Batch: 33 | Loss: 14.4730\n",
      "Epoch 12 | Batch: 34 | Loss: 8.4918\n",
      "Epoch 12 | Batch: 35 | Loss: 6.7860\n",
      "Epoch 12 | Batch: 36 | Loss: 10.6224\n",
      "Epoch 12 | Batch: 37 | Loss: 7.6953\n",
      "Epoch 12 | Batch: 38 | Loss: 8.1763\n",
      "Epoch 12 | Batch: 39 | Loss: 9.2149\n",
      "Epoch 12 | Batch: 40 | Loss: 11.2744\n",
      "Epoch 12 | Batch: 41 | Loss: 5.6385\n",
      "Epoch 12 | Batch: 42 | Loss: 7.3310\n",
      "Epoch 12 | Batch: 43 | Loss: 9.3425\n",
      "Epoch 12 | Batch: 44 | Loss: 6.3550\n",
      "Epoch 12 | Batch: 45 | Loss: 11.1601\n",
      "Epoch 12 | Batch: 46 | Loss: 24.7261\n",
      "Epoch 12 | Batch: 47 | Loss: 8.6739\n",
      "Epoch 12 | Batch: 48 | Loss: 5.4786\n",
      "Mean 10.281288733084997\n",
      "Epoch 13 | Batch: 1 | Loss: 5.6015\n",
      "Epoch 13 | Batch: 2 | Loss: 12.0633\n",
      "Epoch 13 | Batch: 3 | Loss: 11.7125\n",
      "Epoch 13 | Batch: 4 | Loss: 9.4510\n",
      "Epoch 13 | Batch: 5 | Loss: 8.9525\n",
      "Epoch 13 | Batch: 6 | Loss: 6.9311\n",
      "Epoch 13 | Batch: 7 | Loss: 19.9153\n",
      "Epoch 13 | Batch: 8 | Loss: 17.4708\n",
      "Epoch 13 | Batch: 9 | Loss: 9.2347\n",
      "Epoch 13 | Batch: 10 | Loss: 9.4255\n",
      "Epoch 13 | Batch: 11 | Loss: 6.9273\n",
      "Epoch 13 | Batch: 12 | Loss: 6.6638\n",
      "Epoch 13 | Batch: 13 | Loss: 16.4781\n",
      "Epoch 13 | Batch: 14 | Loss: 20.2706\n",
      "Epoch 13 | Batch: 15 | Loss: 9.1669\n",
      "Epoch 13 | Batch: 16 | Loss: 7.1488\n",
      "Epoch 13 | Batch: 17 | Loss: 8.1429\n",
      "Epoch 13 | Batch: 18 | Loss: 4.3384\n",
      "Epoch 13 | Batch: 19 | Loss: 15.2510\n",
      "Epoch 13 | Batch: 20 | Loss: 18.8792\n",
      "Epoch 13 | Batch: 21 | Loss: 8.6969\n",
      "Epoch 13 | Batch: 22 | Loss: 17.1520\n",
      "Epoch 13 | Batch: 23 | Loss: 16.2978\n",
      "Epoch 13 | Batch: 24 | Loss: 29.8279\n",
      "Epoch 13 | Batch: 25 | Loss: 23.1969\n",
      "Epoch 13 | Batch: 26 | Loss: 9.3342\n",
      "Epoch 13 | Batch: 27 | Loss: 8.8119\n",
      "Epoch 13 | Batch: 28 | Loss: 19.9465\n",
      "Epoch 13 | Batch: 29 | Loss: 18.0708\n",
      "Epoch 13 | Batch: 30 | Loss: 16.5676\n",
      "Epoch 13 | Batch: 31 | Loss: 11.5298\n",
      "Epoch 13 | Batch: 32 | Loss: 10.3901\n",
      "Epoch 13 | Batch: 33 | Loss: 9.4570\n",
      "Epoch 13 | Batch: 34 | Loss: 21.2083\n",
      "Epoch 13 | Batch: 35 | Loss: 26.8261\n",
      "Epoch 13 | Batch: 36 | Loss: 18.6672\n",
      "Epoch 13 | Batch: 37 | Loss: 8.1896\n",
      "Epoch 13 | Batch: 38 | Loss: 4.6655\n",
      "Epoch 13 | Batch: 39 | Loss: 7.6937\n",
      "Epoch 13 | Batch: 40 | Loss: 7.0832\n",
      "Epoch 13 | Batch: 41 | Loss: 4.0660\n",
      "Epoch 13 | Batch: 42 | Loss: 13.6952\n",
      "Epoch 13 | Batch: 43 | Loss: 4.5121\n",
      "Epoch 13 | Batch: 44 | Loss: 9.4531\n",
      "Epoch 13 | Batch: 45 | Loss: 5.7127\n",
      "Epoch 13 | Batch: 46 | Loss: 10.7845\n",
      "Epoch 13 | Batch: 47 | Loss: 9.8937\n",
      "Epoch 13 | Batch: 48 | Loss: 8.0646\n",
      "Mean 12.162915766239166\n",
      "Epoch 14 | Batch: 1 | Loss: 12.8571\n",
      "Epoch 14 | Batch: 2 | Loss: 15.1878\n",
      "Epoch 14 | Batch: 3 | Loss: 21.5053\n",
      "Epoch 14 | Batch: 4 | Loss: 15.8370\n",
      "Epoch 14 | Batch: 5 | Loss: 18.6686\n",
      "Epoch 14 | Batch: 6 | Loss: 15.3210\n",
      "Epoch 14 | Batch: 7 | Loss: 9.9662\n",
      "Epoch 14 | Batch: 8 | Loss: 6.8457\n",
      "Epoch 14 | Batch: 9 | Loss: 13.6019\n",
      "Epoch 14 | Batch: 10 | Loss: 15.3203\n",
      "Epoch 14 | Batch: 11 | Loss: 18.7880\n",
      "Epoch 14 | Batch: 12 | Loss: 13.7545\n",
      "Epoch 14 | Batch: 13 | Loss: 12.5901\n",
      "Epoch 14 | Batch: 14 | Loss: 6.8462\n",
      "Epoch 14 | Batch: 15 | Loss: 8.0226\n",
      "Epoch 14 | Batch: 16 | Loss: 4.7809\n",
      "Epoch 14 | Batch: 17 | Loss: 8.3455\n",
      "Epoch 14 | Batch: 18 | Loss: 7.0480\n",
      "Epoch 14 | Batch: 19 | Loss: 7.0097\n",
      "Epoch 14 | Batch: 20 | Loss: 8.5197\n",
      "Epoch 14 | Batch: 21 | Loss: 11.4980\n",
      "Epoch 14 | Batch: 22 | Loss: 17.5928\n",
      "Epoch 14 | Batch: 23 | Loss: 12.1572\n",
      "Epoch 14 | Batch: 24 | Loss: 9.0099\n",
      "Epoch 14 | Batch: 25 | Loss: 14.0956\n",
      "Epoch 14 | Batch: 26 | Loss: 16.6581\n",
      "Epoch 14 | Batch: 27 | Loss: 11.5647\n",
      "Epoch 14 | Batch: 28 | Loss: 11.4487\n",
      "Epoch 14 | Batch: 29 | Loss: 19.1770\n",
      "Epoch 14 | Batch: 30 | Loss: 8.2901\n",
      "Epoch 14 | Batch: 31 | Loss: 7.0486\n",
      "Epoch 14 | Batch: 32 | Loss: 12.5762\n",
      "Epoch 14 | Batch: 33 | Loss: 7.1192\n",
      "Epoch 14 | Batch: 34 | Loss: 5.2914\n",
      "Epoch 14 | Batch: 35 | Loss: 7.2338\n",
      "Epoch 14 | Batch: 36 | Loss: 4.7792\n",
      "Epoch 14 | Batch: 37 | Loss: 9.2035\n",
      "Epoch 14 | Batch: 38 | Loss: 5.8665\n",
      "Epoch 14 | Batch: 39 | Loss: 11.0573\n",
      "Epoch 14 | Batch: 40 | Loss: 10.3967\n",
      "Epoch 14 | Batch: 41 | Loss: 5.7799\n",
      "Epoch 14 | Batch: 42 | Loss: 9.5617\n",
      "Epoch 14 | Batch: 43 | Loss: 13.0441\n",
      "Epoch 14 | Batch: 44 | Loss: 13.7382\n",
      "Epoch 14 | Batch: 45 | Loss: 10.1957\n",
      "Epoch 14 | Batch: 46 | Loss: 12.4419\n",
      "Epoch 14 | Batch: 47 | Loss: 9.9410\n",
      "Epoch 14 | Batch: 48 | Loss: 4.5699\n",
      "Mean 11.08652439713478\n",
      "Epoch 15 | Batch: 1 | Loss: 7.3105\n",
      "Epoch 15 | Batch: 2 | Loss: 10.4777\n",
      "Epoch 15 | Batch: 3 | Loss: 9.3436\n",
      "Epoch 15 | Batch: 4 | Loss: 9.8726\n",
      "Epoch 15 | Batch: 5 | Loss: 11.5113\n",
      "Epoch 15 | Batch: 6 | Loss: 7.2138\n",
      "Epoch 15 | Batch: 7 | Loss: 8.7405\n",
      "Epoch 15 | Batch: 8 | Loss: 7.6458\n",
      "Epoch 15 | Batch: 9 | Loss: 10.8478\n",
      "Epoch 15 | Batch: 10 | Loss: 20.9133\n",
      "Epoch 15 | Batch: 11 | Loss: 16.2833\n",
      "Epoch 15 | Batch: 12 | Loss: 11.3812\n",
      "Epoch 15 | Batch: 13 | Loss: 4.1738\n",
      "Epoch 15 | Batch: 14 | Loss: 8.4272\n",
      "Epoch 15 | Batch: 15 | Loss: 4.4725\n",
      "Epoch 15 | Batch: 16 | Loss: 8.2496\n",
      "Epoch 15 | Batch: 17 | Loss: 6.7698\n",
      "Epoch 15 | Batch: 18 | Loss: 10.7311\n",
      "Epoch 15 | Batch: 19 | Loss: 15.8881\n",
      "Epoch 15 | Batch: 20 | Loss: 16.7641\n",
      "Epoch 15 | Batch: 21 | Loss: 10.4308\n",
      "Epoch 15 | Batch: 22 | Loss: 7.2509\n",
      "Epoch 15 | Batch: 23 | Loss: 6.1013\n",
      "Epoch 15 | Batch: 24 | Loss: 10.1310\n",
      "Epoch 15 | Batch: 25 | Loss: 15.4316\n",
      "Epoch 15 | Batch: 26 | Loss: 7.8714\n",
      "Epoch 15 | Batch: 27 | Loss: 11.1356\n",
      "Epoch 15 | Batch: 28 | Loss: 11.0983\n",
      "Epoch 15 | Batch: 29 | Loss: 12.9220\n",
      "Epoch 15 | Batch: 30 | Loss: 8.7871\n",
      "Epoch 15 | Batch: 31 | Loss: 6.0441\n",
      "Epoch 15 | Batch: 32 | Loss: 6.3971\n",
      "Epoch 15 | Batch: 33 | Loss: 10.4637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Batch: 34 | Loss: 5.0823\n",
      "Epoch 15 | Batch: 35 | Loss: 11.8118\n",
      "Epoch 15 | Batch: 36 | Loss: 6.7169\n",
      "Epoch 15 | Batch: 37 | Loss: 4.3385\n",
      "Epoch 15 | Batch: 38 | Loss: 12.0389\n",
      "Epoch 15 | Batch: 39 | Loss: 6.8147\n",
      "Epoch 15 | Batch: 40 | Loss: 10.1074\n",
      "Epoch 15 | Batch: 41 | Loss: 6.5383\n",
      "Epoch 15 | Batch: 42 | Loss: 11.5123\n",
      "Epoch 15 | Batch: 43 | Loss: 11.4545\n",
      "Epoch 15 | Batch: 44 | Loss: 22.7542\n",
      "Epoch 15 | Batch: 45 | Loss: 11.1149\n",
      "Epoch 15 | Batch: 46 | Loss: 5.8525\n",
      "Epoch 15 | Batch: 47 | Loss: 6.7053\n",
      "Epoch 15 | Batch: 48 | Loss: 3.2885\n",
      "Mean 9.733614186445871\n",
      "Epoch 16 | Batch: 1 | Loss: 11.4019\n",
      "Epoch 16 | Batch: 2 | Loss: 10.7662\n",
      "Epoch 16 | Batch: 3 | Loss: 13.9804\n",
      "Epoch 16 | Batch: 4 | Loss: 14.6747\n",
      "Epoch 16 | Batch: 5 | Loss: 19.0618\n",
      "Epoch 16 | Batch: 6 | Loss: 10.5511\n",
      "Epoch 16 | Batch: 7 | Loss: 10.6504\n",
      "Epoch 16 | Batch: 8 | Loss: 14.3417\n",
      "Epoch 16 | Batch: 9 | Loss: 17.5759\n",
      "Epoch 16 | Batch: 10 | Loss: 9.7348\n",
      "Epoch 16 | Batch: 11 | Loss: 9.9143\n",
      "Epoch 16 | Batch: 12 | Loss: 13.2399\n",
      "Epoch 16 | Batch: 13 | Loss: 14.7613\n",
      "Epoch 16 | Batch: 14 | Loss: 8.5576\n",
      "Epoch 16 | Batch: 15 | Loss: 7.8924\n",
      "Epoch 16 | Batch: 16 | Loss: 7.7963\n",
      "Epoch 16 | Batch: 17 | Loss: 13.5879\n",
      "Epoch 16 | Batch: 18 | Loss: 8.3803\n",
      "Epoch 16 | Batch: 19 | Loss: 7.4172\n",
      "Epoch 16 | Batch: 20 | Loss: 5.9291\n",
      "Epoch 16 | Batch: 21 | Loss: 8.1776\n",
      "Epoch 16 | Batch: 22 | Loss: 8.4583\n",
      "Epoch 16 | Batch: 23 | Loss: 7.1476\n",
      "Epoch 16 | Batch: 24 | Loss: 13.2698\n",
      "Epoch 16 | Batch: 25 | Loss: 11.3458\n",
      "Epoch 16 | Batch: 26 | Loss: 10.3501\n",
      "Epoch 16 | Batch: 27 | Loss: 9.4602\n",
      "Epoch 16 | Batch: 28 | Loss: 17.8606\n",
      "Epoch 16 | Batch: 29 | Loss: 13.6877\n",
      "Epoch 16 | Batch: 30 | Loss: 10.4221\n",
      "Epoch 16 | Batch: 31 | Loss: 9.1426\n",
      "Epoch 16 | Batch: 32 | Loss: 28.1668\n",
      "Epoch 16 | Batch: 33 | Loss: 27.6792\n",
      "Epoch 16 | Batch: 34 | Loss: 14.8704\n",
      "Epoch 16 | Batch: 35 | Loss: 6.5037\n",
      "Epoch 16 | Batch: 36 | Loss: 11.4650\n",
      "Epoch 16 | Batch: 37 | Loss: 11.7098\n",
      "Epoch 16 | Batch: 38 | Loss: 9.0532\n",
      "Epoch 16 | Batch: 39 | Loss: 6.5150\n",
      "Epoch 16 | Batch: 40 | Loss: 9.1090\n",
      "Epoch 16 | Batch: 41 | Loss: 10.6819\n",
      "Epoch 16 | Batch: 42 | Loss: 9.8821\n",
      "Epoch 16 | Batch: 43 | Loss: 11.1112\n",
      "Epoch 16 | Batch: 44 | Loss: 8.5808\n",
      "Epoch 16 | Batch: 45 | Loss: 11.5845\n",
      "Epoch 16 | Batch: 46 | Loss: 16.1558\n",
      "Epoch 16 | Batch: 47 | Loss: 8.3809\n",
      "Epoch 16 | Batch: 48 | Loss: 0.1370\n",
      "Mean 11.481757419804731\n",
      "Epoch 17 | Batch: 1 | Loss: 23.4901\n",
      "Epoch 17 | Batch: 2 | Loss: 10.0084\n",
      "Epoch 17 | Batch: 3 | Loss: 11.9541\n",
      "Epoch 17 | Batch: 4 | Loss: 8.3169\n",
      "Epoch 17 | Batch: 5 | Loss: 10.1424\n",
      "Epoch 17 | Batch: 6 | Loss: 7.0135\n",
      "Epoch 17 | Batch: 7 | Loss: 5.9150\n",
      "Epoch 17 | Batch: 8 | Loss: 7.8651\n",
      "Epoch 17 | Batch: 9 | Loss: 13.1235\n",
      "Epoch 17 | Batch: 10 | Loss: 22.2236\n",
      "Epoch 17 | Batch: 11 | Loss: 9.3139\n",
      "Epoch 17 | Batch: 12 | Loss: 5.9935\n",
      "Epoch 17 | Batch: 13 | Loss: 9.5503\n",
      "Epoch 17 | Batch: 14 | Loss: 10.6111\n",
      "Epoch 17 | Batch: 15 | Loss: 12.4333\n",
      "Epoch 17 | Batch: 16 | Loss: 12.1535\n",
      "Epoch 17 | Batch: 17 | Loss: 9.9651\n",
      "Epoch 17 | Batch: 18 | Loss: 11.3501\n",
      "Epoch 17 | Batch: 19 | Loss: 9.7137\n",
      "Epoch 17 | Batch: 20 | Loss: 6.8283\n",
      "Epoch 17 | Batch: 21 | Loss: 7.6366\n",
      "Epoch 17 | Batch: 22 | Loss: 7.3057\n",
      "Epoch 17 | Batch: 23 | Loss: 8.9833\n",
      "Epoch 17 | Batch: 24 | Loss: 6.5174\n",
      "Epoch 17 | Batch: 25 | Loss: 2.7535\n",
      "Epoch 17 | Batch: 26 | Loss: 5.5239\n",
      "Epoch 17 | Batch: 27 | Loss: 6.0711\n",
      "Epoch 17 | Batch: 28 | Loss: 7.7252\n",
      "Epoch 17 | Batch: 29 | Loss: 3.4826\n",
      "Epoch 17 | Batch: 30 | Loss: 9.6342\n",
      "Epoch 17 | Batch: 31 | Loss: 13.4097\n",
      "Epoch 17 | Batch: 32 | Loss: 8.7122\n",
      "Epoch 17 | Batch: 33 | Loss: 4.4231\n",
      "Epoch 17 | Batch: 34 | Loss: 9.8768\n",
      "Epoch 17 | Batch: 35 | Loss: 13.6342\n",
      "Epoch 17 | Batch: 36 | Loss: 17.3740\n",
      "Epoch 17 | Batch: 37 | Loss: 3.7694\n",
      "Epoch 17 | Batch: 38 | Loss: 9.8249\n",
      "Epoch 17 | Batch: 39 | Loss: 16.0288\n",
      "Epoch 17 | Batch: 40 | Loss: 10.1338\n",
      "Epoch 17 | Batch: 41 | Loss: 7.1169\n",
      "Epoch 17 | Batch: 42 | Loss: 10.2758\n",
      "Epoch 17 | Batch: 43 | Loss: 8.6622\n",
      "Epoch 17 | Batch: 44 | Loss: 10.5337\n",
      "Epoch 17 | Batch: 45 | Loss: 7.0570\n",
      "Epoch 17 | Batch: 46 | Loss: 15.0002\n",
      "Epoch 17 | Batch: 47 | Loss: 10.3457\n",
      "Epoch 17 | Batch: 48 | Loss: 6.6508\n",
      "Mean 9.71724966665109\n",
      "Epoch 18 | Batch: 1 | Loss: 8.1261\n",
      "Epoch 18 | Batch: 2 | Loss: 8.9152\n",
      "Epoch 18 | Batch: 3 | Loss: 11.4974\n",
      "Epoch 18 | Batch: 4 | Loss: 8.1357\n",
      "Epoch 18 | Batch: 5 | Loss: 6.9391\n",
      "Epoch 18 | Batch: 6 | Loss: 6.8822\n",
      "Epoch 18 | Batch: 7 | Loss: 5.3869\n",
      "Epoch 18 | Batch: 8 | Loss: 6.0010\n",
      "Epoch 18 | Batch: 9 | Loss: 7.4597\n",
      "Epoch 18 | Batch: 10 | Loss: 15.0874\n",
      "Epoch 18 | Batch: 11 | Loss: 17.6161\n",
      "Epoch 18 | Batch: 12 | Loss: 13.0780\n",
      "Epoch 18 | Batch: 13 | Loss: 7.0869\n",
      "Epoch 18 | Batch: 14 | Loss: 7.8205\n",
      "Epoch 18 | Batch: 15 | Loss: 8.0335\n",
      "Epoch 18 | Batch: 16 | Loss: 10.3755\n",
      "Epoch 18 | Batch: 17 | Loss: 9.2326\n",
      "Epoch 18 | Batch: 18 | Loss: 15.8413\n",
      "Epoch 18 | Batch: 19 | Loss: 6.2373\n",
      "Epoch 18 | Batch: 20 | Loss: 8.2305\n",
      "Epoch 18 | Batch: 21 | Loss: 7.5012\n",
      "Epoch 18 | Batch: 22 | Loss: 11.0130\n",
      "Epoch 18 | Batch: 23 | Loss: 10.1631\n",
      "Epoch 18 | Batch: 24 | Loss: 13.6312\n",
      "Epoch 18 | Batch: 25 | Loss: 7.6245\n",
      "Epoch 18 | Batch: 26 | Loss: 9.0645\n",
      "Epoch 18 | Batch: 27 | Loss: 8.7296\n",
      "Epoch 18 | Batch: 28 | Loss: 9.0718\n",
      "Epoch 18 | Batch: 29 | Loss: 8.0680\n",
      "Epoch 18 | Batch: 30 | Loss: 7.4474\n",
      "Epoch 18 | Batch: 31 | Loss: 10.7658\n",
      "Epoch 18 | Batch: 32 | Loss: 8.3234\n",
      "Epoch 18 | Batch: 33 | Loss: 16.4609\n",
      "Epoch 18 | Batch: 34 | Loss: 6.2589\n",
      "Epoch 18 | Batch: 35 | Loss: 9.6510\n",
      "Epoch 18 | Batch: 36 | Loss: 11.0092\n",
      "Epoch 18 | Batch: 37 | Loss: 26.7153\n",
      "Epoch 18 | Batch: 38 | Loss: 25.8115\n",
      "Epoch 18 | Batch: 39 | Loss: 17.7715\n",
      "Epoch 18 | Batch: 40 | Loss: 7.5640\n",
      "Epoch 18 | Batch: 41 | Loss: 9.9624\n",
      "Epoch 18 | Batch: 42 | Loss: 22.2138\n",
      "Epoch 18 | Batch: 43 | Loss: 17.0102\n",
      "Epoch 18 | Batch: 44 | Loss: 7.5525\n",
      "Epoch 18 | Batch: 45 | Loss: 6.4849\n",
      "Epoch 18 | Batch: 46 | Loss: 14.4077\n",
      "Epoch 18 | Batch: 47 | Loss: 15.8512\n",
      "Epoch 18 | Batch: 48 | Loss: 4.2736\n",
      "Mean 10.799687296152115\n",
      "Epoch 19 | Batch: 1 | Loss: 8.8277\n",
      "Epoch 19 | Batch: 2 | Loss: 8.9081\n",
      "Epoch 19 | Batch: 3 | Loss: 14.4622\n",
      "Epoch 19 | Batch: 4 | Loss: 6.7684\n",
      "Epoch 19 | Batch: 5 | Loss: 7.8215\n",
      "Epoch 19 | Batch: 6 | Loss: 9.9401\n",
      "Epoch 19 | Batch: 7 | Loss: 12.1293\n",
      "Epoch 19 | Batch: 8 | Loss: 15.1052\n",
      "Epoch 19 | Batch: 9 | Loss: 13.9750\n",
      "Epoch 19 | Batch: 10 | Loss: 6.4657\n",
      "Epoch 19 | Batch: 11 | Loss: 4.8526\n",
      "Epoch 19 | Batch: 12 | Loss: 14.5294\n",
      "Epoch 19 | Batch: 13 | Loss: 17.8772\n",
      "Epoch 19 | Batch: 14 | Loss: 13.7155\n",
      "Epoch 19 | Batch: 15 | Loss: 7.5514\n",
      "Epoch 19 | Batch: 16 | Loss: 8.9554\n",
      "Epoch 19 | Batch: 17 | Loss: 12.7462\n",
      "Epoch 19 | Batch: 18 | Loss: 12.4064\n",
      "Epoch 19 | Batch: 19 | Loss: 11.5472\n",
      "Epoch 19 | Batch: 20 | Loss: 8.9297\n",
      "Epoch 19 | Batch: 21 | Loss: 15.8359\n",
      "Epoch 19 | Batch: 22 | Loss: 29.0105\n",
      "Epoch 19 | Batch: 23 | Loss: 23.1287\n",
      "Epoch 19 | Batch: 24 | Loss: 7.5695\n",
      "Epoch 19 | Batch: 25 | Loss: 8.6273\n",
      "Epoch 19 | Batch: 26 | Loss: 4.6830\n",
      "Epoch 19 | Batch: 27 | Loss: 9.9662\n",
      "Epoch 19 | Batch: 28 | Loss: 6.1870\n",
      "Epoch 19 | Batch: 29 | Loss: 6.0197\n",
      "Epoch 19 | Batch: 30 | Loss: 12.8151\n",
      "Epoch 19 | Batch: 31 | Loss: 6.1839\n",
      "Epoch 19 | Batch: 32 | Loss: 7.4183\n",
      "Epoch 19 | Batch: 33 | Loss: 5.7201\n",
      "Epoch 19 | Batch: 34 | Loss: 8.0055\n",
      "Epoch 19 | Batch: 35 | Loss: 7.7056\n",
      "Epoch 19 | Batch: 36 | Loss: 6.9635\n",
      "Epoch 19 | Batch: 37 | Loss: 6.6156\n",
      "Epoch 19 | Batch: 38 | Loss: 11.1407\n",
      "Epoch 19 | Batch: 39 | Loss: 15.3239\n",
      "Epoch 19 | Batch: 40 | Loss: 23.5889\n",
      "Epoch 19 | Batch: 41 | Loss: 23.0473\n",
      "Epoch 19 | Batch: 42 | Loss: 8.0521\n",
      "Epoch 19 | Batch: 43 | Loss: 7.2789\n",
      "Epoch 19 | Batch: 44 | Loss: 8.5279\n",
      "Epoch 19 | Batch: 45 | Loss: 12.9619\n",
      "Epoch 19 | Batch: 46 | Loss: 13.5351\n",
      "Epoch 19 | Batch: 47 | Loss: 14.6253\n",
      "Epoch 19 | Batch: 48 | Loss: 7.0080\n",
      "Mean 11.147082279125849\n",
      "Epoch 20 | Batch: 1 | Loss: 13.5722\n",
      "Epoch 20 | Batch: 2 | Loss: 10.6390\n",
      "Epoch 20 | Batch: 3 | Loss: 7.5614\n",
      "Epoch 20 | Batch: 4 | Loss: 12.9327\n",
      "Epoch 20 | Batch: 5 | Loss: 16.1919\n",
      "Epoch 20 | Batch: 6 | Loss: 7.6132\n",
      "Epoch 20 | Batch: 7 | Loss: 12.5496\n",
      "Epoch 20 | Batch: 8 | Loss: 18.9886\n",
      "Epoch 20 | Batch: 9 | Loss: 8.2647\n",
      "Epoch 20 | Batch: 10 | Loss: 7.6655\n",
      "Epoch 20 | Batch: 11 | Loss: 11.7919\n",
      "Epoch 20 | Batch: 12 | Loss: 10.2851\n",
      "Epoch 20 | Batch: 13 | Loss: 14.4803\n",
      "Epoch 20 | Batch: 14 | Loss: 11.4228\n",
      "Epoch 20 | Batch: 15 | Loss: 3.5857\n",
      "Epoch 20 | Batch: 16 | Loss: 4.2790\n",
      "Epoch 20 | Batch: 17 | Loss: 10.8879\n",
      "Epoch 20 | Batch: 18 | Loss: 7.2777\n",
      "Epoch 20 | Batch: 19 | Loss: 6.7082\n",
      "Epoch 20 | Batch: 20 | Loss: 8.2159\n",
      "Epoch 20 | Batch: 21 | Loss: 7.4083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | Batch: 22 | Loss: 16.5751\n",
      "Epoch 20 | Batch: 23 | Loss: 31.0379\n",
      "Epoch 20 | Batch: 24 | Loss: 25.3739\n",
      "Epoch 20 | Batch: 25 | Loss: 7.8341\n",
      "Epoch 20 | Batch: 26 | Loss: 13.7043\n",
      "Epoch 20 | Batch: 27 | Loss: 18.6030\n",
      "Epoch 20 | Batch: 28 | Loss: 23.8912\n",
      "Epoch 20 | Batch: 29 | Loss: 11.3049\n",
      "Epoch 20 | Batch: 30 | Loss: 11.1135\n",
      "Epoch 20 | Batch: 31 | Loss: 6.2549\n",
      "Epoch 20 | Batch: 32 | Loss: 7.1239\n",
      "Epoch 20 | Batch: 33 | Loss: 5.5702\n",
      "Epoch 20 | Batch: 34 | Loss: 12.3162\n",
      "Epoch 20 | Batch: 35 | Loss: 6.4417\n",
      "Epoch 20 | Batch: 36 | Loss: 13.3647\n",
      "Epoch 20 | Batch: 37 | Loss: 8.0846\n",
      "Epoch 20 | Batch: 38 | Loss: 2.2670\n",
      "Epoch 20 | Batch: 39 | Loss: 13.5876\n",
      "Epoch 20 | Batch: 40 | Loss: 21.2649\n",
      "Epoch 20 | Batch: 41 | Loss: 13.0430\n",
      "Epoch 20 | Batch: 42 | Loss: 9.6858\n",
      "Epoch 20 | Batch: 43 | Loss: 8.1595\n",
      "Epoch 20 | Batch: 44 | Loss: 9.6320\n",
      "Epoch 20 | Batch: 45 | Loss: 20.7696\n",
      "Epoch 20 | Batch: 46 | Loss: 18.3259\n",
      "Epoch 20 | Batch: 47 | Loss: 10.3250\n",
      "Epoch 20 | Batch: 48 | Loss: 6.6832\n",
      "Mean 11.763737504680952\n",
      "Epoch 21 | Batch: 1 | Loss: 8.6436\n",
      "Epoch 21 | Batch: 2 | Loss: 7.9427\n",
      "Epoch 21 | Batch: 3 | Loss: 7.5965\n",
      "Epoch 21 | Batch: 4 | Loss: 8.5617\n",
      "Epoch 21 | Batch: 5 | Loss: 12.4912\n",
      "Epoch 21 | Batch: 6 | Loss: 2.7737\n",
      "Epoch 21 | Batch: 7 | Loss: 12.5505\n",
      "Epoch 21 | Batch: 8 | Loss: 2.8209\n",
      "Epoch 21 | Batch: 9 | Loss: 7.4273\n",
      "Epoch 21 | Batch: 10 | Loss: 11.5774\n",
      "Epoch 21 | Batch: 11 | Loss: 15.7783\n",
      "Epoch 21 | Batch: 12 | Loss: 24.7510\n",
      "Epoch 21 | Batch: 13 | Loss: 32.5259\n",
      "Epoch 21 | Batch: 14 | Loss: 9.3634\n",
      "Epoch 21 | Batch: 15 | Loss: 12.5567\n",
      "Epoch 21 | Batch: 16 | Loss: 8.3857\n",
      "Epoch 21 | Batch: 17 | Loss: 10.1198\n",
      "Epoch 21 | Batch: 18 | Loss: 14.3525\n",
      "Epoch 21 | Batch: 19 | Loss: 22.4253\n",
      "Epoch 21 | Batch: 20 | Loss: 21.6016\n",
      "Epoch 21 | Batch: 21 | Loss: 10.5123\n",
      "Epoch 21 | Batch: 22 | Loss: 8.0028\n",
      "Epoch 21 | Batch: 23 | Loss: 8.0448\n",
      "Epoch 21 | Batch: 24 | Loss: 2.8054\n",
      "Epoch 21 | Batch: 25 | Loss: 4.3101\n",
      "Epoch 21 | Batch: 26 | Loss: 11.7514\n",
      "Epoch 21 | Batch: 27 | Loss: 10.3990\n",
      "Epoch 21 | Batch: 28 | Loss: 8.5663\n",
      "Epoch 21 | Batch: 29 | Loss: 6.6220\n",
      "Epoch 21 | Batch: 30 | Loss: 8.0064\n",
      "Epoch 21 | Batch: 31 | Loss: 7.1013\n",
      "Epoch 21 | Batch: 32 | Loss: 12.1209\n",
      "Epoch 21 | Batch: 33 | Loss: 16.4241\n",
      "Epoch 21 | Batch: 34 | Loss: 7.9604\n",
      "Epoch 21 | Batch: 35 | Loss: 10.5415\n",
      "Epoch 21 | Batch: 36 | Loss: 6.3517\n",
      "Epoch 21 | Batch: 37 | Loss: 7.4468\n",
      "Epoch 21 | Batch: 38 | Loss: 4.5686\n",
      "Epoch 21 | Batch: 39 | Loss: 9.5321\n",
      "Epoch 21 | Batch: 40 | Loss: 7.7959\n",
      "Epoch 21 | Batch: 41 | Loss: 2.3389\n",
      "Epoch 21 | Batch: 42 | Loss: 11.5362\n",
      "Epoch 21 | Batch: 43 | Loss: 9.1295\n",
      "Epoch 21 | Batch: 44 | Loss: 8.9629\n",
      "Epoch 21 | Batch: 45 | Loss: 6.7222\n",
      "Epoch 21 | Batch: 46 | Loss: 7.3186\n",
      "Epoch 21 | Batch: 47 | Loss: 16.7862\n",
      "Epoch 21 | Batch: 48 | Loss: 3.3261\n",
      "Mean 10.192293137311935\n",
      "Epoch 22 | Batch: 1 | Loss: 20.3016\n",
      "Epoch 22 | Batch: 2 | Loss: 29.0395\n",
      "Epoch 22 | Batch: 3 | Loss: 6.5698\n",
      "Epoch 22 | Batch: 4 | Loss: 6.9687\n",
      "Epoch 22 | Batch: 5 | Loss: 9.7073\n",
      "Epoch 22 | Batch: 6 | Loss: 11.8309\n",
      "Epoch 22 | Batch: 7 | Loss: 7.9555\n",
      "Epoch 22 | Batch: 8 | Loss: 6.2471\n",
      "Epoch 22 | Batch: 9 | Loss: 10.8912\n",
      "Epoch 22 | Batch: 10 | Loss: 7.3534\n",
      "Epoch 22 | Batch: 11 | Loss: 5.3604\n",
      "Epoch 22 | Batch: 12 | Loss: 10.8658\n",
      "Epoch 22 | Batch: 13 | Loss: 7.2129\n",
      "Epoch 22 | Batch: 14 | Loss: 11.1656\n",
      "Epoch 22 | Batch: 15 | Loss: 6.6373\n",
      "Epoch 22 | Batch: 16 | Loss: 10.0720\n",
      "Epoch 22 | Batch: 17 | Loss: 16.8667\n",
      "Epoch 22 | Batch: 18 | Loss: 1.5236\n",
      "Epoch 22 | Batch: 19 | Loss: 13.2717\n",
      "Epoch 22 | Batch: 20 | Loss: 6.8561\n",
      "Epoch 22 | Batch: 21 | Loss: 6.2822\n",
      "Epoch 22 | Batch: 22 | Loss: 7.0144\n",
      "Epoch 22 | Batch: 23 | Loss: 10.7026\n",
      "Epoch 22 | Batch: 24 | Loss: 9.4625\n",
      "Epoch 22 | Batch: 25 | Loss: 19.1229\n",
      "Epoch 22 | Batch: 26 | Loss: 17.9344\n",
      "Epoch 22 | Batch: 27 | Loss: 19.8149\n",
      "Epoch 22 | Batch: 28 | Loss: 17.7541\n",
      "Epoch 22 | Batch: 29 | Loss: 15.8038\n",
      "Epoch 22 | Batch: 30 | Loss: 8.2520\n",
      "Epoch 22 | Batch: 31 | Loss: 7.6623\n",
      "Epoch 22 | Batch: 32 | Loss: 9.5715\n",
      "Epoch 22 | Batch: 33 | Loss: 10.9058\n",
      "Epoch 22 | Batch: 34 | Loss: 10.0986\n",
      "Epoch 22 | Batch: 35 | Loss: 5.4276\n",
      "Epoch 22 | Batch: 36 | Loss: 8.6214\n",
      "Epoch 22 | Batch: 37 | Loss: 10.4770\n",
      "Epoch 22 | Batch: 38 | Loss: 13.1634\n",
      "Epoch 22 | Batch: 39 | Loss: 9.5673\n",
      "Epoch 22 | Batch: 40 | Loss: 8.8313\n",
      "Epoch 22 | Batch: 41 | Loss: 6.8667\n",
      "Epoch 22 | Batch: 42 | Loss: 7.1843\n",
      "Epoch 22 | Batch: 43 | Loss: 8.5751\n",
      "Epoch 22 | Batch: 44 | Loss: 8.6078\n",
      "Epoch 22 | Batch: 45 | Loss: 6.5023\n",
      "Epoch 22 | Batch: 46 | Loss: 8.3920\n",
      "Epoch 22 | Batch: 47 | Loss: 15.4652\n",
      "Epoch 22 | Batch: 48 | Loss: 3.7183\n",
      "Mean 10.384980027874311\n",
      "Epoch 23 | Batch: 1 | Loss: 7.2320\n",
      "Epoch 23 | Batch: 2 | Loss: 9.4498\n",
      "Epoch 23 | Batch: 3 | Loss: 7.5681\n",
      "Epoch 23 | Batch: 4 | Loss: 5.1740\n",
      "Epoch 23 | Batch: 5 | Loss: 6.0764\n",
      "Epoch 23 | Batch: 6 | Loss: 4.3154\n",
      "Epoch 23 | Batch: 7 | Loss: 11.3951\n",
      "Epoch 23 | Batch: 8 | Loss: 6.0956\n",
      "Epoch 23 | Batch: 9 | Loss: 14.6783\n",
      "Epoch 23 | Batch: 10 | Loss: 13.5063\n",
      "Epoch 23 | Batch: 11 | Loss: 8.2025\n",
      "Epoch 23 | Batch: 12 | Loss: 11.7899\n",
      "Epoch 23 | Batch: 13 | Loss: 9.3218\n",
      "Epoch 23 | Batch: 14 | Loss: 6.1384\n",
      "Epoch 23 | Batch: 15 | Loss: 9.9133\n",
      "Epoch 23 | Batch: 16 | Loss: 8.5053\n",
      "Epoch 23 | Batch: 17 | Loss: 7.8900\n",
      "Epoch 23 | Batch: 18 | Loss: 10.2722\n",
      "Epoch 23 | Batch: 19 | Loss: 14.4640\n",
      "Epoch 23 | Batch: 20 | Loss: 23.0780\n",
      "Epoch 23 | Batch: 21 | Loss: 10.0138\n",
      "Epoch 23 | Batch: 22 | Loss: 9.1070\n",
      "Epoch 23 | Batch: 23 | Loss: 5.7465\n",
      "Epoch 23 | Batch: 24 | Loss: 6.2521\n",
      "Epoch 23 | Batch: 25 | Loss: 8.2513\n",
      "Epoch 23 | Batch: 26 | Loss: 15.7756\n",
      "Epoch 23 | Batch: 27 | Loss: 9.3470\n",
      "Epoch 23 | Batch: 28 | Loss: 7.8510\n",
      "Epoch 23 | Batch: 29 | Loss: 8.1803\n",
      "Epoch 23 | Batch: 30 | Loss: 8.1937\n",
      "Epoch 23 | Batch: 31 | Loss: 20.1487\n",
      "Epoch 23 | Batch: 32 | Loss: 11.1033\n",
      "Epoch 23 | Batch: 33 | Loss: 11.8179\n",
      "Epoch 23 | Batch: 34 | Loss: 11.5470\n",
      "Epoch 23 | Batch: 35 | Loss: 8.7429\n",
      "Epoch 23 | Batch: 36 | Loss: 7.7399\n",
      "Epoch 23 | Batch: 37 | Loss: 9.9965\n",
      "Epoch 23 | Batch: 38 | Loss: 8.6620\n",
      "Epoch 23 | Batch: 39 | Loss: 6.8484\n",
      "Epoch 23 | Batch: 40 | Loss: 8.1348\n",
      "Epoch 23 | Batch: 41 | Loss: 5.6419\n",
      "Epoch 23 | Batch: 42 | Loss: 17.4718\n",
      "Epoch 23 | Batch: 43 | Loss: 8.9724\n",
      "Epoch 23 | Batch: 44 | Loss: 9.3339\n",
      "Epoch 23 | Batch: 45 | Loss: 6.9839\n",
      "Epoch 23 | Batch: 46 | Loss: 7.2324\n",
      "Epoch 23 | Batch: 47 | Loss: 8.3377\n",
      "Epoch 23 | Batch: 48 | Loss: 4.8304\n",
      "Mean 9.527722309033075\n",
      "Epoch 24 | Batch: 1 | Loss: 8.8438\n",
      "Epoch 24 | Batch: 2 | Loss: 9.7869\n",
      "Epoch 24 | Batch: 3 | Loss: 9.2575\n",
      "Epoch 24 | Batch: 4 | Loss: 21.5890\n",
      "Epoch 24 | Batch: 5 | Loss: 32.8248\n",
      "Epoch 24 | Batch: 6 | Loss: 37.2801\n",
      "Epoch 24 | Batch: 7 | Loss: 9.3116\n",
      "Epoch 24 | Batch: 8 | Loss: 13.3626\n",
      "Epoch 24 | Batch: 9 | Loss: 9.5788\n",
      "Epoch 24 | Batch: 10 | Loss: 5.4532\n",
      "Epoch 24 | Batch: 11 | Loss: 7.5185\n",
      "Epoch 24 | Batch: 12 | Loss: 15.9115\n",
      "Epoch 24 | Batch: 13 | Loss: 20.8009\n",
      "Epoch 24 | Batch: 14 | Loss: 13.0514\n",
      "Epoch 24 | Batch: 15 | Loss: 18.4292\n",
      "Epoch 24 | Batch: 16 | Loss: 13.1668\n",
      "Epoch 24 | Batch: 17 | Loss: 8.7684\n",
      "Epoch 24 | Batch: 18 | Loss: 3.9273\n",
      "Epoch 24 | Batch: 19 | Loss: 10.2828\n",
      "Epoch 24 | Batch: 20 | Loss: 11.5355\n",
      "Epoch 24 | Batch: 21 | Loss: 9.9721\n",
      "Epoch 24 | Batch: 22 | Loss: 6.5451\n",
      "Epoch 24 | Batch: 23 | Loss: 4.5750\n",
      "Epoch 24 | Batch: 24 | Loss: 11.1432\n",
      "Epoch 24 | Batch: 25 | Loss: 8.0044\n",
      "Epoch 24 | Batch: 26 | Loss: 12.7258\n",
      "Epoch 24 | Batch: 27 | Loss: 6.6920\n",
      "Epoch 24 | Batch: 28 | Loss: 12.9619\n",
      "Epoch 24 | Batch: 29 | Loss: 18.2212\n",
      "Epoch 24 | Batch: 30 | Loss: 8.0043\n",
      "Epoch 24 | Batch: 31 | Loss: 13.7682\n",
      "Epoch 24 | Batch: 32 | Loss: 7.7709\n",
      "Epoch 24 | Batch: 33 | Loss: 7.8950\n",
      "Epoch 24 | Batch: 34 | Loss: 6.8932\n",
      "Epoch 24 | Batch: 35 | Loss: 5.8561\n",
      "Epoch 24 | Batch: 36 | Loss: 9.7896\n",
      "Epoch 24 | Batch: 37 | Loss: 8.3197\n",
      "Epoch 24 | Batch: 38 | Loss: 9.3188\n",
      "Epoch 24 | Batch: 39 | Loss: 6.2842\n",
      "Epoch 24 | Batch: 40 | Loss: 5.7073\n",
      "Epoch 24 | Batch: 41 | Loss: 6.7752\n",
      "Epoch 24 | Batch: 42 | Loss: 5.9687\n",
      "Epoch 24 | Batch: 43 | Loss: 6.0925\n",
      "Epoch 24 | Batch: 44 | Loss: 21.3096\n",
      "Epoch 24 | Batch: 45 | Loss: 21.0608\n",
      "Epoch 24 | Batch: 46 | Loss: 10.9367\n",
      "Epoch 24 | Batch: 47 | Loss: 12.6211\n",
      "Epoch 24 | Batch: 48 | Loss: 4.5824\n",
      "Mean 11.468246812621752\n",
      "Epoch 25 | Batch: 1 | Loss: 11.5719\n",
      "Epoch 25 | Batch: 2 | Loss: 8.3572\n",
      "Epoch 25 | Batch: 3 | Loss: 10.4292\n",
      "Epoch 25 | Batch: 4 | Loss: 8.0253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 | Batch: 5 | Loss: 8.2291\n",
      "Epoch 25 | Batch: 6 | Loss: 7.4486\n",
      "Epoch 25 | Batch: 7 | Loss: 21.6457\n",
      "Epoch 25 | Batch: 8 | Loss: 25.0338\n",
      "Epoch 25 | Batch: 9 | Loss: 15.5549\n",
      "Epoch 25 | Batch: 10 | Loss: 23.6173\n",
      "Epoch 25 | Batch: 11 | Loss: 15.2731\n",
      "Epoch 25 | Batch: 12 | Loss: 11.2985\n",
      "Epoch 25 | Batch: 13 | Loss: 7.4383\n",
      "Epoch 25 | Batch: 14 | Loss: 9.0576\n",
      "Epoch 25 | Batch: 15 | Loss: 7.2290\n",
      "Epoch 25 | Batch: 16 | Loss: 15.0711\n",
      "Epoch 25 | Batch: 17 | Loss: 7.2547\n",
      "Epoch 25 | Batch: 18 | Loss: 7.7053\n",
      "Epoch 25 | Batch: 19 | Loss: 5.1419\n",
      "Epoch 25 | Batch: 20 | Loss: 15.6520\n",
      "Epoch 25 | Batch: 21 | Loss: 36.2091\n",
      "Epoch 25 | Batch: 22 | Loss: 32.2179\n",
      "Epoch 25 | Batch: 23 | Loss: 5.9583\n",
      "Epoch 25 | Batch: 24 | Loss: 8.4049\n",
      "Epoch 25 | Batch: 25 | Loss: 7.9533\n",
      "Epoch 25 | Batch: 26 | Loss: 7.7811\n",
      "Epoch 25 | Batch: 27 | Loss: 8.3109\n",
      "Epoch 25 | Batch: 28 | Loss: 6.9609\n",
      "Epoch 25 | Batch: 29 | Loss: 7.0030\n",
      "Epoch 25 | Batch: 30 | Loss: 10.3328\n",
      "Epoch 25 | Batch: 31 | Loss: 6.0407\n",
      "Epoch 25 | Batch: 32 | Loss: 7.1314\n",
      "Epoch 25 | Batch: 33 | Loss: 10.2068\n",
      "Epoch 25 | Batch: 34 | Loss: 7.7703\n",
      "Epoch 25 | Batch: 35 | Loss: 7.5594\n",
      "Epoch 25 | Batch: 36 | Loss: 6.8795\n",
      "Epoch 25 | Batch: 37 | Loss: 8.1402\n",
      "Epoch 25 | Batch: 38 | Loss: 14.1340\n",
      "Epoch 25 | Batch: 39 | Loss: 5.0843\n",
      "Epoch 25 | Batch: 40 | Loss: 8.4754\n",
      "Epoch 25 | Batch: 41 | Loss: 5.6332\n",
      "Epoch 25 | Batch: 42 | Loss: 11.1379\n",
      "Epoch 25 | Batch: 43 | Loss: 8.3281\n",
      "Epoch 25 | Batch: 44 | Loss: 15.8006\n",
      "Epoch 25 | Batch: 45 | Loss: 12.7299\n",
      "Epoch 25 | Batch: 46 | Loss: 5.3792\n",
      "Epoch 25 | Batch: 47 | Loss: 7.7223\n",
      "Epoch 25 | Batch: 48 | Loss: 6.3639\n",
      "Mean 10.972574273745218\n",
      "Epoch 26 | Batch: 1 | Loss: 6.5486\n",
      "Epoch 26 | Batch: 2 | Loss: 4.9184\n",
      "Epoch 26 | Batch: 3 | Loss: 9.4099\n",
      "Epoch 26 | Batch: 4 | Loss: 9.8134\n",
      "Epoch 26 | Batch: 5 | Loss: 10.7735\n",
      "Epoch 26 | Batch: 6 | Loss: 7.5338\n",
      "Epoch 26 | Batch: 7 | Loss: 11.4571\n",
      "Epoch 26 | Batch: 8 | Loss: 12.8564\n",
      "Epoch 26 | Batch: 9 | Loss: 20.0503\n",
      "Epoch 26 | Batch: 10 | Loss: 12.5471\n",
      "Epoch 26 | Batch: 11 | Loss: 27.0823\n",
      "Epoch 26 | Batch: 12 | Loss: 7.3917\n",
      "Epoch 26 | Batch: 13 | Loss: 10.8300\n",
      "Epoch 26 | Batch: 14 | Loss: 19.6181\n",
      "Epoch 26 | Batch: 15 | Loss: 18.4300\n",
      "Epoch 26 | Batch: 16 | Loss: 10.4079\n",
      "Epoch 26 | Batch: 17 | Loss: 8.9898\n",
      "Epoch 26 | Batch: 18 | Loss: 10.5847\n",
      "Epoch 26 | Batch: 19 | Loss: 15.8446\n",
      "Epoch 26 | Batch: 20 | Loss: 7.7264\n",
      "Epoch 26 | Batch: 21 | Loss: 4.6584\n",
      "Epoch 26 | Batch: 22 | Loss: 13.6171\n",
      "Epoch 26 | Batch: 23 | Loss: 10.7582\n",
      "Epoch 26 | Batch: 24 | Loss: 16.9562\n",
      "Epoch 26 | Batch: 25 | Loss: 22.8447\n",
      "Epoch 26 | Batch: 26 | Loss: 29.2821\n",
      "Epoch 26 | Batch: 27 | Loss: 17.0637\n",
      "Epoch 26 | Batch: 28 | Loss: 8.4767\n",
      "Epoch 26 | Batch: 29 | Loss: 13.8037\n",
      "Epoch 26 | Batch: 30 | Loss: 10.7182\n",
      "Epoch 26 | Batch: 31 | Loss: 10.1516\n",
      "Epoch 26 | Batch: 32 | Loss: 6.7559\n",
      "Epoch 26 | Batch: 33 | Loss: 5.4760\n",
      "Epoch 26 | Batch: 34 | Loss: 9.1503\n",
      "Epoch 26 | Batch: 35 | Loss: 7.2140\n",
      "Epoch 26 | Batch: 36 | Loss: 13.6580\n",
      "Epoch 26 | Batch: 37 | Loss: 11.9487\n",
      "Epoch 26 | Batch: 38 | Loss: 8.0279\n",
      "Epoch 26 | Batch: 39 | Loss: 9.0095\n",
      "Epoch 26 | Batch: 40 | Loss: 8.9023\n",
      "Epoch 26 | Batch: 41 | Loss: 9.0285\n",
      "Epoch 26 | Batch: 42 | Loss: 7.0727\n",
      "Epoch 26 | Batch: 43 | Loss: 5.8703\n",
      "Epoch 26 | Batch: 44 | Loss: 5.8496\n",
      "Epoch 26 | Batch: 45 | Loss: 6.5012\n",
      "Epoch 26 | Batch: 46 | Loss: 5.0952\n",
      "Epoch 26 | Batch: 47 | Loss: 6.4925\n",
      "Epoch 26 | Batch: 48 | Loss: 1.4845\n",
      "Mean 11.014205368856588\n",
      "Epoch 27 | Batch: 1 | Loss: 7.3985\n",
      "Epoch 27 | Batch: 2 | Loss: 14.5493\n",
      "Epoch 27 | Batch: 3 | Loss: 9.6908\n",
      "Epoch 27 | Batch: 4 | Loss: 15.5541\n",
      "Epoch 27 | Batch: 5 | Loss: 4.2045\n",
      "Epoch 27 | Batch: 6 | Loss: 11.4773\n",
      "Epoch 27 | Batch: 7 | Loss: 12.6988\n",
      "Epoch 27 | Batch: 8 | Loss: 13.7623\n",
      "Epoch 27 | Batch: 9 | Loss: 10.8601\n",
      "Epoch 27 | Batch: 10 | Loss: 10.3484\n",
      "Epoch 27 | Batch: 11 | Loss: 17.3522\n",
      "Epoch 27 | Batch: 12 | Loss: 10.3969\n",
      "Epoch 27 | Batch: 13 | Loss: 11.3610\n",
      "Epoch 27 | Batch: 14 | Loss: 11.7859\n",
      "Epoch 27 | Batch: 15 | Loss: 11.5798\n",
      "Epoch 27 | Batch: 16 | Loss: 6.9050\n",
      "Epoch 27 | Batch: 17 | Loss: 5.9717\n",
      "Epoch 27 | Batch: 18 | Loss: 9.2373\n",
      "Epoch 27 | Batch: 19 | Loss: 8.4739\n",
      "Epoch 27 | Batch: 20 | Loss: 3.0724\n",
      "Epoch 27 | Batch: 21 | Loss: 7.6474\n",
      "Epoch 27 | Batch: 22 | Loss: 10.7141\n",
      "Epoch 27 | Batch: 23 | Loss: 5.9557\n",
      "Epoch 27 | Batch: 24 | Loss: 6.6922\n",
      "Epoch 27 | Batch: 25 | Loss: 4.1835\n",
      "Epoch 27 | Batch: 26 | Loss: 8.4403\n",
      "Epoch 27 | Batch: 27 | Loss: 15.1668\n",
      "Epoch 27 | Batch: 28 | Loss: 11.2328\n",
      "Epoch 27 | Batch: 29 | Loss: 3.6136\n",
      "Epoch 27 | Batch: 30 | Loss: 5.1659\n",
      "Epoch 27 | Batch: 31 | Loss: 14.1361\n",
      "Epoch 27 | Batch: 32 | Loss: 15.8204\n",
      "Epoch 27 | Batch: 33 | Loss: 11.3933\n",
      "Epoch 27 | Batch: 34 | Loss: 10.0589\n",
      "Epoch 27 | Batch: 35 | Loss: 5.0291\n",
      "Epoch 27 | Batch: 36 | Loss: 6.5742\n",
      "Epoch 27 | Batch: 37 | Loss: 7.8975\n",
      "Epoch 27 | Batch: 38 | Loss: 8.6628\n",
      "Epoch 27 | Batch: 39 | Loss: 2.3753\n",
      "Epoch 27 | Batch: 40 | Loss: 8.1174\n",
      "Epoch 27 | Batch: 41 | Loss: 12.2510\n",
      "Epoch 27 | Batch: 42 | Loss: 25.8114\n",
      "Epoch 27 | Batch: 43 | Loss: 39.5944\n",
      "Epoch 27 | Batch: 44 | Loss: 20.1049\n",
      "Epoch 27 | Batch: 45 | Loss: 8.0028\n",
      "Epoch 27 | Batch: 46 | Loss: 9.4235\n",
      "Epoch 27 | Batch: 47 | Loss: 11.8799\n",
      "Epoch 27 | Batch: 48 | Loss: 3.2845\n",
      "Mean 10.539780393242836\n",
      "Epoch 28 | Batch: 1 | Loss: 6.0372\n",
      "Epoch 28 | Batch: 2 | Loss: 8.1166\n",
      "Epoch 28 | Batch: 3 | Loss: 5.8713\n",
      "Epoch 28 | Batch: 4 | Loss: 14.0793\n",
      "Epoch 28 | Batch: 5 | Loss: 17.3667\n",
      "Epoch 28 | Batch: 6 | Loss: 13.4485\n",
      "Epoch 28 | Batch: 7 | Loss: 25.2053\n",
      "Epoch 28 | Batch: 8 | Loss: 24.4963\n",
      "Epoch 28 | Batch: 9 | Loss: 11.3096\n",
      "Epoch 28 | Batch: 10 | Loss: 10.1233\n",
      "Epoch 28 | Batch: 11 | Loss: 12.3220\n",
      "Epoch 28 | Batch: 12 | Loss: 9.2670\n",
      "Epoch 28 | Batch: 13 | Loss: 10.5264\n",
      "Epoch 28 | Batch: 14 | Loss: 13.1882\n",
      "Epoch 28 | Batch: 15 | Loss: 5.3784\n",
      "Epoch 28 | Batch: 16 | Loss: 9.1207\n",
      "Epoch 28 | Batch: 17 | Loss: 8.8217\n",
      "Epoch 28 | Batch: 18 | Loss: 6.4090\n",
      "Epoch 28 | Batch: 19 | Loss: 7.1089\n",
      "Epoch 28 | Batch: 20 | Loss: 11.9636\n",
      "Epoch 28 | Batch: 21 | Loss: 16.9787\n",
      "Epoch 28 | Batch: 22 | Loss: 16.2948\n",
      "Epoch 28 | Batch: 23 | Loss: 9.4723\n",
      "Epoch 28 | Batch: 24 | Loss: 6.7884\n",
      "Epoch 28 | Batch: 25 | Loss: 7.8379\n",
      "Epoch 28 | Batch: 26 | Loss: 8.1047\n",
      "Epoch 28 | Batch: 27 | Loss: 6.5838\n",
      "Epoch 28 | Batch: 28 | Loss: 7.8969\n",
      "Epoch 28 | Batch: 29 | Loss: 10.0124\n",
      "Epoch 28 | Batch: 30 | Loss: 7.8526\n",
      "Epoch 28 | Batch: 31 | Loss: 8.5254\n",
      "Epoch 28 | Batch: 32 | Loss: 8.6269\n",
      "Epoch 28 | Batch: 33 | Loss: 5.6524\n",
      "Epoch 28 | Batch: 34 | Loss: 3.3436\n",
      "Epoch 28 | Batch: 35 | Loss: 14.7526\n",
      "Epoch 28 | Batch: 36 | Loss: 6.9207\n",
      "Epoch 28 | Batch: 37 | Loss: 7.9465\n",
      "Epoch 28 | Batch: 38 | Loss: 12.7554\n",
      "Epoch 28 | Batch: 39 | Loss: 11.0727\n",
      "Epoch 28 | Batch: 40 | Loss: 9.2619\n",
      "Epoch 28 | Batch: 41 | Loss: 10.2954\n",
      "Epoch 28 | Batch: 42 | Loss: 14.5250\n",
      "Epoch 28 | Batch: 43 | Loss: 10.9784\n",
      "Epoch 28 | Batch: 44 | Loss: 9.9023\n",
      "Epoch 28 | Batch: 45 | Loss: 10.9274\n",
      "Epoch 28 | Batch: 46 | Loss: 10.5888\n",
      "Epoch 28 | Batch: 47 | Loss: 13.1332\n",
      "Epoch 28 | Batch: 48 | Loss: 6.1128\n",
      "Mean 10.485500325759253\n",
      "Epoch 29 | Batch: 1 | Loss: 11.0151\n",
      "Epoch 29 | Batch: 2 | Loss: 13.0619\n",
      "Epoch 29 | Batch: 3 | Loss: 9.0594\n",
      "Epoch 29 | Batch: 4 | Loss: 7.3017\n",
      "Epoch 29 | Batch: 5 | Loss: 7.5088\n",
      "Epoch 29 | Batch: 6 | Loss: 5.7117\n",
      "Epoch 29 | Batch: 7 | Loss: 12.9539\n",
      "Epoch 29 | Batch: 8 | Loss: 14.2576\n",
      "Epoch 29 | Batch: 9 | Loss: 11.3838\n",
      "Epoch 29 | Batch: 10 | Loss: 8.1007\n",
      "Epoch 29 | Batch: 11 | Loss: 7.5219\n",
      "Epoch 29 | Batch: 12 | Loss: 4.0697\n",
      "Epoch 29 | Batch: 13 | Loss: 5.3296\n",
      "Epoch 29 | Batch: 14 | Loss: 10.9223\n",
      "Epoch 29 | Batch: 15 | Loss: 5.4756\n",
      "Epoch 29 | Batch: 16 | Loss: 6.9713\n",
      "Epoch 29 | Batch: 17 | Loss: 12.1717\n",
      "Epoch 29 | Batch: 18 | Loss: 19.9688\n",
      "Epoch 29 | Batch: 19 | Loss: 9.6220\n",
      "Epoch 29 | Batch: 20 | Loss: 5.5229\n",
      "Epoch 29 | Batch: 21 | Loss: 6.8528\n",
      "Epoch 29 | Batch: 22 | Loss: 11.3017\n",
      "Epoch 29 | Batch: 23 | Loss: 9.7369\n",
      "Epoch 29 | Batch: 24 | Loss: 4.6729\n",
      "Epoch 29 | Batch: 25 | Loss: 4.6149\n",
      "Epoch 29 | Batch: 26 | Loss: 7.2561\n",
      "Epoch 29 | Batch: 27 | Loss: 9.1222\n",
      "Epoch 29 | Batch: 28 | Loss: 11.7419\n",
      "Epoch 29 | Batch: 29 | Loss: 7.6270\n",
      "Epoch 29 | Batch: 30 | Loss: 6.2869\n",
      "Epoch 29 | Batch: 31 | Loss: 11.2430\n",
      "Epoch 29 | Batch: 32 | Loss: 10.9094\n",
      "Epoch 29 | Batch: 33 | Loss: 7.8758\n",
      "Epoch 29 | Batch: 34 | Loss: 10.2390\n",
      "Epoch 29 | Batch: 35 | Loss: 13.4172\n",
      "Epoch 29 | Batch: 36 | Loss: 7.9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 | Batch: 37 | Loss: 11.0307\n",
      "Epoch 29 | Batch: 38 | Loss: 11.3559\n",
      "Epoch 29 | Batch: 39 | Loss: 9.6579\n",
      "Epoch 29 | Batch: 40 | Loss: 16.2464\n",
      "Epoch 29 | Batch: 41 | Loss: 14.6816\n",
      "Epoch 29 | Batch: 42 | Loss: 12.4732\n",
      "Epoch 29 | Batch: 43 | Loss: 11.0305\n",
      "Epoch 29 | Batch: 44 | Loss: 10.7253\n",
      "Epoch 29 | Batch: 45 | Loss: 6.2150\n",
      "Epoch 29 | Batch: 46 | Loss: 6.8263\n",
      "Epoch 29 | Batch: 47 | Loss: 3.4465\n",
      "Epoch 29 | Batch: 48 | Loss: 3.1110\n",
      "Mean 9.282095660765966\n",
      "Epoch 30 | Batch: 1 | Loss: 6.4129\n",
      "Epoch 30 | Batch: 2 | Loss: 15.3633\n",
      "Epoch 30 | Batch: 3 | Loss: 6.8854\n",
      "Epoch 30 | Batch: 4 | Loss: 11.1305\n",
      "Epoch 30 | Batch: 5 | Loss: 16.9008\n",
      "Epoch 30 | Batch: 6 | Loss: 10.8144\n",
      "Epoch 30 | Batch: 7 | Loss: 11.5541\n",
      "Epoch 30 | Batch: 8 | Loss: 8.4184\n",
      "Epoch 30 | Batch: 9 | Loss: 3.6685\n",
      "Epoch 30 | Batch: 10 | Loss: 6.5046\n",
      "Epoch 30 | Batch: 11 | Loss: 10.0598\n",
      "Epoch 30 | Batch: 12 | Loss: 9.0108\n",
      "Epoch 30 | Batch: 13 | Loss: 7.0948\n",
      "Epoch 30 | Batch: 14 | Loss: 9.9719\n",
      "Epoch 30 | Batch: 15 | Loss: 4.4899\n",
      "Epoch 30 | Batch: 16 | Loss: 8.6733\n",
      "Epoch 30 | Batch: 17 | Loss: 7.8135\n",
      "Epoch 30 | Batch: 18 | Loss: 13.4458\n",
      "Epoch 30 | Batch: 19 | Loss: 8.4062\n",
      "Epoch 30 | Batch: 20 | Loss: 6.2942\n",
      "Epoch 30 | Batch: 21 | Loss: 8.0481\n",
      "Epoch 30 | Batch: 22 | Loss: 9.2386\n",
      "Epoch 30 | Batch: 23 | Loss: 5.2568\n",
      "Epoch 30 | Batch: 24 | Loss: 4.6341\n",
      "Epoch 30 | Batch: 25 | Loss: 5.8689\n",
      "Epoch 30 | Batch: 26 | Loss: 12.6562\n",
      "Epoch 30 | Batch: 27 | Loss: 9.6012\n",
      "Epoch 30 | Batch: 28 | Loss: 8.9899\n",
      "Epoch 30 | Batch: 29 | Loss: 8.5139\n",
      "Epoch 30 | Batch: 30 | Loss: 16.6756\n",
      "Epoch 30 | Batch: 31 | Loss: 4.0350\n",
      "Epoch 30 | Batch: 32 | Loss: 5.4088\n",
      "Epoch 30 | Batch: 33 | Loss: 6.5020\n",
      "Epoch 30 | Batch: 34 | Loss: 11.8010\n",
      "Epoch 30 | Batch: 35 | Loss: 10.9679\n",
      "Epoch 30 | Batch: 36 | Loss: 10.5846\n",
      "Epoch 30 | Batch: 37 | Loss: 16.6373\n",
      "Epoch 30 | Batch: 38 | Loss: 12.9108\n",
      "Epoch 30 | Batch: 39 | Loss: 6.7465\n",
      "Epoch 30 | Batch: 40 | Loss: 9.8657\n",
      "Epoch 30 | Batch: 41 | Loss: 17.0350\n",
      "Epoch 30 | Batch: 42 | Loss: 12.4476\n",
      "Epoch 30 | Batch: 43 | Loss: 13.0619\n",
      "Epoch 30 | Batch: 44 | Loss: 7.1357\n",
      "Epoch 30 | Batch: 45 | Loss: 7.3474\n",
      "Epoch 30 | Batch: 46 | Loss: 10.4614\n",
      "Epoch 30 | Batch: 47 | Loss: 6.0792\n",
      "Epoch 30 | Batch: 48 | Loss: 3.4876\n",
      "Mean 9.269000391165415\n",
      "Epoch 31 | Batch: 1 | Loss: 7.6212\n",
      "Epoch 31 | Batch: 2 | Loss: 9.8752\n",
      "Epoch 31 | Batch: 3 | Loss: 4.2668\n",
      "Epoch 31 | Batch: 4 | Loss: 13.5483\n",
      "Epoch 31 | Batch: 5 | Loss: 15.1157\n",
      "Epoch 31 | Batch: 6 | Loss: 14.1039\n",
      "Epoch 31 | Batch: 7 | Loss: 18.3350\n",
      "Epoch 31 | Batch: 8 | Loss: 19.4951\n",
      "Epoch 31 | Batch: 9 | Loss: 12.5399\n",
      "Epoch 31 | Batch: 10 | Loss: 7.2011\n",
      "Epoch 31 | Batch: 11 | Loss: 11.2220\n",
      "Epoch 31 | Batch: 12 | Loss: 13.7579\n",
      "Epoch 31 | Batch: 13 | Loss: 12.9748\n",
      "Epoch 31 | Batch: 14 | Loss: 10.4881\n",
      "Epoch 31 | Batch: 15 | Loss: 10.6454\n",
      "Epoch 31 | Batch: 16 | Loss: 6.3357\n",
      "Epoch 31 | Batch: 17 | Loss: 13.4181\n",
      "Epoch 31 | Batch: 18 | Loss: 6.9195\n",
      "Epoch 31 | Batch: 19 | Loss: 3.9321\n",
      "Epoch 31 | Batch: 20 | Loss: 7.3407\n",
      "Epoch 31 | Batch: 21 | Loss: 13.0025\n",
      "Epoch 31 | Batch: 22 | Loss: 6.4544\n",
      "Epoch 31 | Batch: 23 | Loss: 7.4819\n",
      "Epoch 31 | Batch: 24 | Loss: 9.4341\n",
      "Epoch 31 | Batch: 25 | Loss: 8.5633\n",
      "Epoch 31 | Batch: 26 | Loss: 7.6784\n",
      "Epoch 31 | Batch: 27 | Loss: 5.6641\n",
      "Epoch 31 | Batch: 28 | Loss: 10.8784\n",
      "Epoch 31 | Batch: 29 | Loss: 5.1135\n",
      "Epoch 31 | Batch: 30 | Loss: 10.6388\n",
      "Epoch 31 | Batch: 31 | Loss: 17.0491\n",
      "Epoch 31 | Batch: 32 | Loss: 13.4911\n",
      "Epoch 31 | Batch: 33 | Loss: 7.2129\n",
      "Epoch 31 | Batch: 34 | Loss: 7.1605\n",
      "Epoch 31 | Batch: 35 | Loss: 13.4569\n",
      "Epoch 31 | Batch: 36 | Loss: 17.8809\n",
      "Epoch 31 | Batch: 37 | Loss: 10.7748\n",
      "Epoch 31 | Batch: 38 | Loss: 12.2584\n",
      "Epoch 31 | Batch: 39 | Loss: 28.9726\n",
      "Epoch 31 | Batch: 40 | Loss: 4.7907\n",
      "Epoch 31 | Batch: 41 | Loss: 7.9375\n",
      "Epoch 31 | Batch: 42 | Loss: 13.6523\n",
      "Epoch 31 | Batch: 43 | Loss: 13.1547\n",
      "Epoch 31 | Batch: 44 | Loss: 9.1070\n",
      "Epoch 31 | Batch: 45 | Loss: 7.7252\n",
      "Epoch 31 | Batch: 46 | Loss: 6.5574\n",
      "Epoch 31 | Batch: 47 | Loss: 5.4551\n",
      "Epoch 31 | Batch: 48 | Loss: 3.0108\n",
      "Mean 10.493620986739794\n",
      "Epoch 32 | Batch: 1 | Loss: 8.6966\n",
      "Epoch 32 | Batch: 2 | Loss: 13.7578\n",
      "Epoch 32 | Batch: 3 | Loss: 10.8064\n",
      "Epoch 32 | Batch: 4 | Loss: 9.2511\n",
      "Epoch 32 | Batch: 5 | Loss: 6.6875\n",
      "Epoch 32 | Batch: 6 | Loss: 4.5408\n",
      "Epoch 32 | Batch: 7 | Loss: 9.8677\n",
      "Epoch 32 | Batch: 8 | Loss: 6.5478\n",
      "Epoch 32 | Batch: 9 | Loss: 9.3121\n",
      "Epoch 32 | Batch: 10 | Loss: 9.7509\n",
      "Epoch 32 | Batch: 11 | Loss: 8.7433\n",
      "Epoch 32 | Batch: 12 | Loss: 11.7284\n",
      "Epoch 32 | Batch: 13 | Loss: 18.6188\n",
      "Epoch 32 | Batch: 14 | Loss: 13.8087\n",
      "Epoch 32 | Batch: 15 | Loss: 11.6460\n",
      "Epoch 32 | Batch: 16 | Loss: 7.5894\n",
      "Epoch 32 | Batch: 17 | Loss: 8.6838\n",
      "Epoch 32 | Batch: 18 | Loss: 7.7149\n",
      "Epoch 32 | Batch: 19 | Loss: 13.5111\n",
      "Epoch 32 | Batch: 20 | Loss: 12.5202\n",
      "Epoch 32 | Batch: 21 | Loss: 18.5756\n",
      "Epoch 32 | Batch: 22 | Loss: 10.1079\n",
      "Epoch 32 | Batch: 23 | Loss: 10.5291\n",
      "Epoch 32 | Batch: 24 | Loss: 4.0763\n",
      "Epoch 32 | Batch: 25 | Loss: 4.7707\n",
      "Epoch 32 | Batch: 26 | Loss: 13.4147\n",
      "Epoch 32 | Batch: 27 | Loss: 10.0580\n",
      "Epoch 32 | Batch: 28 | Loss: 9.4263\n",
      "Epoch 32 | Batch: 29 | Loss: 11.7745\n",
      "Epoch 32 | Batch: 30 | Loss: 7.3849\n",
      "Epoch 32 | Batch: 31 | Loss: 11.4842\n",
      "Epoch 32 | Batch: 32 | Loss: 11.1813\n",
      "Epoch 32 | Batch: 33 | Loss: 8.3172\n",
      "Epoch 32 | Batch: 34 | Loss: 7.0631\n",
      "Epoch 32 | Batch: 35 | Loss: 6.6637\n",
      "Epoch 32 | Batch: 36 | Loss: 6.2403\n",
      "Epoch 32 | Batch: 37 | Loss: 6.8645\n",
      "Epoch 32 | Batch: 38 | Loss: 5.3165\n",
      "Epoch 32 | Batch: 39 | Loss: 10.3781\n",
      "Epoch 32 | Batch: 40 | Loss: 15.6814\n",
      "Epoch 32 | Batch: 41 | Loss: 35.9379\n",
      "Epoch 32 | Batch: 42 | Loss: 13.8520\n",
      "Epoch 32 | Batch: 43 | Loss: 12.0477\n",
      "Epoch 32 | Batch: 44 | Loss: 14.2119\n",
      "Epoch 32 | Batch: 45 | Loss: 10.4979\n",
      "Epoch 32 | Batch: 46 | Loss: 9.2463\n",
      "Epoch 32 | Batch: 47 | Loss: 9.6607\n",
      "Epoch 32 | Batch: 48 | Loss: 2.1103\n",
      "Mean 10.430334766705831\n",
      "Epoch 33 | Batch: 1 | Loss: 9.4433\n",
      "Epoch 33 | Batch: 2 | Loss: 11.0985\n",
      "Epoch 33 | Batch: 3 | Loss: 8.8419\n",
      "Epoch 33 | Batch: 4 | Loss: 10.9769\n",
      "Epoch 33 | Batch: 5 | Loss: 7.1608\n",
      "Epoch 33 | Batch: 6 | Loss: 8.9566\n",
      "Epoch 33 | Batch: 7 | Loss: 9.1054\n",
      "Epoch 33 | Batch: 8 | Loss: 7.7958\n",
      "Epoch 33 | Batch: 9 | Loss: 5.9081\n",
      "Epoch 33 | Batch: 10 | Loss: 18.7453\n",
      "Epoch 33 | Batch: 11 | Loss: 16.6839\n",
      "Epoch 33 | Batch: 12 | Loss: 16.5582\n",
      "Epoch 33 | Batch: 13 | Loss: 13.4272\n",
      "Epoch 33 | Batch: 14 | Loss: 17.0440\n",
      "Epoch 33 | Batch: 15 | Loss: 12.9284\n",
      "Epoch 33 | Batch: 16 | Loss: 8.7953\n",
      "Epoch 33 | Batch: 17 | Loss: 9.7167\n",
      "Epoch 33 | Batch: 18 | Loss: 7.1897\n",
      "Epoch 33 | Batch: 19 | Loss: 9.1445\n",
      "Epoch 33 | Batch: 20 | Loss: 9.5578\n",
      "Epoch 33 | Batch: 21 | Loss: 14.3471\n",
      "Epoch 33 | Batch: 22 | Loss: 20.8301\n",
      "Epoch 33 | Batch: 23 | Loss: 7.0506\n",
      "Epoch 33 | Batch: 24 | Loss: 9.7035\n",
      "Epoch 33 | Batch: 25 | Loss: 8.6654\n",
      "Epoch 33 | Batch: 26 | Loss: 7.5802\n",
      "Epoch 33 | Batch: 27 | Loss: 6.5557\n",
      "Epoch 33 | Batch: 28 | Loss: 9.5159\n",
      "Epoch 33 | Batch: 29 | Loss: 4.9273\n",
      "Epoch 33 | Batch: 30 | Loss: 3.9686\n",
      "Epoch 33 | Batch: 31 | Loss: 3.0564\n",
      "Epoch 33 | Batch: 32 | Loss: 7.0523\n",
      "Epoch 33 | Batch: 33 | Loss: 9.3547\n",
      "Epoch 33 | Batch: 34 | Loss: 9.4150\n",
      "Epoch 33 | Batch: 35 | Loss: 4.8254\n",
      "Epoch 33 | Batch: 36 | Loss: 6.0842\n",
      "Epoch 33 | Batch: 37 | Loss: 9.7494\n",
      "Epoch 33 | Batch: 38 | Loss: 9.4743\n",
      "Epoch 33 | Batch: 39 | Loss: 10.2831\n",
      "Epoch 33 | Batch: 40 | Loss: 15.7675\n",
      "Epoch 33 | Batch: 41 | Loss: 21.3597\n",
      "Epoch 33 | Batch: 42 | Loss: 8.4555\n",
      "Epoch 33 | Batch: 43 | Loss: 8.1554\n",
      "Epoch 33 | Batch: 44 | Loss: 16.8548\n",
      "Epoch 33 | Batch: 45 | Loss: 18.3613\n",
      "Epoch 33 | Batch: 46 | Loss: 6.8182\n",
      "Epoch 33 | Batch: 47 | Loss: 6.8590\n",
      "Epoch 33 | Batch: 48 | Loss: 2.7710\n",
      "Mean 10.144174034396807\n",
      "Epoch 34 | Batch: 1 | Loss: 8.1785\n",
      "Epoch 34 | Batch: 2 | Loss: 9.0541\n",
      "Epoch 34 | Batch: 3 | Loss: 13.0332\n",
      "Epoch 34 | Batch: 4 | Loss: 7.9366\n",
      "Epoch 34 | Batch: 5 | Loss: 7.3847\n",
      "Epoch 34 | Batch: 6 | Loss: 4.7995\n",
      "Epoch 34 | Batch: 7 | Loss: 5.2711\n",
      "Epoch 34 | Batch: 8 | Loss: 11.2996\n",
      "Epoch 34 | Batch: 9 | Loss: 6.8782\n",
      "Epoch 34 | Batch: 10 | Loss: 5.8073\n",
      "Epoch 34 | Batch: 11 | Loss: 9.0990\n",
      "Epoch 34 | Batch: 12 | Loss: 4.4227\n",
      "Epoch 34 | Batch: 13 | Loss: 8.6430\n",
      "Epoch 34 | Batch: 14 | Loss: 10.7444\n",
      "Epoch 34 | Batch: 15 | Loss: 11.0757\n",
      "Epoch 34 | Batch: 16 | Loss: 18.4077\n",
      "Epoch 34 | Batch: 17 | Loss: 8.6583\n",
      "Epoch 34 | Batch: 18 | Loss: 13.5591\n",
      "Epoch 34 | Batch: 19 | Loss: 22.0963\n",
      "Epoch 34 | Batch: 20 | Loss: 14.4642\n",
      "Epoch 34 | Batch: 21 | Loss: 11.3636\n",
      "Epoch 34 | Batch: 22 | Loss: 7.2079\n",
      "Epoch 34 | Batch: 23 | Loss: 10.2335\n",
      "Epoch 34 | Batch: 24 | Loss: 9.2564\n",
      "Epoch 34 | Batch: 25 | Loss: 5.7031\n",
      "Epoch 34 | Batch: 26 | Loss: 12.0364\n",
      "Epoch 34 | Batch: 27 | Loss: 16.8665\n",
      "Epoch 34 | Batch: 28 | Loss: 21.3569\n",
      "Epoch 34 | Batch: 29 | Loss: 16.3827\n",
      "Epoch 34 | Batch: 30 | Loss: 16.6801\n",
      "Epoch 34 | Batch: 31 | Loss: 28.7603\n",
      "Epoch 34 | Batch: 32 | Loss: 22.9597\n",
      "Epoch 34 | Batch: 33 | Loss: 9.3155\n",
      "Epoch 34 | Batch: 34 | Loss: 10.6930\n",
      "Epoch 34 | Batch: 35 | Loss: 7.7981\n",
      "Epoch 34 | Batch: 36 | Loss: 14.8022\n",
      "Epoch 34 | Batch: 37 | Loss: 14.9051\n",
      "Epoch 34 | Batch: 38 | Loss: 5.1030\n",
      "Epoch 34 | Batch: 39 | Loss: 10.9878\n",
      "Epoch 34 | Batch: 40 | Loss: 8.4198\n",
      "Epoch 34 | Batch: 41 | Loss: 7.2209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 | Batch: 42 | Loss: 4.8984\n",
      "Epoch 34 | Batch: 43 | Loss: 8.3342\n",
      "Epoch 34 | Batch: 44 | Loss: 6.7980\n",
      "Epoch 34 | Batch: 45 | Loss: 8.1338\n",
      "Epoch 34 | Batch: 46 | Loss: 8.5017\n",
      "Epoch 34 | Batch: 47 | Loss: 5.6095\n",
      "Epoch 34 | Batch: 48 | Loss: 0.3000\n",
      "Mean 10.655032944555083\n",
      "Epoch 35 | Batch: 1 | Loss: 7.7701\n",
      "Epoch 35 | Batch: 2 | Loss: 4.7583\n",
      "Epoch 35 | Batch: 3 | Loss: 14.0262\n",
      "Epoch 35 | Batch: 4 | Loss: 2.5087\n",
      "Epoch 35 | Batch: 5 | Loss: 12.5578\n",
      "Epoch 35 | Batch: 6 | Loss: 5.6824\n",
      "Epoch 35 | Batch: 7 | Loss: 4.5122\n",
      "Epoch 35 | Batch: 8 | Loss: 7.4111\n",
      "Epoch 35 | Batch: 9 | Loss: 7.9350\n",
      "Epoch 35 | Batch: 10 | Loss: 12.9955\n",
      "Epoch 35 | Batch: 11 | Loss: 13.3103\n",
      "Epoch 35 | Batch: 12 | Loss: 5.9102\n",
      "Epoch 35 | Batch: 13 | Loss: 6.0234\n",
      "Epoch 35 | Batch: 14 | Loss: 4.4413\n",
      "Epoch 35 | Batch: 15 | Loss: 8.3557\n",
      "Epoch 35 | Batch: 16 | Loss: 13.8930\n",
      "Epoch 35 | Batch: 17 | Loss: 10.1900\n",
      "Epoch 35 | Batch: 18 | Loss: 10.0807\n",
      "Epoch 35 | Batch: 19 | Loss: 2.3573\n",
      "Epoch 35 | Batch: 20 | Loss: 8.0236\n",
      "Epoch 35 | Batch: 21 | Loss: 4.7506\n",
      "Epoch 35 | Batch: 22 | Loss: 16.7299\n",
      "Epoch 35 | Batch: 23 | Loss: 7.9667\n",
      "Epoch 35 | Batch: 24 | Loss: 5.0956\n",
      "Epoch 35 | Batch: 25 | Loss: 10.1504\n",
      "Epoch 35 | Batch: 26 | Loss: 9.7529\n",
      "Epoch 35 | Batch: 27 | Loss: 6.2709\n",
      "Epoch 35 | Batch: 28 | Loss: 10.8318\n",
      "Epoch 35 | Batch: 29 | Loss: 6.8070\n",
      "Epoch 35 | Batch: 30 | Loss: 12.5879\n",
      "Epoch 35 | Batch: 31 | Loss: 21.1049\n",
      "Epoch 35 | Batch: 32 | Loss: 5.8330\n",
      "Epoch 35 | Batch: 33 | Loss: 9.1739\n",
      "Epoch 35 | Batch: 34 | Loss: 8.3101\n",
      "Epoch 35 | Batch: 35 | Loss: 13.9858\n",
      "Epoch 35 | Batch: 36 | Loss: 5.9050\n",
      "Epoch 35 | Batch: 37 | Loss: 7.9326\n",
      "Epoch 35 | Batch: 38 | Loss: 7.3817\n",
      "Epoch 35 | Batch: 39 | Loss: 8.9074\n",
      "Epoch 35 | Batch: 40 | Loss: 8.2062\n",
      "Epoch 35 | Batch: 41 | Loss: 12.3182\n",
      "Epoch 35 | Batch: 42 | Loss: 25.2250\n",
      "Epoch 35 | Batch: 43 | Loss: 39.6538\n",
      "Epoch 35 | Batch: 44 | Loss: 9.9789\n",
      "Epoch 35 | Batch: 45 | Loss: 9.2923\n",
      "Epoch 35 | Batch: 46 | Loss: 8.5972\n",
      "Epoch 35 | Batch: 47 | Loss: 10.8877\n",
      "Epoch 35 | Batch: 48 | Loss: 6.1177\n",
      "Mean 9.843707909186682\n",
      "Epoch 36 | Batch: 1 | Loss: 8.6911\n",
      "Epoch 36 | Batch: 2 | Loss: 9.2424\n",
      "Epoch 36 | Batch: 3 | Loss: 7.6672\n",
      "Epoch 36 | Batch: 4 | Loss: 6.7761\n",
      "Epoch 36 | Batch: 5 | Loss: 9.5468\n",
      "Epoch 36 | Batch: 6 | Loss: 27.3728\n",
      "Epoch 36 | Batch: 7 | Loss: 21.8580\n",
      "Epoch 36 | Batch: 8 | Loss: 6.8573\n",
      "Epoch 36 | Batch: 9 | Loss: 8.5261\n",
      "Epoch 36 | Batch: 10 | Loss: 14.3214\n",
      "Epoch 36 | Batch: 11 | Loss: 7.8663\n",
      "Epoch 36 | Batch: 12 | Loss: 9.0880\n",
      "Epoch 36 | Batch: 13 | Loss: 4.6778\n",
      "Epoch 36 | Batch: 14 | Loss: 9.0450\n",
      "Epoch 36 | Batch: 15 | Loss: 8.6990\n",
      "Epoch 36 | Batch: 16 | Loss: 10.1130\n",
      "Epoch 36 | Batch: 17 | Loss: 7.6030\n",
      "Epoch 36 | Batch: 18 | Loss: 8.2757\n",
      "Epoch 36 | Batch: 19 | Loss: 4.5098\n",
      "Epoch 36 | Batch: 20 | Loss: 16.8556\n",
      "Epoch 36 | Batch: 21 | Loss: 24.0921\n",
      "Epoch 36 | Batch: 22 | Loss: 10.3739\n",
      "Epoch 36 | Batch: 23 | Loss: 7.9956\n",
      "Epoch 36 | Batch: 24 | Loss: 9.9257\n",
      "Epoch 36 | Batch: 25 | Loss: 12.6665\n",
      "Epoch 36 | Batch: 26 | Loss: 9.7690\n",
      "Epoch 36 | Batch: 27 | Loss: 10.4185\n",
      "Epoch 36 | Batch: 28 | Loss: 7.3145\n",
      "Epoch 36 | Batch: 29 | Loss: 14.6523\n",
      "Epoch 36 | Batch: 30 | Loss: 7.8861\n",
      "Epoch 36 | Batch: 31 | Loss: 8.1479\n",
      "Epoch 36 | Batch: 32 | Loss: 11.4370\n",
      "Epoch 36 | Batch: 33 | Loss: 4.4020\n",
      "Epoch 36 | Batch: 34 | Loss: 9.4758\n",
      "Epoch 36 | Batch: 35 | Loss: 10.1659\n",
      "Epoch 36 | Batch: 36 | Loss: 5.4192\n",
      "Epoch 36 | Batch: 37 | Loss: 5.2888\n",
      "Epoch 36 | Batch: 38 | Loss: 13.0684\n",
      "Epoch 36 | Batch: 39 | Loss: 6.5946\n",
      "Epoch 36 | Batch: 40 | Loss: 13.0119\n",
      "Epoch 36 | Batch: 41 | Loss: 5.3861\n",
      "Epoch 36 | Batch: 42 | Loss: 13.7075\n",
      "Epoch 36 | Batch: 43 | Loss: 20.3303\n",
      "Epoch 36 | Batch: 44 | Loss: 13.2168\n",
      "Epoch 36 | Batch: 45 | Loss: 29.4448\n",
      "Epoch 36 | Batch: 46 | Loss: 11.0092\n",
      "Epoch 36 | Batch: 47 | Loss: 25.2822\n",
      "Epoch 36 | Batch: 48 | Loss: 6.0128\n",
      "Mean 11.126829653978348\n",
      "Epoch 37 | Batch: 1 | Loss: 8.5828\n",
      "Epoch 37 | Batch: 2 | Loss: 8.1559\n",
      "Epoch 37 | Batch: 3 | Loss: 6.0384\n",
      "Epoch 37 | Batch: 4 | Loss: 9.1430\n",
      "Epoch 37 | Batch: 5 | Loss: 6.7886\n",
      "Epoch 37 | Batch: 6 | Loss: 7.3299\n",
      "Epoch 37 | Batch: 7 | Loss: 11.3031\n",
      "Epoch 37 | Batch: 8 | Loss: 12.8560\n",
      "Epoch 37 | Batch: 9 | Loss: 11.0888\n",
      "Epoch 37 | Batch: 10 | Loss: 7.4099\n",
      "Epoch 37 | Batch: 11 | Loss: 8.8793\n",
      "Epoch 37 | Batch: 12 | Loss: 17.5693\n",
      "Epoch 37 | Batch: 13 | Loss: 12.0977\n",
      "Epoch 37 | Batch: 14 | Loss: 13.9013\n",
      "Epoch 37 | Batch: 15 | Loss: 6.5326\n",
      "Epoch 37 | Batch: 16 | Loss: 7.8327\n",
      "Epoch 37 | Batch: 17 | Loss: 6.8222\n",
      "Epoch 37 | Batch: 18 | Loss: 8.1820\n",
      "Epoch 37 | Batch: 19 | Loss: 5.6710\n",
      "Epoch 37 | Batch: 20 | Loss: 9.0641\n",
      "Epoch 37 | Batch: 21 | Loss: 12.2774\n",
      "Epoch 37 | Batch: 22 | Loss: 9.4049\n",
      "Epoch 37 | Batch: 23 | Loss: 9.9615\n",
      "Epoch 37 | Batch: 24 | Loss: 11.5341\n",
      "Epoch 37 | Batch: 25 | Loss: 8.3282\n",
      "Epoch 37 | Batch: 26 | Loss: 18.8455\n",
      "Epoch 37 | Batch: 27 | Loss: 14.6819\n",
      "Epoch 37 | Batch: 28 | Loss: 8.0799\n",
      "Epoch 37 | Batch: 29 | Loss: 8.4514\n",
      "Epoch 37 | Batch: 30 | Loss: 6.3714\n",
      "Epoch 37 | Batch: 31 | Loss: 7.2012\n",
      "Epoch 37 | Batch: 32 | Loss: 8.5715\n",
      "Epoch 37 | Batch: 33 | Loss: 6.5404\n",
      "Epoch 37 | Batch: 34 | Loss: 6.0187\n",
      "Epoch 37 | Batch: 35 | Loss: 11.3389\n",
      "Epoch 37 | Batch: 36 | Loss: 8.9470\n",
      "Epoch 37 | Batch: 37 | Loss: 12.1256\n",
      "Epoch 37 | Batch: 38 | Loss: 13.6690\n",
      "Epoch 37 | Batch: 39 | Loss: 15.8113\n",
      "Epoch 37 | Batch: 40 | Loss: 12.2575\n",
      "Epoch 37 | Batch: 41 | Loss: 18.2249\n",
      "Epoch 37 | Batch: 42 | Loss: 9.1233\n",
      "Epoch 37 | Batch: 43 | Loss: 15.4873\n",
      "Epoch 37 | Batch: 44 | Loss: 10.3531\n",
      "Epoch 37 | Batch: 45 | Loss: 5.8459\n",
      "Epoch 37 | Batch: 46 | Loss: 6.9625\n",
      "Epoch 37 | Batch: 47 | Loss: 9.5898\n",
      "Epoch 37 | Batch: 48 | Loss: 5.2131\n",
      "Mean 9.926370054483414\n",
      "Epoch 38 | Batch: 1 | Loss: 7.5283\n",
      "Epoch 38 | Batch: 2 | Loss: 17.7712\n",
      "Epoch 38 | Batch: 3 | Loss: 11.4836\n",
      "Epoch 38 | Batch: 4 | Loss: 12.8739\n",
      "Epoch 38 | Batch: 5 | Loss: 10.8890\n",
      "Epoch 38 | Batch: 6 | Loss: 14.4687\n",
      "Epoch 38 | Batch: 7 | Loss: 8.2051\n",
      "Epoch 38 | Batch: 8 | Loss: 9.2201\n",
      "Epoch 38 | Batch: 9 | Loss: 9.4047\n",
      "Epoch 38 | Batch: 10 | Loss: 9.1378\n",
      "Epoch 38 | Batch: 11 | Loss: 8.9316\n",
      "Epoch 38 | Batch: 12 | Loss: 4.6211\n",
      "Epoch 38 | Batch: 13 | Loss: 5.5705\n",
      "Epoch 38 | Batch: 14 | Loss: 9.7995\n",
      "Epoch 38 | Batch: 15 | Loss: 6.9735\n",
      "Epoch 38 | Batch: 16 | Loss: 6.1965\n",
      "Epoch 38 | Batch: 17 | Loss: 5.8421\n",
      "Epoch 38 | Batch: 18 | Loss: 8.7825\n",
      "Epoch 38 | Batch: 19 | Loss: 5.7355\n",
      "Epoch 38 | Batch: 20 | Loss: 14.9833\n",
      "Epoch 38 | Batch: 21 | Loss: 18.9498\n",
      "Epoch 38 | Batch: 22 | Loss: 8.6377\n",
      "Epoch 38 | Batch: 23 | Loss: 8.6523\n",
      "Epoch 38 | Batch: 24 | Loss: 7.7927\n",
      "Epoch 38 | Batch: 25 | Loss: 9.0447\n",
      "Epoch 38 | Batch: 26 | Loss: 10.3885\n",
      "Epoch 38 | Batch: 27 | Loss: 4.2821\n",
      "Epoch 38 | Batch: 28 | Loss: 5.2931\n",
      "Epoch 38 | Batch: 29 | Loss: 8.1986\n",
      "Epoch 38 | Batch: 30 | Loss: 7.0482\n",
      "Epoch 38 | Batch: 31 | Loss: 7.1974\n",
      "Epoch 38 | Batch: 32 | Loss: 9.2140\n",
      "Epoch 38 | Batch: 33 | Loss: 6.3324\n",
      "Epoch 38 | Batch: 34 | Loss: 13.6801\n",
      "Epoch 38 | Batch: 35 | Loss: 19.5989\n",
      "Epoch 38 | Batch: 36 | Loss: 7.8953\n",
      "Epoch 38 | Batch: 37 | Loss: 7.5374\n",
      "Epoch 38 | Batch: 38 | Loss: 8.8870\n",
      "Epoch 38 | Batch: 39 | Loss: 9.6495\n",
      "Epoch 38 | Batch: 40 | Loss: 15.9905\n",
      "Epoch 38 | Batch: 41 | Loss: 10.7948\n",
      "Epoch 38 | Batch: 42 | Loss: 13.5127\n",
      "Epoch 38 | Batch: 43 | Loss: 7.9659\n",
      "Epoch 38 | Batch: 44 | Loss: 8.6165\n",
      "Epoch 38 | Batch: 45 | Loss: 12.2324\n",
      "Epoch 38 | Batch: 46 | Loss: 12.7379\n",
      "Epoch 38 | Batch: 47 | Loss: 6.2843\n",
      "Epoch 38 | Batch: 48 | Loss: 6.1025\n",
      "Mean 9.602830737829208\n",
      "Epoch 39 | Batch: 1 | Loss: 12.7659\n",
      "Epoch 39 | Batch: 2 | Loss: 12.1937\n",
      "Epoch 39 | Batch: 3 | Loss: 14.9255\n",
      "Epoch 39 | Batch: 4 | Loss: 19.9888\n",
      "Epoch 39 | Batch: 5 | Loss: 6.6260\n",
      "Epoch 39 | Batch: 6 | Loss: 5.6652\n",
      "Epoch 39 | Batch: 7 | Loss: 13.1856\n",
      "Epoch 39 | Batch: 8 | Loss: 5.2940\n",
      "Epoch 39 | Batch: 9 | Loss: 7.2197\n",
      "Epoch 39 | Batch: 10 | Loss: 10.1767\n",
      "Epoch 39 | Batch: 11 | Loss: 7.6125\n",
      "Epoch 39 | Batch: 12 | Loss: 28.5080\n",
      "Epoch 39 | Batch: 13 | Loss: 27.6518\n",
      "Epoch 39 | Batch: 14 | Loss: 9.0951\n",
      "Epoch 39 | Batch: 15 | Loss: 4.8866\n",
      "Epoch 39 | Batch: 16 | Loss: 16.0153\n",
      "Epoch 39 | Batch: 17 | Loss: 7.8997\n",
      "Epoch 39 | Batch: 18 | Loss: 7.6227\n",
      "Epoch 39 | Batch: 19 | Loss: 8.7036\n",
      "Epoch 39 | Batch: 20 | Loss: 10.6473\n",
      "Epoch 39 | Batch: 21 | Loss: 11.0228\n",
      "Epoch 39 | Batch: 22 | Loss: 16.6003\n",
      "Epoch 39 | Batch: 23 | Loss: 15.2982\n",
      "Epoch 39 | Batch: 24 | Loss: 6.2940\n",
      "Epoch 39 | Batch: 25 | Loss: 14.3272\n",
      "Epoch 39 | Batch: 26 | Loss: 8.0432\n",
      "Epoch 39 | Batch: 27 | Loss: 18.0209\n",
      "Epoch 39 | Batch: 28 | Loss: 10.9697\n",
      "Epoch 39 | Batch: 29 | Loss: 9.7806\n",
      "Epoch 39 | Batch: 30 | Loss: 10.5715\n",
      "Epoch 39 | Batch: 31 | Loss: 13.0661\n",
      "Epoch 39 | Batch: 32 | Loss: 8.1098\n",
      "Epoch 39 | Batch: 33 | Loss: 5.8661\n",
      "Epoch 39 | Batch: 34 | Loss: 8.2027\n",
      "Epoch 39 | Batch: 35 | Loss: 10.4623\n",
      "Epoch 39 | Batch: 36 | Loss: 3.4625\n",
      "Epoch 39 | Batch: 37 | Loss: 6.6011\n",
      "Epoch 39 | Batch: 38 | Loss: 8.1742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 | Batch: 39 | Loss: 13.3995\n",
      "Epoch 39 | Batch: 40 | Loss: 12.3020\n",
      "Epoch 39 | Batch: 41 | Loss: 17.8628\n",
      "Epoch 39 | Batch: 42 | Loss: 25.5575\n",
      "Epoch 39 | Batch: 43 | Loss: 17.0714\n",
      "Epoch 39 | Batch: 44 | Loss: 9.9069\n",
      "Epoch 39 | Batch: 45 | Loss: 4.6893\n",
      "Epoch 39 | Batch: 46 | Loss: 18.5651\n",
      "Epoch 39 | Batch: 47 | Loss: 18.9109\n",
      "Epoch 39 | Batch: 48 | Loss: 4.3363\n",
      "Mean 11.753304397066435\n",
      "Epoch 40 | Batch: 1 | Loss: 9.7645\n",
      "Epoch 40 | Batch: 2 | Loss: 13.2611\n",
      "Epoch 40 | Batch: 3 | Loss: 12.1799\n",
      "Epoch 40 | Batch: 4 | Loss: 23.1045\n",
      "Epoch 40 | Batch: 5 | Loss: 19.7750\n",
      "Epoch 40 | Batch: 6 | Loss: 13.7855\n",
      "Epoch 40 | Batch: 7 | Loss: 6.2343\n",
      "Epoch 40 | Batch: 8 | Loss: 10.7261\n",
      "Epoch 40 | Batch: 9 | Loss: 19.3948\n",
      "Epoch 40 | Batch: 10 | Loss: 18.5563\n",
      "Epoch 40 | Batch: 11 | Loss: 21.7637\n",
      "Epoch 40 | Batch: 12 | Loss: 17.6583\n",
      "Epoch 40 | Batch: 13 | Loss: 7.4706\n",
      "Epoch 40 | Batch: 14 | Loss: 21.1376\n",
      "Epoch 40 | Batch: 15 | Loss: 20.9729\n",
      "Epoch 40 | Batch: 16 | Loss: 16.0690\n",
      "Epoch 40 | Batch: 17 | Loss: 9.1540\n",
      "Epoch 40 | Batch: 18 | Loss: 9.6670\n",
      "Epoch 40 | Batch: 19 | Loss: 5.5325\n",
      "Epoch 40 | Batch: 20 | Loss: 7.6415\n",
      "Epoch 40 | Batch: 21 | Loss: 8.9976\n",
      "Epoch 40 | Batch: 22 | Loss: 8.2328\n",
      "Epoch 40 | Batch: 23 | Loss: 16.4100\n",
      "Epoch 40 | Batch: 24 | Loss: 15.3602\n",
      "Epoch 40 | Batch: 25 | Loss: 8.2827\n",
      "Epoch 40 | Batch: 26 | Loss: 5.6419\n",
      "Epoch 40 | Batch: 27 | Loss: 10.8242\n",
      "Epoch 40 | Batch: 28 | Loss: 12.2924\n",
      "Epoch 40 | Batch: 29 | Loss: 12.8878\n",
      "Epoch 40 | Batch: 30 | Loss: 4.8449\n",
      "Epoch 40 | Batch: 31 | Loss: 5.7052\n",
      "Epoch 40 | Batch: 32 | Loss: 6.9995\n",
      "Epoch 40 | Batch: 33 | Loss: 12.9578\n",
      "Epoch 40 | Batch: 34 | Loss: 13.9714\n",
      "Epoch 40 | Batch: 35 | Loss: 14.9282\n",
      "Epoch 40 | Batch: 36 | Loss: 11.1717\n",
      "Epoch 40 | Batch: 37 | Loss: 13.5031\n",
      "Epoch 40 | Batch: 38 | Loss: 7.1163\n",
      "Epoch 40 | Batch: 39 | Loss: 6.2691\n",
      "Epoch 40 | Batch: 40 | Loss: 8.0505\n",
      "Epoch 40 | Batch: 41 | Loss: 6.0888\n",
      "Epoch 40 | Batch: 42 | Loss: 12.8251\n",
      "Epoch 40 | Batch: 43 | Loss: 19.5503\n",
      "Epoch 40 | Batch: 44 | Loss: 18.5949\n",
      "Epoch 40 | Batch: 45 | Loss: 9.1498\n",
      "Epoch 40 | Batch: 46 | Loss: 6.9279\n",
      "Epoch 40 | Batch: 47 | Loss: 7.0074\n",
      "Epoch 40 | Batch: 48 | Loss: 4.2627\n",
      "Mean 11.93131853143374\n",
      "Epoch 41 | Batch: 1 | Loss: 4.4955\n",
      "Epoch 41 | Batch: 2 | Loss: 12.1855\n",
      "Epoch 41 | Batch: 3 | Loss: 7.0835\n",
      "Epoch 41 | Batch: 4 | Loss: 3.9571\n",
      "Epoch 41 | Batch: 5 | Loss: 3.5271\n",
      "Epoch 41 | Batch: 6 | Loss: 10.5745\n",
      "Epoch 41 | Batch: 7 | Loss: 13.2598\n",
      "Epoch 41 | Batch: 8 | Loss: 5.1654\n",
      "Epoch 41 | Batch: 9 | Loss: 8.2668\n",
      "Epoch 41 | Batch: 10 | Loss: 8.5705\n",
      "Epoch 41 | Batch: 11 | Loss: 18.3461\n",
      "Epoch 41 | Batch: 12 | Loss: 10.0244\n",
      "Epoch 41 | Batch: 13 | Loss: 9.3085\n",
      "Epoch 41 | Batch: 14 | Loss: 7.0888\n",
      "Epoch 41 | Batch: 15 | Loss: 10.6680\n",
      "Epoch 41 | Batch: 16 | Loss: 7.2506\n",
      "Epoch 41 | Batch: 17 | Loss: 8.3918\n",
      "Epoch 41 | Batch: 18 | Loss: 10.8336\n",
      "Epoch 41 | Batch: 19 | Loss: 7.2577\n",
      "Epoch 41 | Batch: 20 | Loss: 6.3500\n",
      "Epoch 41 | Batch: 21 | Loss: 9.2007\n",
      "Epoch 41 | Batch: 22 | Loss: 14.2464\n",
      "Epoch 41 | Batch: 23 | Loss: 8.3899\n",
      "Epoch 41 | Batch: 24 | Loss: 8.3791\n",
      "Epoch 41 | Batch: 25 | Loss: 7.6085\n",
      "Epoch 41 | Batch: 26 | Loss: 3.8061\n",
      "Epoch 41 | Batch: 27 | Loss: 8.2959\n",
      "Epoch 41 | Batch: 28 | Loss: 7.2272\n",
      "Epoch 41 | Batch: 29 | Loss: 6.8329\n",
      "Epoch 41 | Batch: 30 | Loss: 11.5386\n",
      "Epoch 41 | Batch: 31 | Loss: 9.1022\n",
      "Epoch 41 | Batch: 32 | Loss: 15.9294\n",
      "Epoch 41 | Batch: 33 | Loss: 12.4363\n",
      "Epoch 41 | Batch: 34 | Loss: 13.7221\n",
      "Epoch 41 | Batch: 35 | Loss: 28.4159\n",
      "Epoch 41 | Batch: 36 | Loss: 6.8970\n",
      "Epoch 41 | Batch: 37 | Loss: 6.4676\n",
      "Epoch 41 | Batch: 38 | Loss: 9.1705\n",
      "Epoch 41 | Batch: 39 | Loss: 7.8969\n",
      "Epoch 41 | Batch: 40 | Loss: 4.4221\n",
      "Epoch 41 | Batch: 41 | Loss: 26.5454\n",
      "Epoch 41 | Batch: 42 | Loss: 21.1897\n",
      "Epoch 41 | Batch: 43 | Loss: 11.2361\n",
      "Epoch 41 | Batch: 44 | Loss: 15.5265\n",
      "Epoch 41 | Batch: 45 | Loss: 15.1102\n",
      "Epoch 41 | Batch: 46 | Loss: 7.3104\n",
      "Epoch 41 | Batch: 47 | Loss: 10.9189\n",
      "Epoch 41 | Batch: 48 | Loss: 1.9012\n",
      "Mean 10.048511952161789\n",
      "Epoch 42 | Batch: 1 | Loss: 9.2337\n",
      "Epoch 42 | Batch: 2 | Loss: 11.5024\n",
      "Epoch 42 | Batch: 3 | Loss: 8.7530\n",
      "Epoch 42 | Batch: 4 | Loss: 11.6479\n",
      "Epoch 42 | Batch: 5 | Loss: 7.6061\n",
      "Epoch 42 | Batch: 6 | Loss: 9.1463\n",
      "Epoch 42 | Batch: 7 | Loss: 2.6829\n",
      "Epoch 42 | Batch: 8 | Loss: 10.9863\n",
      "Epoch 42 | Batch: 9 | Loss: 8.8461\n",
      "Epoch 42 | Batch: 10 | Loss: 13.7270\n",
      "Epoch 42 | Batch: 11 | Loss: 5.7821\n",
      "Epoch 42 | Batch: 12 | Loss: 6.0046\n",
      "Epoch 42 | Batch: 13 | Loss: 6.6457\n",
      "Epoch 42 | Batch: 14 | Loss: 6.7005\n",
      "Epoch 42 | Batch: 15 | Loss: 12.1249\n",
      "Epoch 42 | Batch: 16 | Loss: 9.7625\n",
      "Epoch 42 | Batch: 17 | Loss: 14.9312\n",
      "Epoch 42 | Batch: 18 | Loss: 3.8754\n",
      "Epoch 42 | Batch: 19 | Loss: 14.9313\n",
      "Epoch 42 | Batch: 20 | Loss: 4.4419\n",
      "Epoch 42 | Batch: 21 | Loss: 9.9810\n",
      "Epoch 42 | Batch: 22 | Loss: 8.5640\n",
      "Epoch 42 | Batch: 23 | Loss: 16.1629\n",
      "Epoch 42 | Batch: 24 | Loss: 9.7100\n",
      "Epoch 42 | Batch: 25 | Loss: 8.4894\n",
      "Epoch 42 | Batch: 26 | Loss: 5.7438\n",
      "Epoch 42 | Batch: 27 | Loss: 1.5213\n",
      "Epoch 42 | Batch: 28 | Loss: 15.1921\n",
      "Epoch 42 | Batch: 29 | Loss: 8.3915\n",
      "Epoch 42 | Batch: 30 | Loss: 8.1791\n",
      "Epoch 42 | Batch: 31 | Loss: 7.3970\n",
      "Epoch 42 | Batch: 32 | Loss: 7.7855\n",
      "Epoch 42 | Batch: 33 | Loss: 7.2167\n",
      "Epoch 42 | Batch: 34 | Loss: 7.1460\n",
      "Epoch 42 | Batch: 35 | Loss: 6.7435\n",
      "Epoch 42 | Batch: 36 | Loss: 9.3922\n",
      "Epoch 42 | Batch: 37 | Loss: 13.8149\n",
      "Epoch 42 | Batch: 38 | Loss: 7.2057\n",
      "Epoch 42 | Batch: 39 | Loss: 11.1819\n",
      "Epoch 42 | Batch: 40 | Loss: 10.0136\n",
      "Epoch 42 | Batch: 41 | Loss: 9.1303\n",
      "Epoch 42 | Batch: 42 | Loss: 8.2358\n",
      "Epoch 42 | Batch: 43 | Loss: 14.1098\n",
      "Epoch 42 | Batch: 44 | Loss: 12.9953\n",
      "Epoch 42 | Batch: 45 | Loss: 6.8338\n",
      "Epoch 42 | Batch: 46 | Loss: 9.5260\n",
      "Epoch 42 | Batch: 47 | Loss: 7.7723\n",
      "Epoch 42 | Batch: 48 | Loss: 4.6939\n",
      "Mean 9.00961047410965\n",
      "Epoch 43 | Batch: 1 | Loss: 16.1553\n",
      "Epoch 43 | Batch: 2 | Loss: 9.8776\n",
      "Epoch 43 | Batch: 3 | Loss: 8.0533\n",
      "Epoch 43 | Batch: 4 | Loss: 5.7620\n",
      "Epoch 43 | Batch: 5 | Loss: 6.5619\n",
      "Epoch 43 | Batch: 6 | Loss: 10.6244\n",
      "Epoch 43 | Batch: 7 | Loss: 6.0187\n",
      "Epoch 43 | Batch: 8 | Loss: 10.1319\n",
      "Epoch 43 | Batch: 9 | Loss: 13.9593\n",
      "Epoch 43 | Batch: 10 | Loss: 11.7405\n",
      "Epoch 43 | Batch: 11 | Loss: 10.1838\n",
      "Epoch 43 | Batch: 12 | Loss: 11.4757\n",
      "Epoch 43 | Batch: 13 | Loss: 7.6975\n",
      "Epoch 43 | Batch: 14 | Loss: 4.6841\n",
      "Epoch 43 | Batch: 15 | Loss: 10.6276\n",
      "Epoch 43 | Batch: 16 | Loss: 9.6069\n",
      "Epoch 43 | Batch: 17 | Loss: 11.0785\n",
      "Epoch 43 | Batch: 18 | Loss: 7.6851\n",
      "Epoch 43 | Batch: 19 | Loss: 7.9083\n",
      "Epoch 43 | Batch: 20 | Loss: 10.0454\n",
      "Epoch 43 | Batch: 21 | Loss: 15.5181\n",
      "Epoch 43 | Batch: 22 | Loss: 5.3431\n",
      "Epoch 43 | Batch: 23 | Loss: 13.3466\n",
      "Epoch 43 | Batch: 24 | Loss: 7.5952\n",
      "Epoch 43 | Batch: 25 | Loss: 2.5087\n",
      "Epoch 43 | Batch: 26 | Loss: 7.2013\n",
      "Epoch 43 | Batch: 27 | Loss: 7.8110\n",
      "Epoch 43 | Batch: 28 | Loss: 5.1479\n",
      "Epoch 43 | Batch: 29 | Loss: 7.5942\n",
      "Epoch 43 | Batch: 30 | Loss: 14.0613\n",
      "Epoch 43 | Batch: 31 | Loss: 15.2502\n",
      "Epoch 43 | Batch: 32 | Loss: 10.4654\n",
      "Epoch 43 | Batch: 33 | Loss: 2.1807\n",
      "Epoch 43 | Batch: 34 | Loss: 11.0588\n",
      "Epoch 43 | Batch: 35 | Loss: 9.9580\n",
      "Epoch 43 | Batch: 36 | Loss: 14.1687\n",
      "Epoch 43 | Batch: 37 | Loss: 13.7511\n",
      "Epoch 43 | Batch: 38 | Loss: 7.2210\n",
      "Epoch 43 | Batch: 39 | Loss: 16.2392\n",
      "Epoch 43 | Batch: 40 | Loss: 12.2012\n",
      "Epoch 43 | Batch: 41 | Loss: 8.6390\n",
      "Epoch 43 | Batch: 42 | Loss: 8.4240\n",
      "Epoch 43 | Batch: 43 | Loss: 18.7796\n",
      "Epoch 43 | Batch: 44 | Loss: 14.1549\n",
      "Epoch 43 | Batch: 45 | Loss: 5.1150\n",
      "Epoch 43 | Batch: 46 | Loss: 10.3083\n",
      "Epoch 43 | Batch: 47 | Loss: 10.3455\n",
      "Epoch 43 | Batch: 48 | Loss: 2.7237\n",
      "Mean 9.728953848282496\n",
      "Epoch 44 | Batch: 1 | Loss: 9.0935\n",
      "Epoch 44 | Batch: 2 | Loss: 5.8069\n",
      "Epoch 44 | Batch: 3 | Loss: 13.5852\n",
      "Epoch 44 | Batch: 4 | Loss: 12.9713\n",
      "Epoch 44 | Batch: 5 | Loss: 11.6630\n",
      "Epoch 44 | Batch: 6 | Loss: 7.5047\n",
      "Epoch 44 | Batch: 7 | Loss: 10.3514\n",
      "Epoch 44 | Batch: 8 | Loss: 9.7645\n",
      "Epoch 44 | Batch: 9 | Loss: 5.9462\n",
      "Epoch 44 | Batch: 10 | Loss: 14.5882\n",
      "Epoch 44 | Batch: 11 | Loss: 10.6822\n",
      "Epoch 44 | Batch: 12 | Loss: 11.8096\n",
      "Epoch 44 | Batch: 13 | Loss: 38.4101\n",
      "Epoch 44 | Batch: 14 | Loss: 29.0473\n",
      "Epoch 44 | Batch: 15 | Loss: 20.1119\n",
      "Epoch 44 | Batch: 16 | Loss: 24.8919\n",
      "Epoch 44 | Batch: 17 | Loss: 21.9056\n",
      "Epoch 44 | Batch: 18 | Loss: 13.8405\n",
      "Epoch 44 | Batch: 19 | Loss: 8.7465\n",
      "Epoch 44 | Batch: 20 | Loss: 12.9105\n",
      "Epoch 44 | Batch: 21 | Loss: 9.5545\n",
      "Epoch 44 | Batch: 22 | Loss: 10.9643\n",
      "Epoch 44 | Batch: 23 | Loss: 8.3308\n",
      "Epoch 44 | Batch: 24 | Loss: 6.4855\n",
      "Epoch 44 | Batch: 25 | Loss: 9.6468\n",
      "Epoch 44 | Batch: 26 | Loss: 12.3784\n",
      "Epoch 44 | Batch: 27 | Loss: 15.4343\n",
      "Epoch 44 | Batch: 28 | Loss: 14.3732\n",
      "Epoch 44 | Batch: 29 | Loss: 3.3350\n",
      "Epoch 44 | Batch: 30 | Loss: 5.3842\n",
      "Epoch 44 | Batch: 31 | Loss: 8.6618\n",
      "Epoch 44 | Batch: 32 | Loss: 11.2554\n",
      "Epoch 44 | Batch: 33 | Loss: 9.1332\n",
      "Epoch 44 | Batch: 34 | Loss: 13.1144\n",
      "Epoch 44 | Batch: 35 | Loss: 14.7213\n",
      "Epoch 44 | Batch: 36 | Loss: 25.4216\n",
      "Epoch 44 | Batch: 37 | Loss: 7.8346\n",
      "Epoch 44 | Batch: 38 | Loss: 9.1891\n",
      "Epoch 44 | Batch: 39 | Loss: 6.6442\n",
      "Epoch 44 | Batch: 40 | Loss: 8.8235\n",
      "Epoch 44 | Batch: 41 | Loss: 5.3317\n",
      "Epoch 44 | Batch: 42 | Loss: 8.1599\n",
      "Epoch 44 | Batch: 43 | Loss: 8.9055\n",
      "Epoch 44 | Batch: 44 | Loss: 4.1222\n",
      "Epoch 44 | Batch: 45 | Loss: 3.9626\n",
      "Epoch 44 | Batch: 46 | Loss: 7.9431\n",
      "Epoch 44 | Batch: 47 | Loss: 17.1435\n",
      "Epoch 44 | Batch: 48 | Loss: 7.0894\n",
      "Mean 11.811972990632057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 | Batch: 1 | Loss: 7.8436\n",
      "Epoch 45 | Batch: 2 | Loss: 8.0420\n",
      "Epoch 45 | Batch: 3 | Loss: 10.4310\n",
      "Epoch 45 | Batch: 4 | Loss: 19.8801\n",
      "Epoch 45 | Batch: 5 | Loss: 13.6208\n",
      "Epoch 45 | Batch: 6 | Loss: 11.1402\n",
      "Epoch 45 | Batch: 7 | Loss: 5.6295\n",
      "Epoch 45 | Batch: 8 | Loss: 9.4047\n",
      "Epoch 45 | Batch: 9 | Loss: 10.7241\n",
      "Epoch 45 | Batch: 10 | Loss: 11.5930\n",
      "Epoch 45 | Batch: 11 | Loss: 8.0403\n",
      "Epoch 45 | Batch: 12 | Loss: 5.5966\n",
      "Epoch 45 | Batch: 13 | Loss: 10.0031\n",
      "Epoch 45 | Batch: 14 | Loss: 14.9459\n",
      "Epoch 45 | Batch: 15 | Loss: 23.9697\n",
      "Epoch 45 | Batch: 16 | Loss: 5.0484\n",
      "Epoch 45 | Batch: 17 | Loss: 7.9536\n",
      "Epoch 45 | Batch: 18 | Loss: 9.7159\n",
      "Epoch 45 | Batch: 19 | Loss: 5.3022\n",
      "Epoch 45 | Batch: 20 | Loss: 6.5522\n",
      "Epoch 45 | Batch: 21 | Loss: 7.7350\n",
      "Epoch 45 | Batch: 22 | Loss: 3.4899\n",
      "Epoch 45 | Batch: 23 | Loss: 13.9888\n",
      "Epoch 45 | Batch: 24 | Loss: 15.8981\n",
      "Epoch 45 | Batch: 25 | Loss: 8.7852\n",
      "Epoch 45 | Batch: 26 | Loss: 5.8415\n",
      "Epoch 45 | Batch: 27 | Loss: 9.8322\n",
      "Epoch 45 | Batch: 28 | Loss: 13.0386\n",
      "Epoch 45 | Batch: 29 | Loss: 13.0662\n",
      "Epoch 45 | Batch: 30 | Loss: 8.8372\n",
      "Epoch 45 | Batch: 31 | Loss: 11.4054\n",
      "Epoch 45 | Batch: 32 | Loss: 4.8756\n",
      "Epoch 45 | Batch: 33 | Loss: 12.0357\n",
      "Epoch 45 | Batch: 34 | Loss: 8.2350\n",
      "Epoch 45 | Batch: 35 | Loss: 6.3081\n",
      "Epoch 45 | Batch: 36 | Loss: 8.6475\n",
      "Epoch 45 | Batch: 37 | Loss: 8.2372\n",
      "Epoch 45 | Batch: 38 | Loss: 6.9998\n",
      "Epoch 45 | Batch: 39 | Loss: 8.9431\n",
      "Epoch 45 | Batch: 40 | Loss: 12.6972\n",
      "Epoch 45 | Batch: 41 | Loss: 9.9252\n",
      "Epoch 45 | Batch: 42 | Loss: 10.4139\n",
      "Epoch 45 | Batch: 43 | Loss: 14.5225\n",
      "Epoch 45 | Batch: 44 | Loss: 14.4335\n",
      "Epoch 45 | Batch: 45 | Loss: 10.9726\n",
      "Epoch 45 | Batch: 46 | Loss: 8.5233\n",
      "Epoch 45 | Batch: 47 | Loss: 15.5799\n",
      "Epoch 45 | Batch: 48 | Loss: 5.4098\n",
      "Mean 10.085727746287981\n",
      "Epoch 46 | Batch: 1 | Loss: 7.2957\n",
      "Epoch 46 | Batch: 2 | Loss: 9.7309\n",
      "Epoch 46 | Batch: 3 | Loss: 24.7793\n",
      "Epoch 46 | Batch: 4 | Loss: 17.3960\n",
      "Epoch 46 | Batch: 5 | Loss: 11.3135\n",
      "Epoch 46 | Batch: 6 | Loss: 9.5007\n",
      "Epoch 46 | Batch: 7 | Loss: 8.8259\n",
      "Epoch 46 | Batch: 8 | Loss: 6.4057\n",
      "Epoch 46 | Batch: 9 | Loss: 9.0709\n",
      "Epoch 46 | Batch: 10 | Loss: 7.5345\n",
      "Epoch 46 | Batch: 11 | Loss: 4.5911\n",
      "Epoch 46 | Batch: 12 | Loss: 7.1056\n",
      "Epoch 46 | Batch: 13 | Loss: 7.2738\n",
      "Epoch 46 | Batch: 14 | Loss: 8.2217\n",
      "Epoch 46 | Batch: 15 | Loss: 6.9181\n",
      "Epoch 46 | Batch: 16 | Loss: 5.9052\n",
      "Epoch 46 | Batch: 17 | Loss: 8.6815\n",
      "Epoch 46 | Batch: 18 | Loss: 13.7124\n",
      "Epoch 46 | Batch: 19 | Loss: 8.5550\n",
      "Epoch 46 | Batch: 20 | Loss: 8.1163\n",
      "Epoch 46 | Batch: 21 | Loss: 8.5303\n",
      "Epoch 46 | Batch: 22 | Loss: 9.1887\n",
      "Epoch 46 | Batch: 23 | Loss: 15.2059\n",
      "Epoch 46 | Batch: 24 | Loss: 9.4342\n",
      "Epoch 46 | Batch: 25 | Loss: 12.7184\n",
      "Epoch 46 | Batch: 26 | Loss: 13.2939\n",
      "Epoch 46 | Batch: 27 | Loss: 14.1988\n",
      "Epoch 46 | Batch: 28 | Loss: 14.9468\n",
      "Epoch 46 | Batch: 29 | Loss: 7.8691\n",
      "Epoch 46 | Batch: 30 | Loss: 8.4617\n",
      "Epoch 46 | Batch: 31 | Loss: 9.2618\n",
      "Epoch 46 | Batch: 32 | Loss: 7.9033\n",
      "Epoch 46 | Batch: 33 | Loss: 11.5657\n",
      "Epoch 46 | Batch: 34 | Loss: 15.3536\n",
      "Epoch 46 | Batch: 35 | Loss: 9.2501\n",
      "Epoch 46 | Batch: 36 | Loss: 5.3555\n",
      "Epoch 46 | Batch: 37 | Loss: 10.2001\n",
      "Epoch 46 | Batch: 38 | Loss: 8.5873\n",
      "Epoch 46 | Batch: 39 | Loss: 10.8227\n",
      "Epoch 46 | Batch: 40 | Loss: 12.9137\n",
      "Epoch 46 | Batch: 41 | Loss: 7.4015\n",
      "Epoch 46 | Batch: 42 | Loss: 4.4855\n",
      "Epoch 46 | Batch: 43 | Loss: 9.7069\n",
      "Epoch 46 | Batch: 44 | Loss: 4.3755\n",
      "Epoch 46 | Batch: 45 | Loss: 9.2658\n",
      "Epoch 46 | Batch: 46 | Loss: 3.8860\n",
      "Epoch 46 | Batch: 47 | Loss: 5.1460\n",
      "Epoch 46 | Batch: 48 | Loss: 3.0287\n",
      "Mean 9.44356239338716\n",
      "Epoch 47 | Batch: 1 | Loss: 7.1515\n",
      "Epoch 47 | Batch: 2 | Loss: 17.3672\n",
      "Epoch 47 | Batch: 3 | Loss: 19.2177\n",
      "Epoch 47 | Batch: 4 | Loss: 10.6800\n",
      "Epoch 47 | Batch: 5 | Loss: 19.3850\n",
      "Epoch 47 | Batch: 6 | Loss: 12.1710\n",
      "Epoch 47 | Batch: 7 | Loss: 4.7617\n",
      "Epoch 47 | Batch: 8 | Loss: 13.4454\n",
      "Epoch 47 | Batch: 9 | Loss: 16.6169\n",
      "Epoch 47 | Batch: 10 | Loss: 10.6296\n",
      "Epoch 47 | Batch: 11 | Loss: 4.4111\n",
      "Epoch 47 | Batch: 12 | Loss: 9.4718\n",
      "Epoch 47 | Batch: 13 | Loss: 11.4675\n",
      "Epoch 47 | Batch: 14 | Loss: 10.6189\n",
      "Epoch 47 | Batch: 15 | Loss: 6.2118\n",
      "Epoch 47 | Batch: 16 | Loss: 8.4039\n",
      "Epoch 47 | Batch: 17 | Loss: 9.7163\n",
      "Epoch 47 | Batch: 18 | Loss: 13.1431\n",
      "Epoch 47 | Batch: 19 | Loss: 4.0619\n",
      "Epoch 47 | Batch: 20 | Loss: 8.6840\n",
      "Epoch 47 | Batch: 21 | Loss: 8.2808\n",
      "Epoch 47 | Batch: 22 | Loss: 7.6843\n",
      "Epoch 47 | Batch: 23 | Loss: 5.3159\n",
      "Epoch 47 | Batch: 24 | Loss: 4.9221\n",
      "Epoch 47 | Batch: 25 | Loss: 11.6860\n",
      "Epoch 47 | Batch: 26 | Loss: 27.1328\n",
      "Epoch 47 | Batch: 27 | Loss: 37.9336\n",
      "Epoch 47 | Batch: 28 | Loss: 7.7094\n",
      "Epoch 47 | Batch: 29 | Loss: 8.5411\n",
      "Epoch 47 | Batch: 30 | Loss: 15.4813\n",
      "Epoch 47 | Batch: 31 | Loss: 15.6250\n",
      "Epoch 47 | Batch: 32 | Loss: 15.4343\n",
      "Epoch 47 | Batch: 33 | Loss: 5.2983\n",
      "Epoch 47 | Batch: 34 | Loss: 9.4711\n",
      "Epoch 47 | Batch: 35 | Loss: 10.7334\n",
      "Epoch 47 | Batch: 36 | Loss: 8.4736\n",
      "Epoch 47 | Batch: 37 | Loss: 8.7568\n",
      "Epoch 47 | Batch: 38 | Loss: 19.7328\n",
      "Epoch 47 | Batch: 39 | Loss: 15.5261\n",
      "Epoch 47 | Batch: 40 | Loss: 9.9636\n",
      "Epoch 47 | Batch: 41 | Loss: 8.4810\n",
      "Epoch 47 | Batch: 42 | Loss: 7.2610\n",
      "Epoch 47 | Batch: 43 | Loss: 6.8379\n",
      "Epoch 47 | Batch: 44 | Loss: 9.3980\n",
      "Epoch 47 | Batch: 45 | Loss: 9.8482\n",
      "Epoch 47 | Batch: 46 | Loss: 9.9077\n",
      "Epoch 47 | Batch: 47 | Loss: 9.4959\n",
      "Epoch 47 | Batch: 48 | Loss: 2.9280\n",
      "Mean 11.155752470095953\n",
      "Epoch 48 | Batch: 1 | Loss: 17.3162\n",
      "Epoch 48 | Batch: 2 | Loss: 15.8862\n",
      "Epoch 48 | Batch: 3 | Loss: 14.7073\n",
      "Epoch 48 | Batch: 4 | Loss: 10.9847\n",
      "Epoch 48 | Batch: 5 | Loss: 9.8108\n",
      "Epoch 48 | Batch: 6 | Loss: 8.1939\n",
      "Epoch 48 | Batch: 7 | Loss: 3.9078\n",
      "Epoch 48 | Batch: 8 | Loss: 4.5188\n",
      "Epoch 48 | Batch: 9 | Loss: 7.4501\n",
      "Epoch 48 | Batch: 10 | Loss: 4.1501\n",
      "Epoch 48 | Batch: 11 | Loss: 7.2212\n",
      "Epoch 48 | Batch: 12 | Loss: 5.5298\n",
      "Epoch 48 | Batch: 13 | Loss: 7.6020\n",
      "Epoch 48 | Batch: 14 | Loss: 8.9047\n",
      "Epoch 48 | Batch: 15 | Loss: 8.8401\n",
      "Epoch 48 | Batch: 16 | Loss: 10.9434\n",
      "Epoch 48 | Batch: 17 | Loss: 6.4294\n",
      "Epoch 48 | Batch: 18 | Loss: 11.9461\n",
      "Epoch 48 | Batch: 19 | Loss: 6.2312\n",
      "Epoch 48 | Batch: 20 | Loss: 3.1631\n",
      "Epoch 48 | Batch: 21 | Loss: 4.5641\n",
      "Epoch 48 | Batch: 22 | Loss: 5.1951\n",
      "Epoch 48 | Batch: 23 | Loss: 8.9102\n",
      "Epoch 48 | Batch: 24 | Loss: 3.9463\n",
      "Epoch 48 | Batch: 25 | Loss: 11.1144\n",
      "Epoch 48 | Batch: 26 | Loss: 7.3724\n",
      "Epoch 48 | Batch: 27 | Loss: 11.1800\n",
      "Epoch 48 | Batch: 28 | Loss: 10.5076\n",
      "Epoch 48 | Batch: 29 | Loss: 6.3109\n",
      "Epoch 48 | Batch: 30 | Loss: 9.6024\n",
      "Epoch 48 | Batch: 31 | Loss: 14.3741\n",
      "Epoch 48 | Batch: 32 | Loss: 7.5818\n",
      "Epoch 48 | Batch: 33 | Loss: 7.0527\n",
      "Epoch 48 | Batch: 34 | Loss: 5.4554\n",
      "Epoch 48 | Batch: 35 | Loss: 4.3945\n",
      "Epoch 48 | Batch: 36 | Loss: 6.5275\n",
      "Epoch 48 | Batch: 37 | Loss: 14.9418\n",
      "Epoch 48 | Batch: 38 | Loss: 14.8286\n",
      "Epoch 48 | Batch: 39 | Loss: 10.1879\n",
      "Epoch 48 | Batch: 40 | Loss: 13.9818\n",
      "Epoch 48 | Batch: 41 | Loss: 27.5309\n",
      "Epoch 48 | Batch: 42 | Loss: 14.7198\n",
      "Epoch 48 | Batch: 43 | Loss: 4.5270\n",
      "Epoch 48 | Batch: 44 | Loss: 9.3527\n",
      "Epoch 48 | Batch: 45 | Loss: 8.0660\n",
      "Epoch 48 | Batch: 46 | Loss: 7.7336\n",
      "Epoch 48 | Batch: 47 | Loss: 7.4744\n",
      "Epoch 48 | Batch: 48 | Loss: 1.5380\n",
      "Mean 9.014768260220686\n",
      "Epoch 49 | Batch: 1 | Loss: 8.2425\n",
      "Epoch 49 | Batch: 2 | Loss: 18.6709\n",
      "Epoch 49 | Batch: 3 | Loss: 12.3650\n",
      "Epoch 49 | Batch: 4 | Loss: 15.0642\n",
      "Epoch 49 | Batch: 5 | Loss: 17.8155\n",
      "Epoch 49 | Batch: 6 | Loss: 21.2882\n",
      "Epoch 49 | Batch: 7 | Loss: 5.9818\n",
      "Epoch 49 | Batch: 8 | Loss: 20.1809\n",
      "Epoch 49 | Batch: 9 | Loss: 27.0187\n",
      "Epoch 49 | Batch: 10 | Loss: 5.7772\n",
      "Epoch 49 | Batch: 11 | Loss: 4.5693\n",
      "Epoch 49 | Batch: 12 | Loss: 12.6875\n",
      "Epoch 49 | Batch: 13 | Loss: 6.7533\n",
      "Epoch 49 | Batch: 14 | Loss: 4.1454\n",
      "Epoch 49 | Batch: 15 | Loss: 4.9411\n",
      "Epoch 49 | Batch: 16 | Loss: 8.3995\n",
      "Epoch 49 | Batch: 17 | Loss: 9.0520\n",
      "Epoch 49 | Batch: 18 | Loss: 7.2899\n",
      "Epoch 49 | Batch: 19 | Loss: 8.4376\n",
      "Epoch 49 | Batch: 20 | Loss: 6.4178\n",
      "Epoch 49 | Batch: 21 | Loss: 7.2926\n",
      "Epoch 49 | Batch: 22 | Loss: 6.8611\n",
      "Epoch 49 | Batch: 23 | Loss: 9.0790\n",
      "Epoch 49 | Batch: 24 | Loss: 7.9817\n",
      "Epoch 49 | Batch: 25 | Loss: 8.5419\n",
      "Epoch 49 | Batch: 26 | Loss: 11.2422\n",
      "Epoch 49 | Batch: 27 | Loss: 7.1624\n",
      "Epoch 49 | Batch: 28 | Loss: 5.1560\n",
      "Epoch 49 | Batch: 29 | Loss: 0.7785\n",
      "Epoch 49 | Batch: 30 | Loss: 8.9363\n",
      "Epoch 49 | Batch: 31 | Loss: 8.6139\n",
      "Epoch 49 | Batch: 32 | Loss: 5.4227\n",
      "Epoch 49 | Batch: 33 | Loss: 12.7335\n",
      "Epoch 49 | Batch: 34 | Loss: 12.5163\n",
      "Epoch 49 | Batch: 35 | Loss: 16.3208\n",
      "Epoch 49 | Batch: 36 | Loss: 14.7495\n",
      "Epoch 49 | Batch: 37 | Loss: 12.0386\n",
      "Epoch 49 | Batch: 38 | Loss: 10.2547\n",
      "Epoch 49 | Batch: 39 | Loss: 5.4215\n",
      "Epoch 49 | Batch: 40 | Loss: 7.1635\n",
      "Epoch 49 | Batch: 41 | Loss: 8.7357\n",
      "Epoch 49 | Batch: 42 | Loss: 7.4714\n",
      "Epoch 49 | Batch: 43 | Loss: 11.5056\n",
      "Epoch 49 | Batch: 44 | Loss: 26.4002\n",
      "Epoch 49 | Batch: 45 | Loss: 10.1705\n",
      "Epoch 49 | Batch: 46 | Loss: 14.1698\n",
      "Epoch 49 | Batch: 47 | Loss: 5.5568\n",
      "Epoch 49 | Batch: 48 | Loss: 2.1445\n",
      "Mean 10.198318839073181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 | Batch: 1 | Loss: 9.4115\n",
      "Epoch 50 | Batch: 2 | Loss: 5.9130\n",
      "Epoch 50 | Batch: 3 | Loss: 5.5990\n",
      "Epoch 50 | Batch: 4 | Loss: 5.6479\n",
      "Epoch 50 | Batch: 5 | Loss: 6.1853\n",
      "Epoch 50 | Batch: 6 | Loss: 13.9058\n",
      "Epoch 50 | Batch: 7 | Loss: 7.5651\n",
      "Epoch 50 | Batch: 8 | Loss: 12.7487\n",
      "Epoch 50 | Batch: 9 | Loss: 6.2045\n",
      "Epoch 50 | Batch: 10 | Loss: 7.1717\n",
      "Epoch 50 | Batch: 11 | Loss: 14.2946\n",
      "Epoch 50 | Batch: 12 | Loss: 5.6728\n",
      "Epoch 50 | Batch: 13 | Loss: 8.2695\n",
      "Epoch 50 | Batch: 14 | Loss: 6.4688\n",
      "Epoch 50 | Batch: 15 | Loss: 10.2250\n",
      "Epoch 50 | Batch: 16 | Loss: 4.1777\n",
      "Epoch 50 | Batch: 17 | Loss: 7.6044\n",
      "Epoch 50 | Batch: 18 | Loss: 8.4436\n",
      "Epoch 50 | Batch: 19 | Loss: 22.0106\n",
      "Epoch 50 | Batch: 20 | Loss: 5.0532\n",
      "Epoch 50 | Batch: 21 | Loss: 13.1690\n",
      "Epoch 50 | Batch: 22 | Loss: 11.5465\n",
      "Epoch 50 | Batch: 23 | Loss: 7.6393\n",
      "Epoch 50 | Batch: 24 | Loss: 6.5219\n",
      "Epoch 50 | Batch: 25 | Loss: 6.2962\n",
      "Epoch 50 | Batch: 26 | Loss: 9.4597\n",
      "Epoch 50 | Batch: 27 | Loss: 7.3038\n",
      "Epoch 50 | Batch: 28 | Loss: 4.0560\n",
      "Epoch 50 | Batch: 29 | Loss: 7.5045\n",
      "Epoch 50 | Batch: 30 | Loss: 17.7543\n",
      "Epoch 50 | Batch: 31 | Loss: 25.3492\n",
      "Epoch 50 | Batch: 32 | Loss: 10.6185\n",
      "Epoch 50 | Batch: 33 | Loss: 4.9185\n",
      "Epoch 50 | Batch: 34 | Loss: 5.4685\n",
      "Epoch 50 | Batch: 35 | Loss: 4.9354\n",
      "Epoch 50 | Batch: 36 | Loss: 7.7424\n",
      "Epoch 50 | Batch: 37 | Loss: 7.4697\n",
      "Epoch 50 | Batch: 38 | Loss: 7.0460\n",
      "Epoch 50 | Batch: 39 | Loss: 8.8156\n",
      "Epoch 50 | Batch: 40 | Loss: 10.3140\n",
      "Epoch 50 | Batch: 41 | Loss: 8.9218\n",
      "Epoch 50 | Batch: 42 | Loss: 11.2177\n",
      "Epoch 50 | Batch: 43 | Loss: 32.2917\n",
      "Epoch 50 | Batch: 44 | Loss: 8.7957\n",
      "Epoch 50 | Batch: 45 | Loss: 8.4216\n",
      "Epoch 50 | Batch: 46 | Loss: 5.4979\n",
      "Epoch 50 | Batch: 47 | Loss: 10.9655\n",
      "Epoch 50 | Batch: 48 | Loss: 5.1768\n",
      "Mean 9.328959266344706\n",
      "Epoch 51 | Batch: 1 | Loss: 8.6755\n",
      "Epoch 51 | Batch: 2 | Loss: 7.9853\n",
      "Epoch 51 | Batch: 3 | Loss: 7.1982\n",
      "Epoch 51 | Batch: 4 | Loss: 9.9532\n",
      "Epoch 51 | Batch: 5 | Loss: 8.1048\n",
      "Epoch 51 | Batch: 6 | Loss: 7.9011\n",
      "Epoch 51 | Batch: 7 | Loss: 10.3250\n",
      "Epoch 51 | Batch: 8 | Loss: 8.2444\n",
      "Epoch 51 | Batch: 9 | Loss: 5.3959\n",
      "Epoch 51 | Batch: 10 | Loss: 9.4176\n",
      "Epoch 51 | Batch: 11 | Loss: 10.6571\n",
      "Epoch 51 | Batch: 12 | Loss: 7.7449\n",
      "Epoch 51 | Batch: 13 | Loss: 4.7271\n",
      "Epoch 51 | Batch: 14 | Loss: 16.6297\n",
      "Epoch 51 | Batch: 15 | Loss: 10.3362\n",
      "Epoch 51 | Batch: 16 | Loss: 6.1465\n",
      "Epoch 51 | Batch: 17 | Loss: 7.9168\n",
      "Epoch 51 | Batch: 18 | Loss: 9.0527\n",
      "Epoch 51 | Batch: 19 | Loss: 4.8623\n",
      "Epoch 51 | Batch: 20 | Loss: 6.8424\n",
      "Epoch 51 | Batch: 21 | Loss: 7.9589\n",
      "Epoch 51 | Batch: 22 | Loss: 5.8332\n",
      "Epoch 51 | Batch: 23 | Loss: 12.0064\n",
      "Epoch 51 | Batch: 24 | Loss: 10.3773\n",
      "Epoch 51 | Batch: 25 | Loss: 9.0704\n",
      "Epoch 51 | Batch: 26 | Loss: 7.6905\n",
      "Epoch 51 | Batch: 27 | Loss: 11.3470\n",
      "Epoch 51 | Batch: 28 | Loss: 10.0625\n",
      "Epoch 51 | Batch: 29 | Loss: 8.7492\n",
      "Epoch 51 | Batch: 30 | Loss: 8.6885\n",
      "Epoch 51 | Batch: 31 | Loss: 8.2703\n",
      "Epoch 51 | Batch: 32 | Loss: 8.4285\n",
      "Epoch 51 | Batch: 33 | Loss: 12.9206\n",
      "Epoch 51 | Batch: 34 | Loss: 7.7275\n",
      "Epoch 51 | Batch: 35 | Loss: 9.8537\n",
      "Epoch 51 | Batch: 36 | Loss: 15.6882\n",
      "Epoch 51 | Batch: 37 | Loss: 12.7884\n",
      "Epoch 51 | Batch: 38 | Loss: 18.2773\n",
      "Epoch 51 | Batch: 39 | Loss: 21.5621\n",
      "Epoch 51 | Batch: 40 | Loss: 18.0025\n",
      "Epoch 51 | Batch: 41 | Loss: 12.9610\n",
      "Epoch 51 | Batch: 42 | Loss: 14.8477\n",
      "Epoch 51 | Batch: 43 | Loss: 7.9803\n",
      "Epoch 51 | Batch: 44 | Loss: 2.9288\n",
      "Epoch 51 | Batch: 45 | Loss: 8.2333\n",
      "Epoch 51 | Batch: 46 | Loss: 7.3351\n",
      "Epoch 51 | Batch: 47 | Loss: 14.0682\n",
      "Epoch 51 | Batch: 48 | Loss: 2.2077\n",
      "Mean 9.666286706924438\n",
      "Epoch 52 | Batch: 1 | Loss: 21.3334\n",
      "Epoch 52 | Batch: 2 | Loss: 20.6507\n",
      "Epoch 52 | Batch: 3 | Loss: 9.3235\n",
      "Epoch 52 | Batch: 4 | Loss: 12.1062\n",
      "Epoch 52 | Batch: 5 | Loss: 9.1329\n",
      "Epoch 52 | Batch: 6 | Loss: 10.9927\n",
      "Epoch 52 | Batch: 7 | Loss: 10.9365\n",
      "Epoch 52 | Batch: 8 | Loss: 5.7222\n",
      "Epoch 52 | Batch: 9 | Loss: 5.4385\n",
      "Epoch 52 | Batch: 10 | Loss: 4.1437\n",
      "Epoch 52 | Batch: 11 | Loss: 13.7299\n",
      "Epoch 52 | Batch: 12 | Loss: 11.6971\n",
      "Epoch 52 | Batch: 13 | Loss: 14.6897\n",
      "Epoch 52 | Batch: 14 | Loss: 10.6053\n",
      "Epoch 52 | Batch: 15 | Loss: 15.7286\n",
      "Epoch 52 | Batch: 16 | Loss: 16.5637\n",
      "Epoch 52 | Batch: 17 | Loss: 9.7084\n",
      "Epoch 52 | Batch: 18 | Loss: 9.5493\n",
      "Epoch 52 | Batch: 19 | Loss: 8.4177\n",
      "Epoch 52 | Batch: 20 | Loss: 8.2384\n",
      "Epoch 52 | Batch: 21 | Loss: 5.8695\n",
      "Epoch 52 | Batch: 22 | Loss: 11.2014\n",
      "Epoch 52 | Batch: 23 | Loss: 15.2243\n",
      "Epoch 52 | Batch: 24 | Loss: 11.1619\n",
      "Epoch 52 | Batch: 25 | Loss: 11.9863\n",
      "Epoch 52 | Batch: 26 | Loss: 8.7426\n",
      "Epoch 52 | Batch: 27 | Loss: 4.8976\n",
      "Epoch 52 | Batch: 28 | Loss: 6.2649\n",
      "Epoch 52 | Batch: 29 | Loss: 5.6566\n",
      "Epoch 52 | Batch: 30 | Loss: 6.3219\n",
      "Epoch 52 | Batch: 31 | Loss: 7.0727\n",
      "Epoch 52 | Batch: 32 | Loss: 16.1190\n",
      "Epoch 52 | Batch: 33 | Loss: 29.7080\n",
      "Epoch 52 | Batch: 34 | Loss: 14.8874\n",
      "Epoch 52 | Batch: 35 | Loss: 14.4884\n",
      "Epoch 52 | Batch: 36 | Loss: 6.3379\n",
      "Epoch 52 | Batch: 37 | Loss: 6.1621\n",
      "Epoch 52 | Batch: 38 | Loss: 6.2452\n",
      "Epoch 52 | Batch: 39 | Loss: 3.1424\n",
      "Epoch 52 | Batch: 40 | Loss: 8.2539\n",
      "Epoch 52 | Batch: 41 | Loss: 14.0907\n",
      "Epoch 52 | Batch: 42 | Loss: 9.7813\n",
      "Epoch 52 | Batch: 43 | Loss: 7.1218\n",
      "Epoch 52 | Batch: 44 | Loss: 10.4780\n",
      "Epoch 52 | Batch: 45 | Loss: 13.0508\n",
      "Epoch 52 | Batch: 46 | Loss: 19.9942\n",
      "Epoch 52 | Batch: 47 | Loss: 30.8041\n",
      "Epoch 52 | Batch: 48 | Loss: 3.0393\n",
      "Mean 11.183595483501753\n",
      "Epoch 53 | Batch: 1 | Loss: 9.7537\n",
      "Epoch 53 | Batch: 2 | Loss: 8.5134\n",
      "Epoch 53 | Batch: 3 | Loss: 9.5975\n",
      "Epoch 53 | Batch: 4 | Loss: 3.8576\n",
      "Epoch 53 | Batch: 5 | Loss: 7.8286\n",
      "Epoch 53 | Batch: 6 | Loss: 8.5597\n",
      "Epoch 53 | Batch: 7 | Loss: 19.9715\n",
      "Epoch 53 | Batch: 8 | Loss: 10.5227\n",
      "Epoch 53 | Batch: 9 | Loss: 7.8683\n",
      "Epoch 53 | Batch: 10 | Loss: 7.7189\n",
      "Epoch 53 | Batch: 11 | Loss: 12.4082\n",
      "Epoch 53 | Batch: 12 | Loss: 15.0049\n",
      "Epoch 53 | Batch: 13 | Loss: 9.9555\n",
      "Epoch 53 | Batch: 14 | Loss: 14.7987\n",
      "Epoch 53 | Batch: 15 | Loss: 12.1073\n",
      "Epoch 53 | Batch: 16 | Loss: 19.5640\n",
      "Epoch 53 | Batch: 17 | Loss: 11.4251\n",
      "Epoch 53 | Batch: 18 | Loss: 11.4013\n",
      "Epoch 53 | Batch: 19 | Loss: 14.4816\n",
      "Epoch 53 | Batch: 20 | Loss: 9.9930\n",
      "Epoch 53 | Batch: 21 | Loss: 12.4850\n",
      "Epoch 53 | Batch: 22 | Loss: 21.2175\n",
      "Epoch 53 | Batch: 23 | Loss: 18.6469\n",
      "Epoch 53 | Batch: 24 | Loss: 3.6237\n",
      "Epoch 53 | Batch: 25 | Loss: 8.0557\n",
      "Epoch 53 | Batch: 26 | Loss: 5.3206\n",
      "Epoch 53 | Batch: 27 | Loss: 11.1158\n",
      "Epoch 53 | Batch: 28 | Loss: 10.5311\n",
      "Epoch 53 | Batch: 29 | Loss: 11.5547\n",
      "Epoch 53 | Batch: 30 | Loss: 8.0399\n",
      "Epoch 53 | Batch: 31 | Loss: 6.0011\n",
      "Epoch 53 | Batch: 32 | Loss: 9.5303\n",
      "Epoch 53 | Batch: 33 | Loss: 7.5012\n",
      "Epoch 53 | Batch: 34 | Loss: 3.8417\n",
      "Epoch 53 | Batch: 35 | Loss: 7.5812\n",
      "Epoch 53 | Batch: 36 | Loss: 7.1583\n",
      "Epoch 53 | Batch: 37 | Loss: 10.2504\n",
      "Epoch 53 | Batch: 38 | Loss: 12.9944\n",
      "Epoch 53 | Batch: 39 | Loss: 9.5511\n",
      "Epoch 53 | Batch: 40 | Loss: 4.4783\n",
      "Epoch 53 | Batch: 41 | Loss: 9.3861\n",
      "Epoch 53 | Batch: 42 | Loss: 7.1247\n",
      "Epoch 53 | Batch: 43 | Loss: 6.3655\n",
      "Epoch 53 | Batch: 44 | Loss: 6.8920\n",
      "Epoch 53 | Batch: 45 | Loss: 11.4721\n",
      "Epoch 53 | Batch: 46 | Loss: 5.9217\n",
      "Epoch 53 | Batch: 47 | Loss: 8.9687\n",
      "Epoch 53 | Batch: 48 | Loss: 9.2058\n",
      "Mean 10.003068933884302\n",
      "Epoch 54 | Batch: 1 | Loss: 7.9589\n",
      "Epoch 54 | Batch: 2 | Loss: 4.4752\n",
      "Epoch 54 | Batch: 3 | Loss: 8.3208\n",
      "Epoch 54 | Batch: 4 | Loss: 4.7568\n",
      "Epoch 54 | Batch: 5 | Loss: 3.8106\n",
      "Epoch 54 | Batch: 6 | Loss: 11.7950\n",
      "Epoch 54 | Batch: 7 | Loss: 7.7944\n",
      "Epoch 54 | Batch: 8 | Loss: 8.9817\n",
      "Epoch 54 | Batch: 9 | Loss: 8.4704\n",
      "Epoch 54 | Batch: 10 | Loss: 5.8001\n",
      "Epoch 54 | Batch: 11 | Loss: 12.2660\n",
      "Epoch 54 | Batch: 12 | Loss: 11.0617\n",
      "Epoch 54 | Batch: 13 | Loss: 7.4524\n",
      "Epoch 54 | Batch: 14 | Loss: 3.8990\n",
      "Epoch 54 | Batch: 15 | Loss: 2.7919\n",
      "Epoch 54 | Batch: 16 | Loss: 5.7352\n",
      "Epoch 54 | Batch: 17 | Loss: 7.3682\n",
      "Epoch 54 | Batch: 18 | Loss: 14.6420\n",
      "Epoch 54 | Batch: 19 | Loss: 22.0235\n",
      "Epoch 54 | Batch: 20 | Loss: 15.5299\n",
      "Epoch 54 | Batch: 21 | Loss: 6.7990\n",
      "Epoch 54 | Batch: 22 | Loss: 4.5052\n",
      "Epoch 54 | Batch: 23 | Loss: 4.9666\n",
      "Epoch 54 | Batch: 24 | Loss: 12.0560\n",
      "Epoch 54 | Batch: 25 | Loss: 14.9913\n",
      "Epoch 54 | Batch: 26 | Loss: 13.0104\n",
      "Epoch 54 | Batch: 27 | Loss: 10.6436\n",
      "Epoch 54 | Batch: 28 | Loss: 9.6080\n",
      "Epoch 54 | Batch: 29 | Loss: 14.0472\n",
      "Epoch 54 | Batch: 30 | Loss: 10.8909\n",
      "Epoch 54 | Batch: 31 | Loss: 6.1774\n",
      "Epoch 54 | Batch: 32 | Loss: 8.2015\n",
      "Epoch 54 | Batch: 33 | Loss: 6.8951\n",
      "Epoch 54 | Batch: 34 | Loss: 5.5801\n",
      "Epoch 54 | Batch: 35 | Loss: 13.6374\n",
      "Epoch 54 | Batch: 36 | Loss: 4.2210\n",
      "Epoch 54 | Batch: 37 | Loss: 10.0815\n",
      "Epoch 54 | Batch: 38 | Loss: 13.7937\n",
      "Epoch 54 | Batch: 39 | Loss: 21.8514\n",
      "Epoch 54 | Batch: 40 | Loss: 41.3072\n",
      "Epoch 54 | Batch: 41 | Loss: 7.8277\n",
      "Epoch 54 | Batch: 42 | Loss: 18.7510\n",
      "Epoch 54 | Batch: 43 | Loss: 17.9829\n",
      "Epoch 54 | Batch: 44 | Loss: 14.4669\n",
      "Epoch 54 | Batch: 45 | Loss: 9.3067\n",
      "Epoch 54 | Batch: 46 | Loss: 8.9903\n",
      "Epoch 54 | Batch: 47 | Loss: 4.8680\n",
      "Epoch 54 | Batch: 48 | Loss: 3.5464\n",
      "Mean 10.290380011002222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 | Batch: 1 | Loss: 7.6499\n",
      "Epoch 55 | Batch: 2 | Loss: 18.5207\n",
      "Epoch 55 | Batch: 3 | Loss: 18.6059\n",
      "Epoch 55 | Batch: 4 | Loss: 18.7248\n",
      "Epoch 55 | Batch: 5 | Loss: 6.8315\n",
      "Epoch 55 | Batch: 6 | Loss: 6.2268\n",
      "Epoch 55 | Batch: 7 | Loss: 7.3906\n",
      "Epoch 55 | Batch: 8 | Loss: 8.0856\n",
      "Epoch 55 | Batch: 9 | Loss: 6.9762\n",
      "Epoch 55 | Batch: 10 | Loss: 7.0463\n",
      "Epoch 55 | Batch: 11 | Loss: 7.1530\n",
      "Epoch 55 | Batch: 12 | Loss: 6.0400\n",
      "Epoch 55 | Batch: 13 | Loss: 6.5010\n",
      "Epoch 55 | Batch: 14 | Loss: 4.4361\n",
      "Epoch 55 | Batch: 15 | Loss: 4.3300\n",
      "Epoch 55 | Batch: 16 | Loss: 10.8240\n",
      "Epoch 55 | Batch: 17 | Loss: 7.9503\n",
      "Epoch 55 | Batch: 18 | Loss: 14.3418\n",
      "Epoch 55 | Batch: 19 | Loss: 5.4414\n",
      "Epoch 55 | Batch: 20 | Loss: 10.0413\n",
      "Epoch 55 | Batch: 21 | Loss: 2.0120\n",
      "Epoch 55 | Batch: 22 | Loss: 7.7632\n",
      "Epoch 55 | Batch: 23 | Loss: 5.8612\n",
      "Epoch 55 | Batch: 24 | Loss: 7.5350\n",
      "Epoch 55 | Batch: 25 | Loss: 3.9515\n",
      "Epoch 55 | Batch: 26 | Loss: 6.9971\n",
      "Epoch 55 | Batch: 27 | Loss: 7.0892\n",
      "Epoch 55 | Batch: 28 | Loss: 11.3857\n",
      "Epoch 55 | Batch: 29 | Loss: 11.1219\n",
      "Epoch 55 | Batch: 30 | Loss: 8.0227\n",
      "Epoch 55 | Batch: 31 | Loss: 7.1807\n",
      "Epoch 55 | Batch: 32 | Loss: 8.8590\n",
      "Epoch 55 | Batch: 33 | Loss: 12.6234\n",
      "Epoch 55 | Batch: 34 | Loss: 21.9665\n",
      "Epoch 55 | Batch: 35 | Loss: 12.5167\n",
      "Epoch 55 | Batch: 36 | Loss: 7.1172\n",
      "Epoch 55 | Batch: 37 | Loss: 7.2488\n",
      "Epoch 55 | Batch: 38 | Loss: 7.9085\n",
      "Epoch 55 | Batch: 39 | Loss: 8.1273\n",
      "Epoch 55 | Batch: 40 | Loss: 8.3647\n",
      "Epoch 55 | Batch: 41 | Loss: 7.5868\n",
      "Epoch 55 | Batch: 42 | Loss: 10.2236\n",
      "Epoch 55 | Batch: 43 | Loss: 8.1623\n",
      "Epoch 55 | Batch: 44 | Loss: 9.9258\n",
      "Epoch 55 | Batch: 45 | Loss: 8.3320\n",
      "Epoch 55 | Batch: 46 | Loss: 7.9257\n",
      "Epoch 55 | Batch: 47 | Loss: 6.6490\n",
      "Epoch 55 | Batch: 48 | Loss: 4.0930\n",
      "Mean 8.743080457051596\n",
      "Epoch 56 | Batch: 1 | Loss: 7.2886\n",
      "Epoch 56 | Batch: 2 | Loss: 4.0619\n",
      "Epoch 56 | Batch: 3 | Loss: 8.0232\n",
      "Epoch 56 | Batch: 4 | Loss: 6.3842\n",
      "Epoch 56 | Batch: 5 | Loss: 4.6853\n",
      "Epoch 56 | Batch: 6 | Loss: 5.1977\n",
      "Epoch 56 | Batch: 7 | Loss: 7.1092\n",
      "Epoch 56 | Batch: 8 | Loss: 8.3647\n",
      "Epoch 56 | Batch: 9 | Loss: 6.7217\n",
      "Epoch 56 | Batch: 10 | Loss: 7.5462\n",
      "Epoch 56 | Batch: 11 | Loss: 10.6614\n",
      "Epoch 56 | Batch: 12 | Loss: 4.7280\n",
      "Epoch 56 | Batch: 13 | Loss: 21.6577\n",
      "Epoch 56 | Batch: 14 | Loss: 26.5952\n",
      "Epoch 56 | Batch: 15 | Loss: 7.8906\n",
      "Epoch 56 | Batch: 16 | Loss: 5.6460\n",
      "Epoch 56 | Batch: 17 | Loss: 6.4284\n",
      "Epoch 56 | Batch: 18 | Loss: 5.9718\n",
      "Epoch 56 | Batch: 19 | Loss: 8.2817\n",
      "Epoch 56 | Batch: 20 | Loss: 11.4919\n",
      "Epoch 56 | Batch: 21 | Loss: 12.3142\n",
      "Epoch 56 | Batch: 22 | Loss: 6.6951\n",
      "Epoch 56 | Batch: 23 | Loss: 9.4687\n",
      "Epoch 56 | Batch: 24 | Loss: 9.2017\n",
      "Epoch 56 | Batch: 25 | Loss: 7.8385\n",
      "Epoch 56 | Batch: 26 | Loss: 5.3620\n",
      "Epoch 56 | Batch: 27 | Loss: 7.7360\n",
      "Epoch 56 | Batch: 28 | Loss: 9.3272\n",
      "Epoch 56 | Batch: 29 | Loss: 8.2223\n",
      "Epoch 56 | Batch: 30 | Loss: 3.4019\n",
      "Epoch 56 | Batch: 31 | Loss: 7.4633\n",
      "Epoch 56 | Batch: 32 | Loss: 19.9075\n",
      "Epoch 56 | Batch: 33 | Loss: 13.3746\n",
      "Epoch 56 | Batch: 34 | Loss: 13.1878\n",
      "Epoch 56 | Batch: 35 | Loss: 6.4686\n",
      "Epoch 56 | Batch: 36 | Loss: 8.0590\n",
      "Epoch 56 | Batch: 37 | Loss: 8.4187\n",
      "Epoch 56 | Batch: 38 | Loss: 16.8870\n",
      "Epoch 56 | Batch: 39 | Loss: 8.6735\n",
      "Epoch 56 | Batch: 40 | Loss: 12.2402\n",
      "Epoch 56 | Batch: 41 | Loss: 10.0464\n",
      "Epoch 56 | Batch: 42 | Loss: 11.7246\n",
      "Epoch 56 | Batch: 43 | Loss: 9.4149\n",
      "Epoch 56 | Batch: 44 | Loss: 14.2213\n",
      "Epoch 56 | Batch: 45 | Loss: 7.8089\n",
      "Epoch 56 | Batch: 46 | Loss: 9.3886\n",
      "Epoch 56 | Batch: 47 | Loss: 18.2513\n",
      "Epoch 56 | Batch: 48 | Loss: 7.7300\n",
      "Mean 9.5326925466458\n",
      "Epoch 57 | Batch: 1 | Loss: 9.8575\n",
      "Epoch 57 | Batch: 2 | Loss: 7.1714\n",
      "Epoch 57 | Batch: 3 | Loss: 5.6358\n",
      "Epoch 57 | Batch: 4 | Loss: 18.1315\n",
      "Epoch 57 | Batch: 5 | Loss: 11.3720\n",
      "Epoch 57 | Batch: 6 | Loss: 6.7135\n",
      "Epoch 57 | Batch: 7 | Loss: 5.4244\n",
      "Epoch 57 | Batch: 8 | Loss: 6.3006\n",
      "Epoch 57 | Batch: 9 | Loss: 5.9669\n",
      "Epoch 57 | Batch: 10 | Loss: 8.0662\n",
      "Epoch 57 | Batch: 11 | Loss: 9.9038\n",
      "Epoch 57 | Batch: 12 | Loss: 6.4708\n",
      "Epoch 57 | Batch: 13 | Loss: 12.7654\n",
      "Epoch 57 | Batch: 14 | Loss: 7.5018\n",
      "Epoch 57 | Batch: 15 | Loss: 10.2327\n",
      "Epoch 57 | Batch: 16 | Loss: 4.8940\n",
      "Epoch 57 | Batch: 17 | Loss: 8.1714\n",
      "Epoch 57 | Batch: 18 | Loss: 7.6159\n",
      "Epoch 57 | Batch: 19 | Loss: 6.1550\n",
      "Epoch 57 | Batch: 20 | Loss: 4.7630\n",
      "Epoch 57 | Batch: 21 | Loss: 5.4503\n",
      "Epoch 57 | Batch: 22 | Loss: 9.9996\n",
      "Epoch 57 | Batch: 23 | Loss: 6.2221\n",
      "Epoch 57 | Batch: 24 | Loss: 9.4415\n",
      "Epoch 57 | Batch: 25 | Loss: 12.3679\n",
      "Epoch 57 | Batch: 26 | Loss: 11.1727\n",
      "Epoch 57 | Batch: 27 | Loss: 12.1778\n",
      "Epoch 57 | Batch: 28 | Loss: 12.9005\n",
      "Epoch 57 | Batch: 29 | Loss: 9.1284\n",
      "Epoch 57 | Batch: 30 | Loss: 17.1671\n",
      "Epoch 57 | Batch: 31 | Loss: 10.1846\n",
      "Epoch 57 | Batch: 32 | Loss: 5.3636\n",
      "Epoch 57 | Batch: 33 | Loss: 10.8124\n",
      "Epoch 57 | Batch: 34 | Loss: 11.3987\n",
      "Epoch 57 | Batch: 35 | Loss: 9.1477\n",
      "Epoch 57 | Batch: 36 | Loss: 14.4850\n",
      "Epoch 57 | Batch: 37 | Loss: 23.0251\n",
      "Epoch 57 | Batch: 38 | Loss: 21.0616\n",
      "Epoch 57 | Batch: 39 | Loss: 8.0858\n",
      "Epoch 57 | Batch: 40 | Loss: 13.1952\n",
      "Epoch 57 | Batch: 41 | Loss: 8.1009\n",
      "Epoch 57 | Batch: 42 | Loss: 5.7585\n",
      "Epoch 57 | Batch: 43 | Loss: 5.2694\n",
      "Epoch 57 | Batch: 44 | Loss: 10.1745\n",
      "Epoch 57 | Batch: 45 | Loss: 6.5174\n",
      "Epoch 57 | Batch: 46 | Loss: 5.4747\n",
      "Epoch 57 | Batch: 47 | Loss: 9.6873\n",
      "Epoch 57 | Batch: 48 | Loss: 2.6713\n",
      "Mean 9.365731035669645\n",
      "Epoch 58 | Batch: 1 | Loss: 7.4280\n",
      "Epoch 58 | Batch: 2 | Loss: 0.8464\n",
      "Epoch 58 | Batch: 3 | Loss: 10.1348\n",
      "Epoch 58 | Batch: 4 | Loss: 4.8548\n",
      "Epoch 58 | Batch: 5 | Loss: 9.5315\n",
      "Epoch 58 | Batch: 6 | Loss: 9.8793\n",
      "Epoch 58 | Batch: 7 | Loss: 8.2919\n",
      "Epoch 58 | Batch: 8 | Loss: 11.0423\n",
      "Epoch 58 | Batch: 9 | Loss: 6.4705\n",
      "Epoch 58 | Batch: 10 | Loss: 5.2238\n",
      "Epoch 58 | Batch: 11 | Loss: 8.6124\n",
      "Epoch 58 | Batch: 12 | Loss: 3.4347\n",
      "Epoch 58 | Batch: 13 | Loss: 14.8470\n",
      "Epoch 58 | Batch: 14 | Loss: 22.2991\n",
      "Epoch 58 | Batch: 15 | Loss: 12.1997\n",
      "Epoch 58 | Batch: 16 | Loss: 19.5706\n",
      "Epoch 58 | Batch: 17 | Loss: 13.5366\n",
      "Epoch 58 | Batch: 18 | Loss: 7.5434\n",
      "Epoch 58 | Batch: 19 | Loss: 5.1735\n",
      "Epoch 58 | Batch: 20 | Loss: 12.7645\n",
      "Epoch 58 | Batch: 21 | Loss: 4.2921\n",
      "Epoch 58 | Batch: 22 | Loss: 10.2943\n",
      "Epoch 58 | Batch: 23 | Loss: 11.6918\n",
      "Epoch 58 | Batch: 24 | Loss: 7.8295\n",
      "Epoch 58 | Batch: 25 | Loss: 10.6077\n",
      "Epoch 58 | Batch: 26 | Loss: 7.6911\n",
      "Epoch 58 | Batch: 27 | Loss: 7.6931\n",
      "Epoch 58 | Batch: 28 | Loss: 4.4637\n",
      "Epoch 58 | Batch: 29 | Loss: 7.7336\n",
      "Epoch 58 | Batch: 30 | Loss: 7.9462\n",
      "Epoch 58 | Batch: 31 | Loss: 9.0469\n",
      "Epoch 58 | Batch: 32 | Loss: 6.9482\n",
      "Epoch 58 | Batch: 33 | Loss: 9.2909\n",
      "Epoch 58 | Batch: 34 | Loss: 7.7664\n",
      "Epoch 58 | Batch: 35 | Loss: 17.0664\n",
      "Epoch 58 | Batch: 36 | Loss: 6.9050\n",
      "Epoch 58 | Batch: 37 | Loss: 2.9155\n",
      "Epoch 58 | Batch: 38 | Loss: 6.3214\n",
      "Epoch 58 | Batch: 39 | Loss: 9.8963\n",
      "Epoch 58 | Batch: 40 | Loss: 7.8147\n",
      "Epoch 58 | Batch: 41 | Loss: 6.4182\n",
      "Epoch 58 | Batch: 42 | Loss: 7.7765\n",
      "Epoch 58 | Batch: 43 | Loss: 8.8655\n",
      "Epoch 58 | Batch: 44 | Loss: 5.7634\n",
      "Epoch 58 | Batch: 45 | Loss: 8.4616\n",
      "Epoch 58 | Batch: 46 | Loss: 6.2696\n",
      "Epoch 58 | Batch: 47 | Loss: 7.1626\n",
      "Epoch 58 | Batch: 48 | Loss: 3.5433\n",
      "Mean 8.586675178259611\n",
      "Epoch 59 | Batch: 1 | Loss: 9.1068\n",
      "Epoch 59 | Batch: 2 | Loss: 7.4681\n",
      "Epoch 59 | Batch: 3 | Loss: 9.2159\n",
      "Epoch 59 | Batch: 4 | Loss: 9.4595\n",
      "Epoch 59 | Batch: 5 | Loss: 15.0895\n",
      "Epoch 59 | Batch: 6 | Loss: 8.1959\n",
      "Epoch 59 | Batch: 7 | Loss: 10.4823\n",
      "Epoch 59 | Batch: 8 | Loss: 8.4075\n",
      "Epoch 59 | Batch: 9 | Loss: 5.9820\n",
      "Epoch 59 | Batch: 10 | Loss: 11.1368\n",
      "Epoch 59 | Batch: 11 | Loss: 9.0893\n",
      "Epoch 59 | Batch: 12 | Loss: 13.9886\n",
      "Epoch 59 | Batch: 13 | Loss: 5.1050\n",
      "Epoch 59 | Batch: 14 | Loss: 12.7339\n",
      "Epoch 59 | Batch: 15 | Loss: 25.8239\n",
      "Epoch 59 | Batch: 16 | Loss: 10.3765\n",
      "Epoch 59 | Batch: 17 | Loss: 12.5412\n",
      "Epoch 59 | Batch: 18 | Loss: 8.1077\n",
      "Epoch 59 | Batch: 19 | Loss: 9.6064\n",
      "Epoch 59 | Batch: 20 | Loss: 6.4318\n",
      "Epoch 59 | Batch: 21 | Loss: 6.6184\n",
      "Epoch 59 | Batch: 22 | Loss: 3.0897\n",
      "Epoch 59 | Batch: 23 | Loss: 4.5118\n",
      "Epoch 59 | Batch: 24 | Loss: 6.3271\n",
      "Epoch 59 | Batch: 25 | Loss: 8.4997\n",
      "Epoch 59 | Batch: 26 | Loss: 6.1592\n",
      "Epoch 59 | Batch: 27 | Loss: 8.0965\n",
      "Epoch 59 | Batch: 28 | Loss: 10.1976\n",
      "Epoch 59 | Batch: 29 | Loss: 9.9098\n",
      "Epoch 59 | Batch: 30 | Loss: 6.0259\n",
      "Epoch 59 | Batch: 31 | Loss: 9.4570\n",
      "Epoch 59 | Batch: 32 | Loss: 26.2732\n",
      "Epoch 59 | Batch: 33 | Loss: 11.4349\n",
      "Epoch 59 | Batch: 34 | Loss: 9.2699\n",
      "Epoch 59 | Batch: 35 | Loss: 6.0326\n",
      "Epoch 59 | Batch: 36 | Loss: 10.1702\n",
      "Epoch 59 | Batch: 37 | Loss: 8.1757\n",
      "Epoch 59 | Batch: 38 | Loss: 18.0434\n",
      "Epoch 59 | Batch: 39 | Loss: 14.3055\n",
      "Epoch 59 | Batch: 40 | Loss: 8.4473\n",
      "Epoch 59 | Batch: 41 | Loss: 9.0444\n",
      "Epoch 59 | Batch: 42 | Loss: 6.1929\n",
      "Epoch 59 | Batch: 43 | Loss: 9.8722\n",
      "Epoch 59 | Batch: 44 | Loss: 5.0748\n",
      "Epoch 59 | Batch: 45 | Loss: 6.8399\n",
      "Epoch 59 | Batch: 46 | Loss: 10.3188\n",
      "Epoch 59 | Batch: 47 | Loss: 6.4257\n",
      "Epoch 59 | Batch: 48 | Loss: 3.4604\n",
      "Mean 9.512986779212952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 | Batch: 1 | Loss: 7.5875\n",
      "Epoch 60 | Batch: 2 | Loss: 9.3633\n",
      "Epoch 60 | Batch: 3 | Loss: 7.6550\n",
      "Epoch 60 | Batch: 4 | Loss: 8.0574\n",
      "Epoch 60 | Batch: 5 | Loss: 8.0366\n",
      "Epoch 60 | Batch: 6 | Loss: 7.6764\n",
      "Epoch 60 | Batch: 7 | Loss: 8.6077\n",
      "Epoch 60 | Batch: 8 | Loss: 10.3869\n",
      "Epoch 60 | Batch: 9 | Loss: 12.2965\n",
      "Epoch 60 | Batch: 10 | Loss: 8.8627\n",
      "Epoch 60 | Batch: 11 | Loss: 9.4757\n",
      "Epoch 60 | Batch: 12 | Loss: 8.1917\n",
      "Epoch 60 | Batch: 13 | Loss: 9.6560\n",
      "Epoch 60 | Batch: 14 | Loss: 3.0912\n",
      "Epoch 60 | Batch: 15 | Loss: 8.3617\n",
      "Epoch 60 | Batch: 16 | Loss: 11.6312\n",
      "Epoch 60 | Batch: 17 | Loss: 6.0323\n",
      "Epoch 60 | Batch: 18 | Loss: 11.1070\n",
      "Epoch 60 | Batch: 19 | Loss: 9.5779\n",
      "Epoch 60 | Batch: 20 | Loss: 10.5849\n",
      "Epoch 60 | Batch: 21 | Loss: 10.2501\n",
      "Epoch 60 | Batch: 22 | Loss: 7.1186\n",
      "Epoch 60 | Batch: 23 | Loss: 9.2772\n",
      "Epoch 60 | Batch: 24 | Loss: 6.9535\n",
      "Epoch 60 | Batch: 25 | Loss: 10.7414\n",
      "Epoch 60 | Batch: 26 | Loss: 9.2539\n",
      "Epoch 60 | Batch: 27 | Loss: 10.2524\n",
      "Epoch 60 | Batch: 28 | Loss: 8.1652\n",
      "Epoch 60 | Batch: 29 | Loss: 8.4164\n",
      "Epoch 60 | Batch: 30 | Loss: 12.8765\n",
      "Epoch 60 | Batch: 31 | Loss: 10.0891\n",
      "Epoch 60 | Batch: 32 | Loss: 13.8359\n",
      "Epoch 60 | Batch: 33 | Loss: 16.8333\n",
      "Epoch 60 | Batch: 34 | Loss: 15.9691\n",
      "Epoch 60 | Batch: 35 | Loss: 8.2781\n",
      "Epoch 60 | Batch: 36 | Loss: 4.5648\n",
      "Epoch 60 | Batch: 37 | Loss: 7.2986\n",
      "Epoch 60 | Batch: 38 | Loss: 5.5558\n",
      "Epoch 60 | Batch: 39 | Loss: 7.9406\n",
      "Epoch 60 | Batch: 40 | Loss: 18.7024\n",
      "Epoch 60 | Batch: 41 | Loss: 15.6640\n",
      "Epoch 60 | Batch: 42 | Loss: 6.0534\n",
      "Epoch 60 | Batch: 43 | Loss: 9.5603\n",
      "Epoch 60 | Batch: 44 | Loss: 10.0755\n",
      "Epoch 60 | Batch: 45 | Loss: 9.9159\n",
      "Epoch 60 | Batch: 46 | Loss: 12.0599\n",
      "Epoch 60 | Batch: 47 | Loss: 4.7106\n",
      "Epoch 60 | Batch: 48 | Loss: 4.1283\n",
      "Mean 9.391260807712873\n",
      "Epoch 61 | Batch: 1 | Loss: 5.9957\n",
      "Epoch 61 | Batch: 2 | Loss: 18.4494\n",
      "Epoch 61 | Batch: 3 | Loss: 15.8943\n",
      "Epoch 61 | Batch: 4 | Loss: 10.8305\n",
      "Epoch 61 | Batch: 5 | Loss: 9.0345\n",
      "Epoch 61 | Batch: 6 | Loss: 8.3735\n",
      "Epoch 61 | Batch: 7 | Loss: 5.7713\n",
      "Epoch 61 | Batch: 8 | Loss: 4.4729\n",
      "Epoch 61 | Batch: 9 | Loss: 9.1080\n",
      "Epoch 61 | Batch: 10 | Loss: 9.5715\n",
      "Epoch 61 | Batch: 11 | Loss: 6.6665\n",
      "Epoch 61 | Batch: 12 | Loss: 7.2558\n",
      "Epoch 61 | Batch: 13 | Loss: 8.9818\n",
      "Epoch 61 | Batch: 14 | Loss: 19.0615\n",
      "Epoch 61 | Batch: 15 | Loss: 23.9662\n",
      "Epoch 61 | Batch: 16 | Loss: 7.7764\n",
      "Epoch 61 | Batch: 17 | Loss: 5.0195\n",
      "Epoch 61 | Batch: 18 | Loss: 6.4228\n",
      "Epoch 61 | Batch: 19 | Loss: 11.3120\n",
      "Epoch 61 | Batch: 20 | Loss: 2.5045\n",
      "Epoch 61 | Batch: 21 | Loss: 11.0727\n",
      "Epoch 61 | Batch: 22 | Loss: 10.0744\n",
      "Epoch 61 | Batch: 23 | Loss: 13.7529\n",
      "Epoch 61 | Batch: 24 | Loss: 4.5115\n",
      "Epoch 61 | Batch: 25 | Loss: 5.1603\n",
      "Epoch 61 | Batch: 26 | Loss: 6.8034\n",
      "Epoch 61 | Batch: 27 | Loss: 8.3258\n",
      "Epoch 61 | Batch: 28 | Loss: 7.5847\n",
      "Epoch 61 | Batch: 29 | Loss: 8.0855\n",
      "Epoch 61 | Batch: 30 | Loss: 7.0170\n",
      "Epoch 61 | Batch: 31 | Loss: 7.6542\n",
      "Epoch 61 | Batch: 32 | Loss: 7.8163\n",
      "Epoch 61 | Batch: 33 | Loss: 8.5015\n",
      "Epoch 61 | Batch: 34 | Loss: 12.3773\n",
      "Epoch 61 | Batch: 35 | Loss: 5.5234\n",
      "Epoch 61 | Batch: 36 | Loss: 4.6551\n",
      "Epoch 61 | Batch: 37 | Loss: 11.6165\n",
      "Epoch 61 | Batch: 38 | Loss: 6.7014\n",
      "Epoch 61 | Batch: 39 | Loss: 11.6788\n",
      "Epoch 61 | Batch: 40 | Loss: 12.5542\n",
      "Epoch 61 | Batch: 41 | Loss: 9.5450\n",
      "Epoch 61 | Batch: 42 | Loss: 5.3162\n",
      "Epoch 61 | Batch: 43 | Loss: 10.8659\n",
      "Epoch 61 | Batch: 44 | Loss: 11.6321\n",
      "Epoch 61 | Batch: 45 | Loss: 6.5962\n",
      "Epoch 61 | Batch: 46 | Loss: 15.7884\n",
      "Epoch 61 | Batch: 47 | Loss: 12.8338\n",
      "Epoch 61 | Batch: 48 | Loss: 4.4652\n",
      "Mean 9.27038057645162\n",
      "Epoch 62 | Batch: 1 | Loss: 9.1745\n",
      "Epoch 62 | Batch: 2 | Loss: 2.7149\n",
      "Epoch 62 | Batch: 3 | Loss: 6.3700\n",
      "Epoch 62 | Batch: 4 | Loss: 3.7657\n",
      "Epoch 62 | Batch: 5 | Loss: 6.3240\n",
      "Epoch 62 | Batch: 6 | Loss: 10.4752\n",
      "Epoch 62 | Batch: 7 | Loss: 5.8350\n",
      "Epoch 62 | Batch: 8 | Loss: 10.3854\n",
      "Epoch 62 | Batch: 9 | Loss: 13.6368\n",
      "Epoch 62 | Batch: 10 | Loss: 13.1146\n",
      "Epoch 62 | Batch: 11 | Loss: 9.0092\n",
      "Epoch 62 | Batch: 12 | Loss: 8.2473\n",
      "Epoch 62 | Batch: 13 | Loss: 11.1348\n",
      "Epoch 62 | Batch: 14 | Loss: 9.9172\n",
      "Epoch 62 | Batch: 15 | Loss: 5.2132\n",
      "Epoch 62 | Batch: 16 | Loss: 6.5266\n",
      "Epoch 62 | Batch: 17 | Loss: 6.2507\n",
      "Epoch 62 | Batch: 18 | Loss: 7.9100\n",
      "Epoch 62 | Batch: 19 | Loss: 7.8815\n",
      "Epoch 62 | Batch: 20 | Loss: 7.4599\n",
      "Epoch 62 | Batch: 21 | Loss: 8.7190\n",
      "Epoch 62 | Batch: 22 | Loss: 11.3400\n",
      "Epoch 62 | Batch: 23 | Loss: 13.9446\n",
      "Epoch 62 | Batch: 24 | Loss: 22.7677\n",
      "Epoch 62 | Batch: 25 | Loss: 9.4020\n",
      "Epoch 62 | Batch: 26 | Loss: 6.7900\n",
      "Epoch 62 | Batch: 27 | Loss: 10.1156\n",
      "Epoch 62 | Batch: 28 | Loss: 12.5544\n",
      "Epoch 62 | Batch: 29 | Loss: 26.0524\n",
      "Epoch 62 | Batch: 30 | Loss: 9.8054\n",
      "Epoch 62 | Batch: 31 | Loss: 10.6266\n",
      "Epoch 62 | Batch: 32 | Loss: 7.6532\n",
      "Epoch 62 | Batch: 33 | Loss: 15.9190\n",
      "Epoch 62 | Batch: 34 | Loss: 10.9468\n",
      "Epoch 62 | Batch: 35 | Loss: 9.2478\n",
      "Epoch 62 | Batch: 36 | Loss: 3.7301\n",
      "Epoch 62 | Batch: 37 | Loss: 12.3622\n",
      "Epoch 62 | Batch: 38 | Loss: 7.6674\n",
      "Epoch 62 | Batch: 39 | Loss: 10.0204\n",
      "Epoch 62 | Batch: 40 | Loss: 12.3760\n",
      "Epoch 62 | Batch: 41 | Loss: 11.2799\n",
      "Epoch 62 | Batch: 42 | Loss: 11.8257\n",
      "Epoch 62 | Batch: 43 | Loss: 5.3218\n",
      "Epoch 62 | Batch: 44 | Loss: 8.4953\n",
      "Epoch 62 | Batch: 45 | Loss: 16.7850\n",
      "Epoch 62 | Batch: 46 | Loss: 18.0363\n",
      "Epoch 62 | Batch: 47 | Loss: 17.2243\n",
      "Epoch 62 | Batch: 48 | Loss: 11.6443\n",
      "Mean 10.291662037372589\n",
      "Epoch 63 | Batch: 1 | Loss: 5.9286\n",
      "Epoch 63 | Batch: 2 | Loss: 30.3890\n",
      "Epoch 63 | Batch: 3 | Loss: 13.8198\n",
      "Epoch 63 | Batch: 4 | Loss: 16.2314\n",
      "Epoch 63 | Batch: 5 | Loss: 10.2729\n",
      "Epoch 63 | Batch: 6 | Loss: 6.0865\n",
      "Epoch 63 | Batch: 7 | Loss: 6.9262\n",
      "Epoch 63 | Batch: 8 | Loss: 6.1582\n",
      "Epoch 63 | Batch: 9 | Loss: 7.4148\n",
      "Epoch 63 | Batch: 10 | Loss: 12.7062\n",
      "Epoch 63 | Batch: 11 | Loss: 5.7511\n",
      "Epoch 63 | Batch: 12 | Loss: 7.6868\n",
      "Epoch 63 | Batch: 13 | Loss: 5.0006\n",
      "Epoch 63 | Batch: 14 | Loss: 10.1542\n",
      "Epoch 63 | Batch: 15 | Loss: 10.6238\n",
      "Epoch 63 | Batch: 16 | Loss: 6.5832\n",
      "Epoch 63 | Batch: 17 | Loss: 18.3117\n",
      "Epoch 63 | Batch: 18 | Loss: 10.2029\n",
      "Epoch 63 | Batch: 19 | Loss: 7.7150\n",
      "Epoch 63 | Batch: 20 | Loss: 5.7624\n",
      "Epoch 63 | Batch: 21 | Loss: 13.5277\n",
      "Epoch 63 | Batch: 22 | Loss: 15.5345\n",
      "Epoch 63 | Batch: 23 | Loss: 14.6718\n",
      "Epoch 63 | Batch: 24 | Loss: 5.5006\n",
      "Epoch 63 | Batch: 25 | Loss: 11.1626\n",
      "Epoch 63 | Batch: 26 | Loss: 8.6546\n",
      "Epoch 63 | Batch: 27 | Loss: 10.3921\n",
      "Epoch 63 | Batch: 28 | Loss: 6.0054\n",
      "Epoch 63 | Batch: 29 | Loss: 7.5788\n",
      "Epoch 63 | Batch: 30 | Loss: 6.8246\n",
      "Epoch 63 | Batch: 31 | Loss: 5.5501\n",
      "Epoch 63 | Batch: 32 | Loss: 9.3658\n",
      "Epoch 63 | Batch: 33 | Loss: 6.3109\n",
      "Epoch 63 | Batch: 34 | Loss: 13.2500\n",
      "Epoch 63 | Batch: 35 | Loss: 12.8437\n",
      "Epoch 63 | Batch: 36 | Loss: 8.4370\n",
      "Epoch 63 | Batch: 37 | Loss: 9.3780\n",
      "Epoch 63 | Batch: 38 | Loss: 11.3008\n",
      "Epoch 63 | Batch: 39 | Loss: 10.1755\n",
      "Epoch 63 | Batch: 40 | Loss: 9.2303\n",
      "Epoch 63 | Batch: 41 | Loss: 8.0532\n",
      "Epoch 63 | Batch: 42 | Loss: 12.9491\n",
      "Epoch 63 | Batch: 43 | Loss: 12.6412\n",
      "Epoch 63 | Batch: 44 | Loss: 10.6909\n",
      "Epoch 63 | Batch: 45 | Loss: 6.5302\n",
      "Epoch 63 | Batch: 46 | Loss: 16.3470\n",
      "Epoch 63 | Batch: 47 | Loss: 7.6171\n",
      "Epoch 63 | Batch: 48 | Loss: 2.8457\n",
      "Mean 9.939471671978632\n",
      "Epoch 64 | Batch: 1 | Loss: 4.2444\n",
      "Epoch 64 | Batch: 2 | Loss: 6.7579\n",
      "Epoch 64 | Batch: 3 | Loss: 8.4608\n",
      "Epoch 64 | Batch: 4 | Loss: 14.3880\n",
      "Epoch 64 | Batch: 5 | Loss: 5.8856\n",
      "Epoch 64 | Batch: 6 | Loss: 4.6715\n",
      "Epoch 64 | Batch: 7 | Loss: 15.1810\n",
      "Epoch 64 | Batch: 8 | Loss: 10.6314\n",
      "Epoch 64 | Batch: 9 | Loss: 8.9342\n",
      "Epoch 64 | Batch: 10 | Loss: 10.2438\n",
      "Epoch 64 | Batch: 11 | Loss: 12.6490\n",
      "Epoch 64 | Batch: 12 | Loss: 11.1855\n",
      "Epoch 64 | Batch: 13 | Loss: 9.0750\n",
      "Epoch 64 | Batch: 14 | Loss: 9.8185\n",
      "Epoch 64 | Batch: 15 | Loss: 6.6445\n",
      "Epoch 64 | Batch: 16 | Loss: 5.6289\n",
      "Epoch 64 | Batch: 17 | Loss: 8.9525\n",
      "Epoch 64 | Batch: 18 | Loss: 11.0939\n",
      "Epoch 64 | Batch: 19 | Loss: 8.1614\n",
      "Epoch 64 | Batch: 20 | Loss: 11.5604\n",
      "Epoch 64 | Batch: 21 | Loss: 12.9207\n",
      "Epoch 64 | Batch: 22 | Loss: 6.9524\n",
      "Epoch 64 | Batch: 23 | Loss: 8.1258\n",
      "Epoch 64 | Batch: 24 | Loss: 17.4077\n",
      "Epoch 64 | Batch: 25 | Loss: 9.3839\n",
      "Epoch 64 | Batch: 26 | Loss: 7.1400\n",
      "Epoch 64 | Batch: 27 | Loss: 6.8870\n",
      "Epoch 64 | Batch: 28 | Loss: 11.7093\n",
      "Epoch 64 | Batch: 29 | Loss: 8.4690\n",
      "Epoch 64 | Batch: 30 | Loss: 4.1009\n",
      "Epoch 64 | Batch: 31 | Loss: 14.2209\n",
      "Epoch 64 | Batch: 32 | Loss: 18.5973\n",
      "Epoch 64 | Batch: 33 | Loss: 9.5806\n",
      "Epoch 64 | Batch: 34 | Loss: 10.1570\n",
      "Epoch 64 | Batch: 35 | Loss: 8.3794\n",
      "Epoch 64 | Batch: 36 | Loss: 6.6074\n",
      "Epoch 64 | Batch: 37 | Loss: 4.9872\n",
      "Epoch 64 | Batch: 38 | Loss: 15.3802\n",
      "Epoch 64 | Batch: 39 | Loss: 18.7171\n",
      "Epoch 64 | Batch: 40 | Loss: 24.4074\n",
      "Epoch 64 | Batch: 41 | Loss: 14.0039\n",
      "Epoch 64 | Batch: 42 | Loss: 5.2695\n",
      "Epoch 64 | Batch: 43 | Loss: 7.9253\n",
      "Epoch 64 | Batch: 44 | Loss: 11.1572\n",
      "Epoch 64 | Batch: 45 | Loss: 7.0742\n",
      "Epoch 64 | Batch: 46 | Loss: 4.6636\n",
      "Epoch 64 | Batch: 47 | Loss: 9.4238\n",
      "Epoch 64 | Batch: 48 | Loss: 6.8615\n",
      "Mean 9.88913548986117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 | Batch: 1 | Loss: 5.5731\n",
      "Epoch 65 | Batch: 2 | Loss: 7.0821\n",
      "Epoch 65 | Batch: 3 | Loss: 5.0282\n",
      "Epoch 65 | Batch: 4 | Loss: 5.7815\n",
      "Epoch 65 | Batch: 5 | Loss: 6.7842\n",
      "Epoch 65 | Batch: 6 | Loss: 8.9520\n",
      "Epoch 65 | Batch: 7 | Loss: 3.9803\n",
      "Epoch 65 | Batch: 8 | Loss: 12.6618\n",
      "Epoch 65 | Batch: 9 | Loss: 9.5653\n",
      "Epoch 65 | Batch: 10 | Loss: 13.5438\n",
      "Epoch 65 | Batch: 11 | Loss: 10.5465\n",
      "Epoch 65 | Batch: 12 | Loss: 5.9994\n",
      "Epoch 65 | Batch: 13 | Loss: 7.8757\n",
      "Epoch 65 | Batch: 14 | Loss: 8.6298\n",
      "Epoch 65 | Batch: 15 | Loss: 8.9071\n",
      "Epoch 65 | Batch: 16 | Loss: 10.1927\n",
      "Epoch 65 | Batch: 17 | Loss: 9.5792\n",
      "Epoch 65 | Batch: 18 | Loss: 12.6652\n",
      "Epoch 65 | Batch: 19 | Loss: 12.3108\n",
      "Epoch 65 | Batch: 20 | Loss: 19.1057\n",
      "Epoch 65 | Batch: 21 | Loss: 23.2245\n",
      "Epoch 65 | Batch: 22 | Loss: 14.6462\n",
      "Epoch 65 | Batch: 23 | Loss: 16.3865\n",
      "Epoch 65 | Batch: 24 | Loss: 6.5809\n",
      "Epoch 65 | Batch: 25 | Loss: 8.1970\n",
      "Epoch 65 | Batch: 26 | Loss: 9.9884\n",
      "Epoch 65 | Batch: 27 | Loss: 12.0414\n",
      "Epoch 65 | Batch: 28 | Loss: 4.0342\n",
      "Epoch 65 | Batch: 29 | Loss: 8.8484\n",
      "Epoch 65 | Batch: 30 | Loss: 3.9112\n",
      "Epoch 65 | Batch: 31 | Loss: 7.4665\n",
      "Epoch 65 | Batch: 32 | Loss: 9.5889\n",
      "Epoch 65 | Batch: 33 | Loss: 16.0000\n",
      "Epoch 65 | Batch: 34 | Loss: 8.8727\n",
      "Epoch 65 | Batch: 35 | Loss: 6.8971\n",
      "Epoch 65 | Batch: 36 | Loss: 13.9168\n",
      "Epoch 65 | Batch: 37 | Loss: 8.6264\n",
      "Epoch 65 | Batch: 38 | Loss: 8.3010\n",
      "Epoch 65 | Batch: 39 | Loss: 11.3985\n",
      "Epoch 65 | Batch: 40 | Loss: 8.9664\n",
      "Epoch 65 | Batch: 41 | Loss: 10.0339\n",
      "Epoch 65 | Batch: 42 | Loss: 13.5661\n",
      "Epoch 65 | Batch: 43 | Loss: 8.3099\n",
      "Epoch 65 | Batch: 44 | Loss: 10.1474\n",
      "Epoch 65 | Batch: 45 | Loss: 10.3482\n",
      "Epoch 65 | Batch: 46 | Loss: 9.0503\n",
      "Epoch 65 | Batch: 47 | Loss: 5.8885\n",
      "Epoch 65 | Batch: 48 | Loss: 4.0403\n",
      "Mean 9.667538474003473\n",
      "Epoch 66 | Batch: 1 | Loss: 7.9353\n",
      "Epoch 66 | Batch: 2 | Loss: 12.7174\n",
      "Epoch 66 | Batch: 3 | Loss: 9.0842\n",
      "Epoch 66 | Batch: 4 | Loss: 6.9674\n",
      "Epoch 66 | Batch: 5 | Loss: 9.0862\n",
      "Epoch 66 | Batch: 6 | Loss: 10.3392\n",
      "Epoch 66 | Batch: 7 | Loss: 14.5858\n",
      "Epoch 66 | Batch: 8 | Loss: 5.9345\n",
      "Epoch 66 | Batch: 9 | Loss: 6.6626\n",
      "Epoch 66 | Batch: 10 | Loss: 10.5426\n",
      "Epoch 66 | Batch: 11 | Loss: 7.6132\n",
      "Epoch 66 | Batch: 12 | Loss: 3.5649\n",
      "Epoch 66 | Batch: 13 | Loss: 21.2297\n",
      "Epoch 66 | Batch: 14 | Loss: 30.9976\n",
      "Epoch 66 | Batch: 15 | Loss: 8.2349\n",
      "Epoch 66 | Batch: 16 | Loss: 7.9064\n",
      "Epoch 66 | Batch: 17 | Loss: 11.8742\n",
      "Epoch 66 | Batch: 18 | Loss: 5.2246\n",
      "Epoch 66 | Batch: 19 | Loss: 10.4874\n",
      "Epoch 66 | Batch: 20 | Loss: 7.3860\n",
      "Epoch 66 | Batch: 21 | Loss: 8.1641\n",
      "Epoch 66 | Batch: 22 | Loss: 10.2574\n",
      "Epoch 66 | Batch: 23 | Loss: 9.5931\n",
      "Epoch 66 | Batch: 24 | Loss: 12.9176\n",
      "Epoch 66 | Batch: 25 | Loss: 7.5547\n",
      "Epoch 66 | Batch: 26 | Loss: 8.7286\n",
      "Epoch 66 | Batch: 27 | Loss: 7.9137\n",
      "Epoch 66 | Batch: 28 | Loss: 7.8224\n",
      "Epoch 66 | Batch: 29 | Loss: 10.2848\n",
      "Epoch 66 | Batch: 30 | Loss: 10.3669\n",
      "Epoch 66 | Batch: 31 | Loss: 10.3185\n",
      "Epoch 66 | Batch: 32 | Loss: 11.9447\n",
      "Epoch 66 | Batch: 33 | Loss: 3.9748\n",
      "Epoch 66 | Batch: 34 | Loss: 8.5962\n",
      "Epoch 66 | Batch: 35 | Loss: 22.9614\n",
      "Epoch 66 | Batch: 36 | Loss: 27.6740\n",
      "Epoch 66 | Batch: 37 | Loss: 8.9678\n",
      "Epoch 66 | Batch: 38 | Loss: 9.4573\n",
      "Epoch 66 | Batch: 39 | Loss: 9.8956\n",
      "Epoch 66 | Batch: 40 | Loss: 9.5726\n",
      "Epoch 66 | Batch: 41 | Loss: 19.0197\n",
      "Epoch 66 | Batch: 42 | Loss: 16.1075\n",
      "Epoch 66 | Batch: 43 | Loss: 10.0315\n",
      "Epoch 66 | Batch: 44 | Loss: 5.2904\n",
      "Epoch 66 | Batch: 45 | Loss: 10.4504\n",
      "Epoch 66 | Batch: 46 | Loss: 11.6325\n",
      "Epoch 66 | Batch: 47 | Loss: 16.7866\n",
      "Epoch 66 | Batch: 48 | Loss: 3.5560\n",
      "Mean 10.79614228506883\n",
      "Epoch 67 | Batch: 1 | Loss: 12.1617\n",
      "Epoch 67 | Batch: 2 | Loss: 20.3069\n",
      "Epoch 67 | Batch: 3 | Loss: 19.0136\n",
      "Epoch 67 | Batch: 4 | Loss: 24.0134\n",
      "Epoch 67 | Batch: 5 | Loss: 21.8477\n",
      "Epoch 67 | Batch: 6 | Loss: 6.1958\n",
      "Epoch 67 | Batch: 7 | Loss: 10.3099\n",
      "Epoch 67 | Batch: 8 | Loss: 6.7545\n",
      "Epoch 67 | Batch: 9 | Loss: 3.7708\n",
      "Epoch 67 | Batch: 10 | Loss: 9.5806\n",
      "Epoch 67 | Batch: 11 | Loss: 11.6704\n",
      "Epoch 67 | Batch: 12 | Loss: 5.0884\n",
      "Epoch 67 | Batch: 13 | Loss: 7.1963\n",
      "Epoch 67 | Batch: 14 | Loss: 5.9663\n",
      "Epoch 67 | Batch: 15 | Loss: 15.1934\n",
      "Epoch 67 | Batch: 16 | Loss: 18.9917\n",
      "Epoch 67 | Batch: 17 | Loss: 11.3165\n",
      "Epoch 67 | Batch: 18 | Loss: 4.6942\n",
      "Epoch 67 | Batch: 19 | Loss: 7.1134\n",
      "Epoch 67 | Batch: 20 | Loss: 8.1643\n",
      "Epoch 67 | Batch: 21 | Loss: 6.9407\n",
      "Epoch 67 | Batch: 22 | Loss: 7.8318\n",
      "Epoch 67 | Batch: 23 | Loss: 8.5070\n",
      "Epoch 67 | Batch: 24 | Loss: 4.8707\n",
      "Epoch 67 | Batch: 25 | Loss: 7.4354\n",
      "Epoch 67 | Batch: 26 | Loss: 3.5865\n",
      "Epoch 67 | Batch: 27 | Loss: 12.1020\n",
      "Epoch 67 | Batch: 28 | Loss: 13.8880\n",
      "Epoch 67 | Batch: 29 | Loss: 5.5036\n",
      "Epoch 67 | Batch: 30 | Loss: 17.2944\n",
      "Epoch 67 | Batch: 31 | Loss: 9.6736\n",
      "Epoch 67 | Batch: 32 | Loss: 3.1607\n",
      "Epoch 67 | Batch: 33 | Loss: 5.8280\n",
      "Epoch 67 | Batch: 34 | Loss: 3.7587\n",
      "Epoch 67 | Batch: 35 | Loss: 6.5792\n",
      "Epoch 67 | Batch: 36 | Loss: 11.1012\n",
      "Epoch 67 | Batch: 37 | Loss: 4.5336\n",
      "Epoch 67 | Batch: 38 | Loss: 12.5385\n",
      "Epoch 67 | Batch: 39 | Loss: 9.1397\n",
      "Epoch 67 | Batch: 40 | Loss: 3.5692\n",
      "Epoch 67 | Batch: 41 | Loss: 9.7576\n",
      "Epoch 67 | Batch: 42 | Loss: 6.9198\n",
      "Epoch 67 | Batch: 43 | Loss: 16.4643\n",
      "Epoch 67 | Batch: 44 | Loss: 9.5794\n",
      "Epoch 67 | Batch: 45 | Loss: 13.5609\n",
      "Epoch 67 | Batch: 46 | Loss: 15.8833\n",
      "Epoch 67 | Batch: 47 | Loss: 25.9716\n",
      "Epoch 67 | Batch: 48 | Loss: 22.3943\n",
      "Mean 10.577578365802765\n",
      "Epoch 68 | Batch: 1 | Loss: 5.3539\n",
      "Epoch 68 | Batch: 2 | Loss: 8.7556\n",
      "Epoch 68 | Batch: 3 | Loss: 11.1005\n",
      "Epoch 68 | Batch: 4 | Loss: 6.2325\n",
      "Epoch 68 | Batch: 5 | Loss: 12.8371\n",
      "Epoch 68 | Batch: 6 | Loss: 8.8748\n",
      "Epoch 68 | Batch: 7 | Loss: 5.4784\n",
      "Epoch 68 | Batch: 8 | Loss: 13.1122\n",
      "Epoch 68 | Batch: 9 | Loss: 7.6961\n",
      "Epoch 68 | Batch: 10 | Loss: 8.2763\n",
      "Epoch 68 | Batch: 11 | Loss: 6.4202\n",
      "Epoch 68 | Batch: 12 | Loss: 3.7428\n",
      "Epoch 68 | Batch: 13 | Loss: 5.5789\n",
      "Epoch 68 | Batch: 14 | Loss: 10.3809\n",
      "Epoch 68 | Batch: 15 | Loss: 7.6573\n",
      "Epoch 68 | Batch: 16 | Loss: 4.8386\n",
      "Epoch 68 | Batch: 17 | Loss: 20.4480\n",
      "Epoch 68 | Batch: 18 | Loss: 0.1219\n",
      "Epoch 68 | Batch: 19 | Loss: 42.1725\n",
      "Epoch 68 | Batch: 20 | Loss: 8.9651\n",
      "Epoch 68 | Batch: 21 | Loss: 10.6728\n",
      "Epoch 68 | Batch: 22 | Loss: 12.1048\n",
      "Epoch 68 | Batch: 23 | Loss: 4.3677\n",
      "Epoch 68 | Batch: 24 | Loss: 6.1671\n",
      "Epoch 68 | Batch: 25 | Loss: 6.5482\n",
      "Epoch 68 | Batch: 26 | Loss: 12.1823\n",
      "Epoch 68 | Batch: 27 | Loss: 18.5139\n",
      "Epoch 68 | Batch: 28 | Loss: 7.5877\n",
      "Epoch 68 | Batch: 29 | Loss: 10.8529\n",
      "Epoch 68 | Batch: 30 | Loss: 9.0524\n",
      "Epoch 68 | Batch: 31 | Loss: 7.8804\n",
      "Epoch 68 | Batch: 32 | Loss: 9.0340\n",
      "Epoch 68 | Batch: 33 | Loss: 12.3107\n",
      "Epoch 68 | Batch: 34 | Loss: 15.2959\n",
      "Epoch 68 | Batch: 35 | Loss: 8.7379\n",
      "Epoch 68 | Batch: 36 | Loss: 6.5553\n",
      "Epoch 68 | Batch: 37 | Loss: 8.8036\n",
      "Epoch 68 | Batch: 38 | Loss: 3.5702\n",
      "Epoch 68 | Batch: 39 | Loss: 12.1091\n",
      "Epoch 68 | Batch: 40 | Loss: 10.3797\n",
      "Epoch 68 | Batch: 41 | Loss: 4.9259\n",
      "Epoch 68 | Batch: 42 | Loss: 7.9396\n",
      "Epoch 68 | Batch: 43 | Loss: 6.2994\n",
      "Epoch 68 | Batch: 44 | Loss: 7.9936\n",
      "Epoch 68 | Batch: 45 | Loss: 10.3566\n",
      "Epoch 68 | Batch: 46 | Loss: 7.8242\n",
      "Epoch 68 | Batch: 47 | Loss: 9.6553\n",
      "Epoch 68 | Batch: 48 | Loss: 3.5999\n",
      "Mean 9.36176527136316\n",
      "Epoch 69 | Batch: 1 | Loss: 7.9870\n",
      "Epoch 69 | Batch: 2 | Loss: 9.7104\n",
      "Epoch 69 | Batch: 3 | Loss: 8.4154\n",
      "Epoch 69 | Batch: 4 | Loss: 9.9507\n",
      "Epoch 69 | Batch: 5 | Loss: 26.8047\n",
      "Epoch 69 | Batch: 6 | Loss: 8.6066\n",
      "Epoch 69 | Batch: 7 | Loss: 10.1938\n",
      "Epoch 69 | Batch: 8 | Loss: 9.0229\n",
      "Epoch 69 | Batch: 9 | Loss: 5.8010\n",
      "Epoch 69 | Batch: 10 | Loss: 8.3990\n",
      "Epoch 69 | Batch: 11 | Loss: 12.9142\n",
      "Epoch 69 | Batch: 12 | Loss: 11.9269\n",
      "Epoch 69 | Batch: 13 | Loss: 7.4786\n",
      "Epoch 69 | Batch: 14 | Loss: 6.7813\n",
      "Epoch 69 | Batch: 15 | Loss: 14.5298\n",
      "Epoch 69 | Batch: 16 | Loss: 9.9091\n",
      "Epoch 69 | Batch: 17 | Loss: 10.6142\n",
      "Epoch 69 | Batch: 18 | Loss: 10.4658\n",
      "Epoch 69 | Batch: 19 | Loss: 7.0865\n",
      "Epoch 69 | Batch: 20 | Loss: 8.8475\n",
      "Epoch 69 | Batch: 21 | Loss: 7.5053\n",
      "Epoch 69 | Batch: 22 | Loss: 8.1110\n",
      "Epoch 69 | Batch: 23 | Loss: 7.3395\n",
      "Epoch 69 | Batch: 24 | Loss: 5.5659\n",
      "Epoch 69 | Batch: 25 | Loss: 6.0480\n",
      "Epoch 69 | Batch: 26 | Loss: 10.4219\n",
      "Epoch 69 | Batch: 27 | Loss: 7.3919\n",
      "Epoch 69 | Batch: 28 | Loss: 12.5946\n",
      "Epoch 69 | Batch: 29 | Loss: 18.0675\n",
      "Epoch 69 | Batch: 30 | Loss: 32.1583\n",
      "Epoch 69 | Batch: 31 | Loss: 16.8918\n",
      "Epoch 69 | Batch: 32 | Loss: 33.8465\n",
      "Epoch 69 | Batch: 33 | Loss: 9.5646\n",
      "Epoch 69 | Batch: 34 | Loss: 7.6085\n",
      "Epoch 69 | Batch: 35 | Loss: 6.4584\n",
      "Epoch 69 | Batch: 36 | Loss: 3.9552\n",
      "Epoch 69 | Batch: 37 | Loss: 5.5889\n",
      "Epoch 69 | Batch: 38 | Loss: 8.4215\n",
      "Epoch 69 | Batch: 39 | Loss: 5.2429\n",
      "Epoch 69 | Batch: 40 | Loss: 7.3004\n",
      "Epoch 69 | Batch: 41 | Loss: 7.4782\n",
      "Epoch 69 | Batch: 42 | Loss: 8.9543\n",
      "Epoch 69 | Batch: 43 | Loss: 10.1293\n",
      "Epoch 69 | Batch: 44 | Loss: 5.0068\n",
      "Epoch 69 | Batch: 45 | Loss: 8.4769\n",
      "Epoch 69 | Batch: 46 | Loss: 5.2306\n",
      "Epoch 69 | Batch: 47 | Loss: 15.1648\n",
      "Epoch 69 | Batch: 48 | Loss: 5.4604\n",
      "Mean 10.238107999165853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70 | Batch: 1 | Loss: 3.1279\n",
      "Epoch 70 | Batch: 2 | Loss: 7.9955\n",
      "Epoch 70 | Batch: 3 | Loss: 5.1610\n",
      "Epoch 70 | Batch: 4 | Loss: 8.2981\n",
      "Epoch 70 | Batch: 5 | Loss: 5.5910\n",
      "Epoch 70 | Batch: 6 | Loss: 13.9416\n",
      "Epoch 70 | Batch: 7 | Loss: 28.0038\n",
      "Epoch 70 | Batch: 8 | Loss: 10.9676\n",
      "Epoch 70 | Batch: 9 | Loss: 9.0480\n",
      "Epoch 70 | Batch: 10 | Loss: 6.2817\n",
      "Epoch 70 | Batch: 11 | Loss: 7.3423\n",
      "Epoch 70 | Batch: 12 | Loss: 11.8216\n",
      "Epoch 70 | Batch: 13 | Loss: 16.3484\n",
      "Epoch 70 | Batch: 14 | Loss: 13.2818\n",
      "Epoch 70 | Batch: 15 | Loss: 14.4113\n",
      "Epoch 70 | Batch: 16 | Loss: 5.9646\n",
      "Epoch 70 | Batch: 17 | Loss: 8.2824\n",
      "Epoch 70 | Batch: 18 | Loss: 6.5660\n",
      "Epoch 70 | Batch: 19 | Loss: 4.8549\n",
      "Epoch 70 | Batch: 20 | Loss: 7.5022\n",
      "Epoch 70 | Batch: 21 | Loss: 14.0234\n",
      "Epoch 70 | Batch: 22 | Loss: 5.6366\n",
      "Epoch 70 | Batch: 23 | Loss: 4.9332\n",
      "Epoch 70 | Batch: 24 | Loss: 6.9515\n",
      "Epoch 70 | Batch: 25 | Loss: 8.5569\n",
      "Epoch 70 | Batch: 26 | Loss: 7.3135\n",
      "Epoch 70 | Batch: 27 | Loss: 4.4888\n",
      "Epoch 70 | Batch: 28 | Loss: 4.3175\n",
      "Epoch 70 | Batch: 29 | Loss: 8.1303\n",
      "Epoch 70 | Batch: 30 | Loss: 9.8531\n",
      "Epoch 70 | Batch: 31 | Loss: 6.8809\n",
      "Epoch 70 | Batch: 32 | Loss: 3.2174\n",
      "Epoch 70 | Batch: 33 | Loss: 9.5582\n",
      "Epoch 70 | Batch: 34 | Loss: 21.1506\n",
      "Epoch 70 | Batch: 35 | Loss: 11.8432\n",
      "Epoch 70 | Batch: 36 | Loss: 12.1570\n",
      "Epoch 70 | Batch: 37 | Loss: 9.1535\n",
      "Epoch 70 | Batch: 38 | Loss: 11.3481\n",
      "Epoch 70 | Batch: 39 | Loss: 6.9242\n",
      "Epoch 70 | Batch: 40 | Loss: 6.1820\n",
      "Epoch 70 | Batch: 41 | Loss: 7.0880\n",
      "Epoch 70 | Batch: 42 | Loss: 14.1925\n",
      "Epoch 70 | Batch: 43 | Loss: 9.3910\n",
      "Epoch 70 | Batch: 44 | Loss: 8.3959\n",
      "Epoch 70 | Batch: 45 | Loss: 14.2087\n",
      "Epoch 70 | Batch: 46 | Loss: 17.1614\n",
      "Epoch 70 | Batch: 47 | Loss: 24.6657\n",
      "Epoch 70 | Batch: 48 | Loss: 3.5508\n",
      "Mean 9.70969962577025\n",
      "Epoch 71 | Batch: 1 | Loss: 4.2295\n",
      "Epoch 71 | Batch: 2 | Loss: 5.9662\n",
      "Epoch 71 | Batch: 3 | Loss: 4.9741\n",
      "Epoch 71 | Batch: 4 | Loss: 7.5954\n",
      "Epoch 71 | Batch: 5 | Loss: 6.8082\n",
      "Epoch 71 | Batch: 6 | Loss: 4.5713\n",
      "Epoch 71 | Batch: 7 | Loss: 13.2965\n",
      "Epoch 71 | Batch: 8 | Loss: 16.8226\n",
      "Epoch 71 | Batch: 9 | Loss: 9.0535\n",
      "Epoch 71 | Batch: 10 | Loss: 17.8326\n",
      "Epoch 71 | Batch: 11 | Loss: 12.6520\n",
      "Epoch 71 | Batch: 12 | Loss: 10.4405\n",
      "Epoch 71 | Batch: 13 | Loss: 12.3803\n",
      "Epoch 71 | Batch: 14 | Loss: 26.6863\n",
      "Epoch 71 | Batch: 15 | Loss: 16.6200\n",
      "Epoch 71 | Batch: 16 | Loss: 12.9549\n",
      "Epoch 71 | Batch: 17 | Loss: 15.2520\n",
      "Epoch 71 | Batch: 18 | Loss: 17.0042\n",
      "Epoch 71 | Batch: 19 | Loss: 12.2544\n",
      "Epoch 71 | Batch: 20 | Loss: 4.3685\n",
      "Epoch 71 | Batch: 21 | Loss: 5.5108\n",
      "Epoch 71 | Batch: 22 | Loss: 15.8004\n",
      "Epoch 71 | Batch: 23 | Loss: 10.6488\n",
      "Epoch 71 | Batch: 24 | Loss: 9.5382\n",
      "Epoch 71 | Batch: 25 | Loss: 12.1866\n",
      "Epoch 71 | Batch: 26 | Loss: 12.9603\n",
      "Epoch 71 | Batch: 27 | Loss: 12.2152\n",
      "Epoch 71 | Batch: 28 | Loss: 7.6740\n",
      "Epoch 71 | Batch: 29 | Loss: 6.6565\n",
      "Epoch 71 | Batch: 30 | Loss: 6.5967\n",
      "Epoch 71 | Batch: 31 | Loss: 6.2899\n",
      "Epoch 71 | Batch: 32 | Loss: 6.8466\n",
      "Epoch 71 | Batch: 33 | Loss: 7.9470\n",
      "Epoch 71 | Batch: 34 | Loss: 11.9250\n",
      "Epoch 71 | Batch: 35 | Loss: 11.9737\n",
      "Epoch 71 | Batch: 36 | Loss: 2.6900\n",
      "Epoch 71 | Batch: 37 | Loss: 5.3319\n",
      "Epoch 71 | Batch: 38 | Loss: 13.1778\n",
      "Epoch 71 | Batch: 39 | Loss: 30.1575\n",
      "Epoch 71 | Batch: 40 | Loss: 19.9368\n",
      "Epoch 71 | Batch: 41 | Loss: 19.0026\n",
      "Epoch 71 | Batch: 42 | Loss: 11.2767\n",
      "Epoch 71 | Batch: 43 | Loss: 9.5562\n",
      "Epoch 71 | Batch: 44 | Loss: 9.1403\n",
      "Epoch 71 | Batch: 45 | Loss: 7.3591\n",
      "Epoch 71 | Batch: 46 | Loss: 9.2506\n",
      "Epoch 71 | Batch: 47 | Loss: 13.9012\n",
      "Epoch 71 | Batch: 48 | Loss: 1.8453\n",
      "Mean 11.024141184985638\n",
      "Epoch 72 | Batch: 1 | Loss: 10.6811\n",
      "Epoch 72 | Batch: 2 | Loss: 18.1616\n",
      "Epoch 72 | Batch: 3 | Loss: 14.4964\n",
      "Epoch 72 | Batch: 4 | Loss: 6.8420\n",
      "Epoch 72 | Batch: 5 | Loss: 7.5884\n",
      "Epoch 72 | Batch: 6 | Loss: 6.1123\n",
      "Epoch 72 | Batch: 7 | Loss: 4.3160\n",
      "Epoch 72 | Batch: 8 | Loss: 7.9675\n",
      "Epoch 72 | Batch: 9 | Loss: 10.9418\n",
      "Epoch 72 | Batch: 10 | Loss: 6.3162\n",
      "Epoch 72 | Batch: 11 | Loss: 8.8373\n",
      "Epoch 72 | Batch: 12 | Loss: 12.2568\n",
      "Epoch 72 | Batch: 13 | Loss: 4.3034\n",
      "Epoch 72 | Batch: 14 | Loss: 6.7740\n",
      "Epoch 72 | Batch: 15 | Loss: 3.5074\n",
      "Epoch 72 | Batch: 16 | Loss: 9.5607\n",
      "Epoch 72 | Batch: 17 | Loss: 8.4053\n",
      "Epoch 72 | Batch: 18 | Loss: 13.5938\n",
      "Epoch 72 | Batch: 19 | Loss: 8.0550\n",
      "Epoch 72 | Batch: 20 | Loss: 4.9341\n",
      "Epoch 72 | Batch: 21 | Loss: 17.4935\n",
      "Epoch 72 | Batch: 22 | Loss: 22.6823\n",
      "Epoch 72 | Batch: 23 | Loss: 27.8713\n",
      "Epoch 72 | Batch: 24 | Loss: 18.0793\n",
      "Epoch 72 | Batch: 25 | Loss: 5.6221\n",
      "Epoch 72 | Batch: 26 | Loss: 9.0102\n",
      "Epoch 72 | Batch: 27 | Loss: 8.8190\n",
      "Epoch 72 | Batch: 28 | Loss: 7.7658\n",
      "Epoch 72 | Batch: 29 | Loss: 4.9934\n",
      "Epoch 72 | Batch: 30 | Loss: 11.3437\n",
      "Epoch 72 | Batch: 31 | Loss: 8.5968\n",
      "Epoch 72 | Batch: 32 | Loss: 11.0756\n",
      "Epoch 72 | Batch: 33 | Loss: 8.2367\n",
      "Epoch 72 | Batch: 34 | Loss: 11.8506\n",
      "Epoch 72 | Batch: 35 | Loss: 3.4153\n",
      "Epoch 72 | Batch: 36 | Loss: 10.4712\n",
      "Epoch 72 | Batch: 37 | Loss: 9.7280\n",
      "Epoch 72 | Batch: 38 | Loss: 12.2202\n",
      "Epoch 72 | Batch: 39 | Loss: 9.3744\n",
      "Epoch 72 | Batch: 40 | Loss: 18.2263\n",
      "Epoch 72 | Batch: 41 | Loss: 17.9632\n",
      "Epoch 72 | Batch: 42 | Loss: 8.8984\n",
      "Epoch 72 | Batch: 43 | Loss: 5.0192\n",
      "Epoch 72 | Batch: 44 | Loss: 6.6083\n",
      "Epoch 72 | Batch: 45 | Loss: 4.9119\n",
      "Epoch 72 | Batch: 46 | Loss: 4.3206\n",
      "Epoch 72 | Batch: 47 | Loss: 15.3447\n",
      "Epoch 72 | Batch: 48 | Loss: 2.0462\n",
      "Mean 9.909152830640474\n",
      "Epoch 73 | Batch: 1 | Loss: 19.1591\n",
      "Epoch 73 | Batch: 2 | Loss: 15.8964\n",
      "Epoch 73 | Batch: 3 | Loss: 36.2750\n",
      "Epoch 73 | Batch: 4 | Loss: 20.2368\n",
      "Epoch 73 | Batch: 5 | Loss: 4.5158\n",
      "Epoch 73 | Batch: 6 | Loss: 11.1592\n",
      "Epoch 73 | Batch: 7 | Loss: 9.8456\n",
      "Epoch 73 | Batch: 8 | Loss: 21.6947\n",
      "Epoch 73 | Batch: 9 | Loss: 13.2725\n",
      "Epoch 73 | Batch: 10 | Loss: 13.0020\n",
      "Epoch 73 | Batch: 11 | Loss: 4.8475\n",
      "Epoch 73 | Batch: 12 | Loss: 6.1019\n",
      "Epoch 73 | Batch: 13 | Loss: 7.0765\n",
      "Epoch 73 | Batch: 14 | Loss: 7.8831\n",
      "Epoch 73 | Batch: 15 | Loss: 13.8884\n",
      "Epoch 73 | Batch: 16 | Loss: 3.5612\n",
      "Epoch 73 | Batch: 17 | Loss: 21.6322\n",
      "Epoch 73 | Batch: 18 | Loss: 14.7893\n",
      "Epoch 73 | Batch: 19 | Loss: 14.7890\n",
      "Epoch 73 | Batch: 20 | Loss: 10.4764\n",
      "Epoch 73 | Batch: 21 | Loss: 9.1689\n",
      "Epoch 73 | Batch: 22 | Loss: 6.1295\n",
      "Epoch 73 | Batch: 23 | Loss: 6.0465\n",
      "Epoch 73 | Batch: 24 | Loss: 5.9303\n",
      "Epoch 73 | Batch: 25 | Loss: 9.0511\n",
      "Epoch 73 | Batch: 26 | Loss: 4.5394\n",
      "Epoch 73 | Batch: 27 | Loss: 9.8127\n",
      "Epoch 73 | Batch: 28 | Loss: 19.5806\n",
      "Epoch 73 | Batch: 29 | Loss: 16.5040\n",
      "Epoch 73 | Batch: 30 | Loss: 14.2229\n",
      "Epoch 73 | Batch: 31 | Loss: 14.7944\n",
      "Epoch 73 | Batch: 32 | Loss: 10.4546\n",
      "Epoch 73 | Batch: 33 | Loss: 7.1580\n",
      "Epoch 73 | Batch: 34 | Loss: 7.8511\n",
      "Epoch 73 | Batch: 35 | Loss: 9.7621\n",
      "Epoch 73 | Batch: 36 | Loss: 11.2924\n",
      "Epoch 73 | Batch: 37 | Loss: 10.9904\n",
      "Epoch 73 | Batch: 38 | Loss: 10.5785\n",
      "Epoch 73 | Batch: 39 | Loss: 10.1066\n",
      "Epoch 73 | Batch: 40 | Loss: 9.5746\n",
      "Epoch 73 | Batch: 41 | Loss: 5.0712\n",
      "Epoch 73 | Batch: 42 | Loss: 7.3656\n",
      "Epoch 73 | Batch: 43 | Loss: 7.5804\n",
      "Epoch 73 | Batch: 44 | Loss: 10.0545\n",
      "Epoch 73 | Batch: 45 | Loss: 10.5609\n",
      "Epoch 73 | Batch: 46 | Loss: 6.6400\n",
      "Epoch 73 | Batch: 47 | Loss: 7.0204\n",
      "Epoch 73 | Batch: 48 | Loss: 2.0832\n",
      "Mean 11.042238295078278\n",
      "Epoch 74 | Batch: 1 | Loss: 6.2192\n",
      "Epoch 74 | Batch: 2 | Loss: 13.5005\n",
      "Epoch 74 | Batch: 3 | Loss: 7.4130\n",
      "Epoch 74 | Batch: 4 | Loss: 14.8419\n",
      "Epoch 74 | Batch: 5 | Loss: 11.9800\n",
      "Epoch 74 | Batch: 6 | Loss: 13.0185\n",
      "Epoch 74 | Batch: 7 | Loss: 6.0798\n",
      "Epoch 74 | Batch: 8 | Loss: 8.8675\n",
      "Epoch 74 | Batch: 9 | Loss: 10.4109\n",
      "Epoch 74 | Batch: 10 | Loss: 9.2866\n",
      "Epoch 74 | Batch: 11 | Loss: 9.2478\n",
      "Epoch 74 | Batch: 12 | Loss: 15.5784\n",
      "Epoch 74 | Batch: 13 | Loss: 33.3705\n",
      "Epoch 74 | Batch: 14 | Loss: 22.1839\n",
      "Epoch 74 | Batch: 15 | Loss: 13.1458\n",
      "Epoch 74 | Batch: 16 | Loss: 12.1339\n",
      "Epoch 74 | Batch: 17 | Loss: 9.9833\n",
      "Epoch 74 | Batch: 18 | Loss: 8.3542\n",
      "Epoch 74 | Batch: 19 | Loss: 9.7938\n",
      "Epoch 74 | Batch: 20 | Loss: 12.3546\n",
      "Epoch 74 | Batch: 21 | Loss: 11.0354\n",
      "Epoch 74 | Batch: 22 | Loss: 7.4778\n",
      "Epoch 74 | Batch: 23 | Loss: 10.3681\n",
      "Epoch 74 | Batch: 24 | Loss: 12.3077\n",
      "Epoch 74 | Batch: 25 | Loss: 4.8296\n",
      "Epoch 74 | Batch: 26 | Loss: 7.9499\n",
      "Epoch 74 | Batch: 27 | Loss: 7.2730\n",
      "Epoch 74 | Batch: 28 | Loss: 6.4172\n",
      "Epoch 74 | Batch: 29 | Loss: 7.8288\n",
      "Epoch 74 | Batch: 30 | Loss: 5.0337\n",
      "Epoch 74 | Batch: 31 | Loss: 5.2721\n",
      "Epoch 74 | Batch: 32 | Loss: 9.2349\n",
      "Epoch 74 | Batch: 33 | Loss: 6.8931\n",
      "Epoch 74 | Batch: 34 | Loss: 12.3461\n",
      "Epoch 74 | Batch: 35 | Loss: 6.7616\n",
      "Epoch 74 | Batch: 36 | Loss: 9.8669\n",
      "Epoch 74 | Batch: 37 | Loss: 9.6354\n",
      "Epoch 74 | Batch: 38 | Loss: 9.9105\n",
      "Epoch 74 | Batch: 39 | Loss: 16.0324\n",
      "Epoch 74 | Batch: 40 | Loss: 9.6533\n",
      "Epoch 74 | Batch: 41 | Loss: 8.4666\n",
      "Epoch 74 | Batch: 42 | Loss: 16.7957\n",
      "Epoch 74 | Batch: 43 | Loss: 9.1649\n",
      "Epoch 74 | Batch: 44 | Loss: 3.7520\n",
      "Epoch 74 | Batch: 45 | Loss: 8.5122\n",
      "Epoch 74 | Batch: 46 | Loss: 14.9823\n",
      "Epoch 74 | Batch: 47 | Loss: 5.7524\n",
      "Epoch 74 | Batch: 48 | Loss: 4.2897\n",
      "Mean 10.325153316060701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 | Batch: 1 | Loss: 9.7146\n",
      "Epoch 75 | Batch: 2 | Loss: 5.2981\n",
      "Epoch 75 | Batch: 3 | Loss: 6.6309\n",
      "Epoch 75 | Batch: 4 | Loss: 7.4353\n",
      "Epoch 75 | Batch: 5 | Loss: 13.2624\n",
      "Epoch 75 | Batch: 6 | Loss: 8.8191\n",
      "Epoch 75 | Batch: 7 | Loss: 6.4242\n",
      "Epoch 75 | Batch: 8 | Loss: 4.1328\n",
      "Epoch 75 | Batch: 9 | Loss: 7.9200\n",
      "Epoch 75 | Batch: 10 | Loss: 19.0024\n",
      "Epoch 75 | Batch: 11 | Loss: 31.5746\n",
      "Epoch 75 | Batch: 12 | Loss: 14.6726\n",
      "Epoch 75 | Batch: 13 | Loss: 3.7252\n",
      "Epoch 75 | Batch: 14 | Loss: 8.4299\n",
      "Epoch 75 | Batch: 15 | Loss: 6.2036\n",
      "Epoch 75 | Batch: 16 | Loss: 3.8908\n",
      "Epoch 75 | Batch: 17 | Loss: 8.9753\n",
      "Epoch 75 | Batch: 18 | Loss: 9.6092\n",
      "Epoch 75 | Batch: 19 | Loss: 5.9265\n",
      "Epoch 75 | Batch: 20 | Loss: 7.3011\n",
      "Epoch 75 | Batch: 21 | Loss: 3.9273\n",
      "Epoch 75 | Batch: 22 | Loss: 2.6904\n",
      "Epoch 75 | Batch: 23 | Loss: 10.7595\n",
      "Epoch 75 | Batch: 24 | Loss: 24.3004\n",
      "Epoch 75 | Batch: 25 | Loss: 43.8147\n",
      "Epoch 75 | Batch: 26 | Loss: 7.8091\n",
      "Epoch 75 | Batch: 27 | Loss: 11.1596\n",
      "Epoch 75 | Batch: 28 | Loss: 12.6161\n",
      "Epoch 75 | Batch: 29 | Loss: 11.7737\n",
      "Epoch 75 | Batch: 30 | Loss: 7.5045\n",
      "Epoch 75 | Batch: 31 | Loss: 8.0931\n",
      "Epoch 75 | Batch: 32 | Loss: 12.6261\n",
      "Epoch 75 | Batch: 33 | Loss: 10.6366\n",
      "Epoch 75 | Batch: 34 | Loss: 8.1009\n",
      "Epoch 75 | Batch: 35 | Loss: 12.2525\n",
      "Epoch 75 | Batch: 36 | Loss: 15.2704\n",
      "Epoch 75 | Batch: 37 | Loss: 6.2682\n",
      "Epoch 75 | Batch: 38 | Loss: 6.6794\n",
      "Epoch 75 | Batch: 39 | Loss: 6.3370\n",
      "Epoch 75 | Batch: 40 | Loss: 11.1812\n",
      "Epoch 75 | Batch: 41 | Loss: 4.6973\n",
      "Epoch 75 | Batch: 42 | Loss: 7.5274\n",
      "Epoch 75 | Batch: 43 | Loss: 7.7730\n",
      "Epoch 75 | Batch: 44 | Loss: 25.3120\n",
      "Epoch 75 | Batch: 45 | Loss: 17.1807\n",
      "Epoch 75 | Batch: 46 | Loss: 11.7175\n",
      "Epoch 75 | Batch: 47 | Loss: 16.2046\n",
      "Epoch 75 | Batch: 48 | Loss: 10.2125\n",
      "Mean 10.903632844487825\n",
      "Epoch 76 | Batch: 1 | Loss: 21.1652\n",
      "Epoch 76 | Batch: 2 | Loss: 7.8366\n",
      "Epoch 76 | Batch: 3 | Loss: 9.7514\n",
      "Epoch 76 | Batch: 4 | Loss: 11.6325\n",
      "Epoch 76 | Batch: 5 | Loss: 14.0748\n",
      "Epoch 76 | Batch: 6 | Loss: 8.3137\n",
      "Epoch 76 | Batch: 7 | Loss: 10.1240\n",
      "Epoch 76 | Batch: 8 | Loss: 22.4851\n",
      "Epoch 76 | Batch: 9 | Loss: 20.0006\n",
      "Epoch 76 | Batch: 10 | Loss: 11.7858\n",
      "Epoch 76 | Batch: 11 | Loss: 9.1958\n",
      "Epoch 76 | Batch: 12 | Loss: 8.4485\n",
      "Epoch 76 | Batch: 13 | Loss: 8.6440\n",
      "Epoch 76 | Batch: 14 | Loss: 5.7822\n",
      "Epoch 76 | Batch: 15 | Loss: 9.9283\n",
      "Epoch 76 | Batch: 16 | Loss: 10.2772\n",
      "Epoch 76 | Batch: 17 | Loss: 9.5773\n",
      "Epoch 76 | Batch: 18 | Loss: 15.3513\n",
      "Epoch 76 | Batch: 19 | Loss: 26.5050\n",
      "Epoch 76 | Batch: 20 | Loss: 29.8599\n",
      "Epoch 76 | Batch: 21 | Loss: 11.5436\n",
      "Epoch 76 | Batch: 22 | Loss: 8.5712\n",
      "Epoch 76 | Batch: 23 | Loss: 7.5623\n",
      "Epoch 76 | Batch: 24 | Loss: 12.6127\n",
      "Epoch 76 | Batch: 25 | Loss: 26.4139\n",
      "Epoch 76 | Batch: 26 | Loss: 19.7433\n",
      "Epoch 76 | Batch: 27 | Loss: 5.3415\n",
      "Epoch 76 | Batch: 28 | Loss: 6.0607\n",
      "Epoch 76 | Batch: 29 | Loss: 11.8072\n",
      "Epoch 76 | Batch: 30 | Loss: 5.4633\n",
      "Epoch 76 | Batch: 31 | Loss: 13.0526\n",
      "Epoch 76 | Batch: 32 | Loss: 10.7172\n",
      "Epoch 76 | Batch: 33 | Loss: 4.3471\n",
      "Epoch 76 | Batch: 34 | Loss: 7.6062\n",
      "Epoch 76 | Batch: 35 | Loss: 6.6341\n",
      "Epoch 76 | Batch: 36 | Loss: 12.8236\n",
      "Epoch 76 | Batch: 37 | Loss: 7.4124\n",
      "Epoch 76 | Batch: 38 | Loss: 10.5502\n",
      "Epoch 76 | Batch: 39 | Loss: 20.7783\n",
      "Epoch 76 | Batch: 40 | Loss: 27.4846\n",
      "Epoch 76 | Batch: 41 | Loss: 8.2003\n",
      "Epoch 76 | Batch: 42 | Loss: 14.8515\n",
      "Epoch 76 | Batch: 43 | Loss: 3.1286\n",
      "Epoch 76 | Batch: 44 | Loss: 9.6829\n",
      "Epoch 76 | Batch: 45 | Loss: 6.5848\n",
      "Epoch 76 | Batch: 46 | Loss: 22.5883\n",
      "Epoch 76 | Batch: 47 | Loss: 8.1651\n",
      "Epoch 76 | Batch: 48 | Loss: 2.0475\n",
      "Mean 12.135706762472788\n",
      "Epoch 77 | Batch: 1 | Loss: 9.9458\n",
      "Epoch 77 | Batch: 2 | Loss: 6.2254\n",
      "Epoch 77 | Batch: 3 | Loss: 6.3084\n",
      "Epoch 77 | Batch: 4 | Loss: 15.1782\n",
      "Epoch 77 | Batch: 5 | Loss: 10.4009\n",
      "Epoch 77 | Batch: 6 | Loss: 3.5869\n",
      "Epoch 77 | Batch: 7 | Loss: 3.1976\n",
      "Epoch 77 | Batch: 8 | Loss: 11.2345\n",
      "Epoch 77 | Batch: 9 | Loss: 8.7912\n",
      "Epoch 77 | Batch: 10 | Loss: 6.1935\n",
      "Epoch 77 | Batch: 11 | Loss: 7.0682\n",
      "Epoch 77 | Batch: 12 | Loss: 8.4412\n",
      "Epoch 77 | Batch: 13 | Loss: 3.8405\n",
      "Epoch 77 | Batch: 14 | Loss: 6.6597\n",
      "Epoch 77 | Batch: 15 | Loss: 6.6598\n",
      "Epoch 77 | Batch: 16 | Loss: 7.5736\n",
      "Epoch 77 | Batch: 17 | Loss: 5.5976\n",
      "Epoch 77 | Batch: 18 | Loss: 16.4019\n",
      "Epoch 77 | Batch: 19 | Loss: 10.4382\n",
      "Epoch 77 | Batch: 20 | Loss: 10.9194\n",
      "Epoch 77 | Batch: 21 | Loss: 6.9074\n",
      "Epoch 77 | Batch: 22 | Loss: 7.2167\n",
      "Epoch 77 | Batch: 23 | Loss: 9.9000\n",
      "Epoch 77 | Batch: 24 | Loss: 5.5143\n",
      "Epoch 77 | Batch: 25 | Loss: 6.5418\n",
      "Epoch 77 | Batch: 26 | Loss: 10.6733\n",
      "Epoch 77 | Batch: 27 | Loss: 18.5774\n",
      "Epoch 77 | Batch: 28 | Loss: 31.4119\n",
      "Epoch 77 | Batch: 29 | Loss: 6.0925\n",
      "Epoch 77 | Batch: 30 | Loss: 8.0474\n",
      "Epoch 77 | Batch: 31 | Loss: 9.5050\n",
      "Epoch 77 | Batch: 32 | Loss: 8.6776\n",
      "Epoch 77 | Batch: 33 | Loss: 7.8925\n",
      "Epoch 77 | Batch: 34 | Loss: 7.2922\n",
      "Epoch 77 | Batch: 35 | Loss: 7.7075\n",
      "Epoch 77 | Batch: 36 | Loss: 10.1353\n",
      "Epoch 77 | Batch: 37 | Loss: 7.3122\n",
      "Epoch 77 | Batch: 38 | Loss: 6.9143\n",
      "Epoch 77 | Batch: 39 | Loss: 8.5704\n",
      "Epoch 77 | Batch: 40 | Loss: 6.8023\n",
      "Epoch 77 | Batch: 41 | Loss: 6.7103\n",
      "Epoch 77 | Batch: 42 | Loss: 4.6866\n",
      "Epoch 77 | Batch: 43 | Loss: 6.9923\n",
      "Epoch 77 | Batch: 44 | Loss: 14.5742\n",
      "Epoch 77 | Batch: 45 | Loss: 12.1865\n",
      "Epoch 77 | Batch: 46 | Loss: 10.3732\n",
      "Epoch 77 | Batch: 47 | Loss: 9.6481\n",
      "Epoch 77 | Batch: 48 | Loss: 3.1177\n",
      "Mean 8.846738914648691\n",
      "Epoch 78 | Batch: 1 | Loss: 7.8558\n",
      "Epoch 78 | Batch: 2 | Loss: 8.4500\n",
      "Epoch 78 | Batch: 3 | Loss: 5.1994\n",
      "Epoch 78 | Batch: 4 | Loss: 11.7659\n",
      "Epoch 78 | Batch: 5 | Loss: 9.3318\n",
      "Epoch 78 | Batch: 6 | Loss: 6.3075\n",
      "Epoch 78 | Batch: 7 | Loss: 15.8848\n",
      "Epoch 78 | Batch: 8 | Loss: 25.2127\n",
      "Epoch 78 | Batch: 9 | Loss: 31.0579\n",
      "Epoch 78 | Batch: 10 | Loss: 11.5559\n",
      "Epoch 78 | Batch: 11 | Loss: 15.3940\n",
      "Epoch 78 | Batch: 12 | Loss: 8.3029\n",
      "Epoch 78 | Batch: 13 | Loss: 6.4016\n",
      "Epoch 78 | Batch: 14 | Loss: 10.2375\n",
      "Epoch 78 | Batch: 15 | Loss: 6.7472\n",
      "Epoch 78 | Batch: 16 | Loss: 7.6362\n",
      "Epoch 78 | Batch: 17 | Loss: 8.3280\n",
      "Epoch 78 | Batch: 18 | Loss: 8.0983\n",
      "Epoch 78 | Batch: 19 | Loss: 5.9743\n",
      "Epoch 78 | Batch: 20 | Loss: 5.9243\n",
      "Epoch 78 | Batch: 21 | Loss: 7.7165\n",
      "Epoch 78 | Batch: 22 | Loss: 10.1107\n",
      "Epoch 78 | Batch: 23 | Loss: 6.0731\n",
      "Epoch 78 | Batch: 24 | Loss: 8.7202\n",
      "Epoch 78 | Batch: 25 | Loss: 8.5200\n",
      "Epoch 78 | Batch: 26 | Loss: 9.1960\n",
      "Epoch 78 | Batch: 27 | Loss: 8.0451\n",
      "Epoch 78 | Batch: 28 | Loss: 10.2342\n",
      "Epoch 78 | Batch: 29 | Loss: 4.0107\n",
      "Epoch 78 | Batch: 30 | Loss: 7.5610\n",
      "Epoch 78 | Batch: 31 | Loss: 17.7062\n",
      "Epoch 78 | Batch: 32 | Loss: 9.3268\n",
      "Epoch 78 | Batch: 33 | Loss: 6.8787\n",
      "Epoch 78 | Batch: 34 | Loss: 8.7796\n",
      "Epoch 78 | Batch: 35 | Loss: 5.2787\n",
      "Epoch 78 | Batch: 36 | Loss: 6.7160\n",
      "Epoch 78 | Batch: 37 | Loss: 5.1560\n",
      "Epoch 78 | Batch: 38 | Loss: 8.8432\n",
      "Epoch 78 | Batch: 39 | Loss: 8.9580\n",
      "Epoch 78 | Batch: 40 | Loss: 7.6243\n",
      "Epoch 78 | Batch: 41 | Loss: 8.8683\n",
      "Epoch 78 | Batch: 42 | Loss: 5.4240\n",
      "Epoch 78 | Batch: 43 | Loss: 4.8394\n",
      "Epoch 78 | Batch: 44 | Loss: 5.8768\n",
      "Epoch 78 | Batch: 45 | Loss: 8.6561\n",
      "Epoch 78 | Batch: 46 | Loss: 7.0098\n",
      "Epoch 78 | Batch: 47 | Loss: 5.3532\n",
      "Epoch 78 | Batch: 48 | Loss: 3.8236\n",
      "Mean 8.978585888942083\n",
      "Epoch 79 | Batch: 1 | Loss: 6.2190\n",
      "Epoch 79 | Batch: 2 | Loss: 6.5033\n",
      "Epoch 79 | Batch: 3 | Loss: 10.0232\n",
      "Epoch 79 | Batch: 4 | Loss: 9.2798\n",
      "Epoch 79 | Batch: 5 | Loss: 9.8246\n",
      "Epoch 79 | Batch: 6 | Loss: 12.4670\n",
      "Epoch 79 | Batch: 7 | Loss: 10.9427\n",
      "Epoch 79 | Batch: 8 | Loss: 5.0066\n",
      "Epoch 79 | Batch: 9 | Loss: 13.1729\n",
      "Epoch 79 | Batch: 10 | Loss: 20.0775\n",
      "Epoch 79 | Batch: 11 | Loss: 13.1407\n",
      "Epoch 79 | Batch: 12 | Loss: 5.7713\n",
      "Epoch 79 | Batch: 13 | Loss: 4.6301\n",
      "Epoch 79 | Batch: 14 | Loss: 7.8056\n",
      "Epoch 79 | Batch: 15 | Loss: 6.2127\n",
      "Epoch 79 | Batch: 16 | Loss: 5.8717\n",
      "Epoch 79 | Batch: 17 | Loss: 6.6531\n",
      "Epoch 79 | Batch: 18 | Loss: 20.2874\n",
      "Epoch 79 | Batch: 19 | Loss: 14.0134\n",
      "Epoch 79 | Batch: 20 | Loss: 18.2927\n",
      "Epoch 79 | Batch: 21 | Loss: 16.5212\n",
      "Epoch 79 | Batch: 22 | Loss: 14.1033\n",
      "Epoch 79 | Batch: 23 | Loss: 15.5742\n",
      "Epoch 79 | Batch: 24 | Loss: 10.4339\n",
      "Epoch 79 | Batch: 25 | Loss: 12.2871\n",
      "Epoch 79 | Batch: 26 | Loss: 14.1567\n",
      "Epoch 79 | Batch: 27 | Loss: 14.8438\n",
      "Epoch 79 | Batch: 28 | Loss: 13.7925\n",
      "Epoch 79 | Batch: 29 | Loss: 11.5721\n",
      "Epoch 79 | Batch: 30 | Loss: 12.7058\n",
      "Epoch 79 | Batch: 31 | Loss: 9.6852\n",
      "Epoch 79 | Batch: 32 | Loss: 10.2955\n",
      "Epoch 79 | Batch: 33 | Loss: 9.7226\n",
      "Epoch 79 | Batch: 34 | Loss: 9.7688\n",
      "Epoch 79 | Batch: 35 | Loss: 14.2738\n",
      "Epoch 79 | Batch: 36 | Loss: 16.0032\n",
      "Epoch 79 | Batch: 37 | Loss: 3.5223\n",
      "Epoch 79 | Batch: 38 | Loss: 11.2733\n",
      "Epoch 79 | Batch: 39 | Loss: 11.6051\n",
      "Epoch 79 | Batch: 40 | Loss: 11.2863\n",
      "Epoch 79 | Batch: 41 | Loss: 7.4860\n",
      "Epoch 79 | Batch: 42 | Loss: 13.1286\n",
      "Epoch 79 | Batch: 43 | Loss: 10.2992\n",
      "Epoch 79 | Batch: 44 | Loss: 6.7703\n",
      "Epoch 79 | Batch: 45 | Loss: 9.4575\n",
      "Epoch 79 | Batch: 46 | Loss: 13.1241\n",
      "Epoch 79 | Batch: 47 | Loss: 7.7330\n",
      "Epoch 79 | Batch: 48 | Loss: 1.0979\n",
      "Mean 10.806635409593582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 | Batch: 1 | Loss: 12.9676\n",
      "Epoch 80 | Batch: 2 | Loss: 8.5169\n",
      "Epoch 80 | Batch: 3 | Loss: 15.7761\n",
      "Epoch 80 | Batch: 4 | Loss: 26.6201\n",
      "Epoch 80 | Batch: 5 | Loss: 33.3167\n",
      "Epoch 80 | Batch: 6 | Loss: 19.2130\n",
      "Epoch 80 | Batch: 7 | Loss: 24.1390\n",
      "Epoch 80 | Batch: 8 | Loss: 9.6380\n",
      "Epoch 80 | Batch: 9 | Loss: 11.3230\n",
      "Epoch 80 | Batch: 10 | Loss: 11.7796\n",
      "Epoch 80 | Batch: 11 | Loss: 21.7431\n",
      "Epoch 80 | Batch: 12 | Loss: 10.1873\n",
      "Epoch 80 | Batch: 13 | Loss: 7.5243\n",
      "Epoch 80 | Batch: 14 | Loss: 8.9631\n",
      "Epoch 80 | Batch: 15 | Loss: 5.2403\n",
      "Epoch 80 | Batch: 16 | Loss: 7.4167\n",
      "Epoch 80 | Batch: 17 | Loss: 11.0675\n",
      "Epoch 80 | Batch: 18 | Loss: 16.0944\n",
      "Epoch 80 | Batch: 19 | Loss: 14.9518\n",
      "Epoch 80 | Batch: 20 | Loss: 8.9994\n",
      "Epoch 80 | Batch: 21 | Loss: 5.9637\n",
      "Epoch 80 | Batch: 22 | Loss: 5.1106\n",
      "Epoch 80 | Batch: 23 | Loss: 7.0602\n",
      "Epoch 80 | Batch: 24 | Loss: 10.3338\n",
      "Epoch 80 | Batch: 25 | Loss: 19.0384\n",
      "Epoch 80 | Batch: 26 | Loss: 5.1549\n",
      "Epoch 80 | Batch: 27 | Loss: 4.1927\n",
      "Epoch 80 | Batch: 28 | Loss: 3.8619\n",
      "Epoch 80 | Batch: 29 | Loss: 6.4410\n",
      "Epoch 80 | Batch: 30 | Loss: 13.6573\n",
      "Epoch 80 | Batch: 31 | Loss: 13.7844\n",
      "Epoch 80 | Batch: 32 | Loss: 11.7304\n",
      "Epoch 80 | Batch: 33 | Loss: 15.8255\n",
      "Epoch 80 | Batch: 34 | Loss: 11.5619\n",
      "Epoch 80 | Batch: 35 | Loss: 7.5751\n",
      "Epoch 80 | Batch: 36 | Loss: 6.3195\n",
      "Epoch 80 | Batch: 37 | Loss: 2.2548\n",
      "Epoch 80 | Batch: 38 | Loss: 14.2144\n",
      "Epoch 80 | Batch: 39 | Loss: 7.7293\n",
      "Epoch 80 | Batch: 40 | Loss: 8.2804\n",
      "Epoch 80 | Batch: 41 | Loss: 11.6206\n",
      "Epoch 80 | Batch: 42 | Loss: 4.9724\n",
      "Epoch 80 | Batch: 43 | Loss: 4.9204\n",
      "Epoch 80 | Batch: 44 | Loss: 21.8689\n",
      "Epoch 80 | Batch: 45 | Loss: 29.1795\n",
      "Epoch 80 | Batch: 46 | Loss: 27.2487\n",
      "Epoch 80 | Batch: 47 | Loss: 9.8527\n",
      "Epoch 80 | Batch: 48 | Loss: 1.9519\n",
      "Mean 12.024648432930311\n",
      "Epoch 81 | Batch: 1 | Loss: 3.9628\n",
      "Epoch 81 | Batch: 2 | Loss: 5.2151\n",
      "Epoch 81 | Batch: 3 | Loss: 16.6010\n",
      "Epoch 81 | Batch: 4 | Loss: 7.1395\n",
      "Epoch 81 | Batch: 5 | Loss: 5.1395\n",
      "Epoch 81 | Batch: 6 | Loss: 6.1269\n",
      "Epoch 81 | Batch: 7 | Loss: 3.8602\n",
      "Epoch 81 | Batch: 8 | Loss: 3.3595\n",
      "Epoch 81 | Batch: 9 | Loss: 17.8093\n",
      "Epoch 81 | Batch: 10 | Loss: 26.4850\n",
      "Epoch 81 | Batch: 11 | Loss: 17.1001\n",
      "Epoch 81 | Batch: 12 | Loss: 14.4019\n",
      "Epoch 81 | Batch: 13 | Loss: 7.0146\n",
      "Epoch 81 | Batch: 14 | Loss: 5.7889\n",
      "Epoch 81 | Batch: 15 | Loss: 7.0638\n",
      "Epoch 81 | Batch: 16 | Loss: 20.4560\n",
      "Epoch 81 | Batch: 17 | Loss: 17.0098\n",
      "Epoch 81 | Batch: 18 | Loss: 14.3826\n",
      "Epoch 81 | Batch: 19 | Loss: 8.2977\n",
      "Epoch 81 | Batch: 20 | Loss: 2.5249\n",
      "Epoch 81 | Batch: 21 | Loss: 8.3882\n",
      "Epoch 81 | Batch: 22 | Loss: 7.6760\n",
      "Epoch 81 | Batch: 23 | Loss: 7.9078\n",
      "Epoch 81 | Batch: 24 | Loss: 6.0748\n",
      "Epoch 81 | Batch: 25 | Loss: 9.9401\n",
      "Epoch 81 | Batch: 26 | Loss: 12.8742\n",
      "Epoch 81 | Batch: 27 | Loss: 7.0303\n",
      "Epoch 81 | Batch: 28 | Loss: 8.2205\n",
      "Epoch 81 | Batch: 29 | Loss: 10.7128\n",
      "Epoch 81 | Batch: 30 | Loss: 12.4206\n",
      "Epoch 81 | Batch: 31 | Loss: 18.0085\n",
      "Epoch 81 | Batch: 32 | Loss: 25.4803\n",
      "Epoch 81 | Batch: 33 | Loss: 4.4379\n",
      "Epoch 81 | Batch: 34 | Loss: 10.0904\n",
      "Epoch 81 | Batch: 35 | Loss: 8.4331\n",
      "Epoch 81 | Batch: 36 | Loss: 9.2897\n",
      "Epoch 81 | Batch: 37 | Loss: 15.3143\n",
      "Epoch 81 | Batch: 38 | Loss: 19.3635\n",
      "Epoch 81 | Batch: 39 | Loss: 8.2733\n",
      "Epoch 81 | Batch: 40 | Loss: 8.7001\n",
      "Epoch 81 | Batch: 41 | Loss: 9.3321\n",
      "Epoch 81 | Batch: 42 | Loss: 7.9242\n",
      "Epoch 81 | Batch: 43 | Loss: 8.8851\n",
      "Epoch 81 | Batch: 44 | Loss: 21.4337\n",
      "Epoch 81 | Batch: 45 | Loss: 15.3180\n",
      "Epoch 81 | Batch: 46 | Loss: 8.5985\n",
      "Epoch 81 | Batch: 47 | Loss: 7.6412\n",
      "Epoch 81 | Batch: 48 | Loss: 2.0481\n",
      "Mean 10.615757991870245\n",
      "Epoch 82 | Batch: 1 | Loss: 5.2569\n",
      "Epoch 82 | Batch: 2 | Loss: 8.1334\n",
      "Epoch 82 | Batch: 3 | Loss: 6.4250\n",
      "Epoch 82 | Batch: 4 | Loss: 12.1621\n",
      "Epoch 82 | Batch: 5 | Loss: 10.6697\n",
      "Epoch 82 | Batch: 6 | Loss: 9.3309\n",
      "Epoch 82 | Batch: 7 | Loss: 6.9211\n",
      "Epoch 82 | Batch: 8 | Loss: 7.9067\n",
      "Epoch 82 | Batch: 9 | Loss: 7.1156\n",
      "Epoch 82 | Batch: 10 | Loss: 4.6010\n",
      "Epoch 82 | Batch: 11 | Loss: 9.1897\n",
      "Epoch 82 | Batch: 12 | Loss: 7.0477\n",
      "Epoch 82 | Batch: 13 | Loss: 10.7477\n",
      "Epoch 82 | Batch: 14 | Loss: 11.2382\n",
      "Epoch 82 | Batch: 15 | Loss: 5.5600\n",
      "Epoch 82 | Batch: 16 | Loss: 12.4555\n",
      "Epoch 82 | Batch: 17 | Loss: 10.2408\n",
      "Epoch 82 | Batch: 18 | Loss: 10.5649\n",
      "Epoch 82 | Batch: 19 | Loss: 3.8847\n",
      "Epoch 82 | Batch: 20 | Loss: 7.7149\n",
      "Epoch 82 | Batch: 21 | Loss: 7.1349\n",
      "Epoch 82 | Batch: 22 | Loss: 4.4803\n",
      "Epoch 82 | Batch: 23 | Loss: 9.4200\n",
      "Epoch 82 | Batch: 24 | Loss: 4.4035\n",
      "Epoch 82 | Batch: 25 | Loss: 7.0274\n",
      "Epoch 82 | Batch: 26 | Loss: 18.5867\n",
      "Epoch 82 | Batch: 27 | Loss: 13.1810\n",
      "Epoch 82 | Batch: 28 | Loss: 12.8189\n",
      "Epoch 82 | Batch: 29 | Loss: 8.8135\n",
      "Epoch 82 | Batch: 30 | Loss: 5.8596\n",
      "Epoch 82 | Batch: 31 | Loss: 16.8591\n",
      "Epoch 82 | Batch: 32 | Loss: 7.1883\n",
      "Epoch 82 | Batch: 33 | Loss: 6.6509\n",
      "Epoch 82 | Batch: 34 | Loss: 13.6873\n",
      "Epoch 82 | Batch: 35 | Loss: 31.3380\n",
      "Epoch 82 | Batch: 36 | Loss: 17.0416\n",
      "Epoch 82 | Batch: 37 | Loss: 14.9464\n",
      "Epoch 82 | Batch: 38 | Loss: 6.9235\n",
      "Epoch 82 | Batch: 39 | Loss: 11.3795\n",
      "Epoch 82 | Batch: 40 | Loss: 5.7514\n",
      "Epoch 82 | Batch: 41 | Loss: 7.6974\n",
      "Epoch 82 | Batch: 42 | Loss: 8.3545\n",
      "Epoch 82 | Batch: 43 | Loss: 7.5994\n",
      "Epoch 82 | Batch: 44 | Loss: 6.5037\n",
      "Epoch 82 | Batch: 45 | Loss: 8.4367\n",
      "Epoch 82 | Batch: 46 | Loss: 8.2578\n",
      "Epoch 82 | Batch: 47 | Loss: 15.1943\n",
      "Epoch 82 | Batch: 48 | Loss: 2.4386\n",
      "Mean 9.482096244891485\n",
      "Epoch 83 | Batch: 1 | Loss: 13.4247\n",
      "Epoch 83 | Batch: 2 | Loss: 6.5368\n",
      "Epoch 83 | Batch: 3 | Loss: 2.0802\n",
      "Epoch 83 | Batch: 4 | Loss: 2.7775\n",
      "Epoch 83 | Batch: 5 | Loss: 6.0037\n",
      "Epoch 83 | Batch: 6 | Loss: 2.6298\n",
      "Epoch 83 | Batch: 7 | Loss: 12.1554\n",
      "Epoch 83 | Batch: 8 | Loss: 18.4707\n",
      "Epoch 83 | Batch: 9 | Loss: 10.6409\n",
      "Epoch 83 | Batch: 10 | Loss: 9.2755\n",
      "Epoch 83 | Batch: 11 | Loss: 6.3191\n",
      "Epoch 83 | Batch: 12 | Loss: 17.0417\n",
      "Epoch 83 | Batch: 13 | Loss: 18.0097\n",
      "Epoch 83 | Batch: 14 | Loss: 13.0731\n",
      "Epoch 83 | Batch: 15 | Loss: 11.7512\n",
      "Epoch 83 | Batch: 16 | Loss: 16.9476\n",
      "Epoch 83 | Batch: 17 | Loss: 12.4718\n",
      "Epoch 83 | Batch: 18 | Loss: 9.9669\n",
      "Epoch 83 | Batch: 19 | Loss: 9.3576\n",
      "Epoch 83 | Batch: 20 | Loss: 8.5808\n",
      "Epoch 83 | Batch: 21 | Loss: 10.2439\n",
      "Epoch 83 | Batch: 22 | Loss: 9.5441\n",
      "Epoch 83 | Batch: 23 | Loss: 5.8746\n",
      "Epoch 83 | Batch: 24 | Loss: 10.0490\n",
      "Epoch 83 | Batch: 25 | Loss: 8.3930\n",
      "Epoch 83 | Batch: 26 | Loss: 10.4617\n",
      "Epoch 83 | Batch: 27 | Loss: 7.7335\n",
      "Epoch 83 | Batch: 28 | Loss: 12.2863\n",
      "Epoch 83 | Batch: 29 | Loss: 15.7216\n",
      "Epoch 83 | Batch: 30 | Loss: 10.3388\n",
      "Epoch 83 | Batch: 31 | Loss: 15.6459\n",
      "Epoch 83 | Batch: 32 | Loss: 17.7608\n",
      "Epoch 83 | Batch: 33 | Loss: 9.3020\n",
      "Epoch 83 | Batch: 34 | Loss: 11.5274\n",
      "Epoch 83 | Batch: 35 | Loss: 16.2343\n",
      "Epoch 83 | Batch: 36 | Loss: 8.8408\n",
      "Epoch 83 | Batch: 37 | Loss: 7.7885\n",
      "Epoch 83 | Batch: 38 | Loss: 6.3025\n",
      "Epoch 83 | Batch: 39 | Loss: 6.2407\n",
      "Epoch 83 | Batch: 40 | Loss: 14.8680\n",
      "Epoch 83 | Batch: 41 | Loss: 7.0995\n",
      "Epoch 83 | Batch: 42 | Loss: 4.7306\n",
      "Epoch 83 | Batch: 43 | Loss: 4.2809\n",
      "Epoch 83 | Batch: 44 | Loss: 3.5990\n",
      "Epoch 83 | Batch: 45 | Loss: 11.9314\n",
      "Epoch 83 | Batch: 46 | Loss: 11.0596\n",
      "Epoch 83 | Batch: 47 | Loss: 5.4553\n",
      "Epoch 83 | Batch: 48 | Loss: 4.1892\n",
      "Mean 9.896194085478783\n",
      "Epoch 84 | Batch: 1 | Loss: 14.3242\n",
      "Epoch 84 | Batch: 2 | Loss: 10.3009\n",
      "Epoch 84 | Batch: 3 | Loss: 10.0532\n",
      "Epoch 84 | Batch: 4 | Loss: 3.6406\n",
      "Epoch 84 | Batch: 5 | Loss: 5.2333\n",
      "Epoch 84 | Batch: 6 | Loss: 6.8024\n",
      "Epoch 84 | Batch: 7 | Loss: 3.9740\n",
      "Epoch 84 | Batch: 8 | Loss: 11.4187\n",
      "Epoch 84 | Batch: 9 | Loss: 6.0002\n",
      "Epoch 84 | Batch: 10 | Loss: 8.9741\n",
      "Epoch 84 | Batch: 11 | Loss: 8.3506\n",
      "Epoch 84 | Batch: 12 | Loss: 8.4856\n",
      "Epoch 84 | Batch: 13 | Loss: 4.1910\n",
      "Epoch 84 | Batch: 14 | Loss: 8.8956\n",
      "Epoch 84 | Batch: 15 | Loss: 9.4291\n",
      "Epoch 84 | Batch: 16 | Loss: 3.6214\n",
      "Epoch 84 | Batch: 17 | Loss: 3.5386\n",
      "Epoch 84 | Batch: 18 | Loss: 7.2459\n",
      "Epoch 84 | Batch: 19 | Loss: 12.4749\n",
      "Epoch 84 | Batch: 20 | Loss: 6.7730\n",
      "Epoch 84 | Batch: 21 | Loss: 6.3266\n",
      "Epoch 84 | Batch: 22 | Loss: 9.3324\n",
      "Epoch 84 | Batch: 23 | Loss: 10.5804\n",
      "Epoch 84 | Batch: 24 | Loss: 17.6163\n",
      "Epoch 84 | Batch: 25 | Loss: 11.7023\n",
      "Epoch 84 | Batch: 26 | Loss: 5.9739\n",
      "Epoch 84 | Batch: 27 | Loss: 9.6614\n",
      "Epoch 84 | Batch: 28 | Loss: 8.4587\n",
      "Epoch 84 | Batch: 29 | Loss: 6.7016\n",
      "Epoch 84 | Batch: 30 | Loss: 5.0421\n",
      "Epoch 84 | Batch: 31 | Loss: 7.3181\n",
      "Epoch 84 | Batch: 32 | Loss: 9.5973\n",
      "Epoch 84 | Batch: 33 | Loss: 8.7141\n",
      "Epoch 84 | Batch: 34 | Loss: 20.6146\n",
      "Epoch 84 | Batch: 35 | Loss: 17.4819\n",
      "Epoch 84 | Batch: 36 | Loss: 10.2092\n",
      "Epoch 84 | Batch: 37 | Loss: 14.4336\n",
      "Epoch 84 | Batch: 38 | Loss: 18.3719\n",
      "Epoch 84 | Batch: 39 | Loss: 6.2889\n",
      "Epoch 84 | Batch: 40 | Loss: 7.3232\n",
      "Epoch 84 | Batch: 41 | Loss: 4.9571\n",
      "Epoch 84 | Batch: 42 | Loss: 6.3232\n",
      "Epoch 84 | Batch: 43 | Loss: 8.5159\n",
      "Epoch 84 | Batch: 44 | Loss: 8.1404\n",
      "Epoch 84 | Batch: 45 | Loss: 7.3676\n",
      "Epoch 84 | Batch: 46 | Loss: 6.0092\n",
      "Epoch 84 | Batch: 47 | Loss: 8.0674\n",
      "Epoch 84 | Batch: 48 | Loss: 6.7674\n",
      "Mean 8.783833637833595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85 | Batch: 1 | Loss: 8.0940\n",
      "Epoch 85 | Batch: 2 | Loss: 10.1110\n",
      "Epoch 85 | Batch: 3 | Loss: 6.2573\n",
      "Epoch 85 | Batch: 4 | Loss: 10.4649\n",
      "Epoch 85 | Batch: 5 | Loss: 9.3062\n",
      "Epoch 85 | Batch: 6 | Loss: 6.0465\n",
      "Epoch 85 | Batch: 7 | Loss: 9.5358\n",
      "Epoch 85 | Batch: 8 | Loss: 5.8394\n",
      "Epoch 85 | Batch: 9 | Loss: 9.1230\n",
      "Epoch 85 | Batch: 10 | Loss: 13.1605\n",
      "Epoch 85 | Batch: 11 | Loss: 6.0106\n",
      "Epoch 85 | Batch: 12 | Loss: 6.9499\n",
      "Epoch 85 | Batch: 13 | Loss: 7.2880\n",
      "Epoch 85 | Batch: 14 | Loss: 6.6584\n",
      "Epoch 85 | Batch: 15 | Loss: 12.2105\n",
      "Epoch 85 | Batch: 16 | Loss: 6.1453\n",
      "Epoch 85 | Batch: 17 | Loss: 11.8033\n",
      "Epoch 85 | Batch: 18 | Loss: 9.7424\n",
      "Epoch 85 | Batch: 19 | Loss: 6.1598\n",
      "Epoch 85 | Batch: 20 | Loss: 4.6510\n",
      "Epoch 85 | Batch: 21 | Loss: 11.6004\n",
      "Epoch 85 | Batch: 22 | Loss: 6.8383\n",
      "Epoch 85 | Batch: 23 | Loss: 5.4899\n",
      "Epoch 85 | Batch: 24 | Loss: 0.9823\n",
      "Epoch 85 | Batch: 25 | Loss: 7.8930\n",
      "Epoch 85 | Batch: 26 | Loss: 9.1883\n",
      "Epoch 85 | Batch: 27 | Loss: 5.9308\n",
      "Epoch 85 | Batch: 28 | Loss: 5.1122\n",
      "Epoch 85 | Batch: 29 | Loss: 8.2400\n",
      "Epoch 85 | Batch: 30 | Loss: 33.6092\n",
      "Epoch 85 | Batch: 31 | Loss: 15.3749\n",
      "Epoch 85 | Batch: 32 | Loss: 3.9500\n",
      "Epoch 85 | Batch: 33 | Loss: 5.4768\n",
      "Epoch 85 | Batch: 34 | Loss: 4.7344\n",
      "Epoch 85 | Batch: 35 | Loss: 7.6963\n",
      "Epoch 85 | Batch: 36 | Loss: 9.8004\n",
      "Epoch 85 | Batch: 37 | Loss: 4.3418\n",
      "Epoch 85 | Batch: 38 | Loss: 11.0033\n",
      "Epoch 85 | Batch: 39 | Loss: 10.4814\n",
      "Epoch 85 | Batch: 40 | Loss: 15.1410\n",
      "Epoch 85 | Batch: 41 | Loss: 9.9367\n",
      "Epoch 85 | Batch: 42 | Loss: 3.7792\n",
      "Epoch 85 | Batch: 43 | Loss: 11.6223\n",
      "Epoch 85 | Batch: 44 | Loss: 9.4534\n",
      "Epoch 85 | Batch: 45 | Loss: 22.5060\n",
      "Epoch 85 | Batch: 46 | Loss: 6.8336\n",
      "Epoch 85 | Batch: 47 | Loss: 7.9376\n",
      "Epoch 85 | Batch: 48 | Loss: 1.9274\n",
      "Mean 8.800803449004889\n",
      "Epoch 86 | Batch: 1 | Loss: 9.9579\n",
      "Epoch 86 | Batch: 2 | Loss: 14.0560\n",
      "Epoch 86 | Batch: 3 | Loss: 9.4479\n",
      "Epoch 86 | Batch: 4 | Loss: 4.0953\n",
      "Epoch 86 | Batch: 5 | Loss: 9.5583\n",
      "Epoch 86 | Batch: 6 | Loss: 9.0218\n",
      "Epoch 86 | Batch: 7 | Loss: 6.4768\n",
      "Epoch 86 | Batch: 8 | Loss: 5.4116\n",
      "Epoch 86 | Batch: 9 | Loss: 8.7287\n",
      "Epoch 86 | Batch: 10 | Loss: 17.1569\n",
      "Epoch 86 | Batch: 11 | Loss: 15.3300\n",
      "Epoch 86 | Batch: 12 | Loss: 5.6925\n",
      "Epoch 86 | Batch: 13 | Loss: 3.3136\n",
      "Epoch 86 | Batch: 14 | Loss: 2.8267\n",
      "Epoch 86 | Batch: 15 | Loss: 6.5870\n",
      "Epoch 86 | Batch: 16 | Loss: 7.7868\n",
      "Epoch 86 | Batch: 17 | Loss: 4.7304\n",
      "Epoch 86 | Batch: 18 | Loss: 12.2861\n",
      "Epoch 86 | Batch: 19 | Loss: 8.9834\n",
      "Epoch 86 | Batch: 20 | Loss: 11.5055\n",
      "Epoch 86 | Batch: 21 | Loss: 7.1617\n",
      "Epoch 86 | Batch: 22 | Loss: 4.8644\n",
      "Epoch 86 | Batch: 23 | Loss: 7.2140\n",
      "Epoch 86 | Batch: 24 | Loss: 4.2984\n",
      "Epoch 86 | Batch: 25 | Loss: 6.7421\n",
      "Epoch 86 | Batch: 26 | Loss: 5.9753\n",
      "Epoch 86 | Batch: 27 | Loss: 5.3682\n",
      "Epoch 86 | Batch: 28 | Loss: 9.7131\n",
      "Epoch 86 | Batch: 29 | Loss: 14.2454\n",
      "Epoch 86 | Batch: 30 | Loss: 23.6436\n",
      "Epoch 86 | Batch: 31 | Loss: 13.7079\n",
      "Epoch 86 | Batch: 32 | Loss: 10.8125\n",
      "Epoch 86 | Batch: 33 | Loss: 5.6830\n",
      "Epoch 86 | Batch: 34 | Loss: 11.9893\n",
      "Epoch 86 | Batch: 35 | Loss: 18.0678\n",
      "Epoch 86 | Batch: 36 | Loss: 39.5176\n",
      "Epoch 86 | Batch: 37 | Loss: 31.3716\n",
      "Epoch 86 | Batch: 38 | Loss: 11.0300\n",
      "Epoch 86 | Batch: 39 | Loss: 9.8503\n",
      "Epoch 86 | Batch: 40 | Loss: 7.5308\n",
      "Epoch 86 | Batch: 41 | Loss: 7.1824\n",
      "Epoch 86 | Batch: 42 | Loss: 9.7082\n",
      "Epoch 86 | Batch: 43 | Loss: 9.8221\n",
      "Epoch 86 | Batch: 44 | Loss: 9.6772\n",
      "Epoch 86 | Batch: 45 | Loss: 6.3578\n",
      "Epoch 86 | Batch: 46 | Loss: 5.8100\n",
      "Epoch 86 | Batch: 47 | Loss: 12.2006\n",
      "Epoch 86 | Batch: 48 | Loss: 11.1093\n",
      "Mean 10.283491979042688\n",
      "Epoch 87 | Batch: 1 | Loss: 10.5843\n",
      "Epoch 87 | Batch: 2 | Loss: 10.2244\n",
      "Epoch 87 | Batch: 3 | Loss: 11.7939\n",
      "Epoch 87 | Batch: 4 | Loss: 13.1604\n",
      "Epoch 87 | Batch: 5 | Loss: 15.3629\n",
      "Epoch 87 | Batch: 6 | Loss: 13.0903\n",
      "Epoch 87 | Batch: 7 | Loss: 9.3825\n",
      "Epoch 87 | Batch: 8 | Loss: 11.8468\n",
      "Epoch 87 | Batch: 9 | Loss: 14.2100\n",
      "Epoch 87 | Batch: 10 | Loss: 5.2510\n",
      "Epoch 87 | Batch: 11 | Loss: 6.1338\n",
      "Epoch 87 | Batch: 12 | Loss: 11.0578\n",
      "Epoch 87 | Batch: 13 | Loss: 4.4596\n",
      "Epoch 87 | Batch: 14 | Loss: 7.3928\n",
      "Epoch 87 | Batch: 15 | Loss: 9.8108\n",
      "Epoch 87 | Batch: 16 | Loss: 11.8832\n",
      "Epoch 87 | Batch: 17 | Loss: 19.9762\n",
      "Epoch 87 | Batch: 18 | Loss: 15.4530\n",
      "Epoch 87 | Batch: 19 | Loss: 8.1156\n",
      "Epoch 87 | Batch: 20 | Loss: 11.0387\n",
      "Epoch 87 | Batch: 21 | Loss: 9.6433\n",
      "Epoch 87 | Batch: 22 | Loss: 11.6092\n",
      "Epoch 87 | Batch: 23 | Loss: 13.2494\n",
      "Epoch 87 | Batch: 24 | Loss: 6.2135\n",
      "Epoch 87 | Batch: 25 | Loss: 7.6558\n",
      "Epoch 87 | Batch: 26 | Loss: 8.7465\n",
      "Epoch 87 | Batch: 27 | Loss: 3.9415\n",
      "Epoch 87 | Batch: 28 | Loss: 9.2352\n",
      "Epoch 87 | Batch: 29 | Loss: 10.4538\n",
      "Epoch 87 | Batch: 30 | Loss: 3.1234\n",
      "Epoch 87 | Batch: 31 | Loss: 12.0781\n",
      "Epoch 87 | Batch: 32 | Loss: 14.0531\n",
      "Epoch 87 | Batch: 33 | Loss: 6.9790\n",
      "Epoch 87 | Batch: 34 | Loss: 5.3017\n",
      "Epoch 87 | Batch: 35 | Loss: 5.6547\n",
      "Epoch 87 | Batch: 36 | Loss: 9.2543\n",
      "Epoch 87 | Batch: 37 | Loss: 8.5982\n",
      "Epoch 87 | Batch: 38 | Loss: 7.0959\n",
      "Epoch 87 | Batch: 39 | Loss: 7.3088\n",
      "Epoch 87 | Batch: 40 | Loss: 14.5291\n",
      "Epoch 87 | Batch: 41 | Loss: 13.6176\n",
      "Epoch 87 | Batch: 42 | Loss: 11.2164\n",
      "Epoch 87 | Batch: 43 | Loss: 11.3424\n",
      "Epoch 87 | Batch: 44 | Loss: 9.4625\n",
      "Epoch 87 | Batch: 45 | Loss: 4.7929\n",
      "Epoch 87 | Batch: 46 | Loss: 8.6893\n",
      "Epoch 87 | Batch: 47 | Loss: 4.8464\n",
      "Epoch 87 | Batch: 48 | Loss: 3.1693\n",
      "Mean 9.626857494314512\n",
      "Epoch 88 | Batch: 1 | Loss: 11.4697\n",
      "Epoch 88 | Batch: 2 | Loss: 2.1386\n",
      "Epoch 88 | Batch: 3 | Loss: 8.3204\n",
      "Epoch 88 | Batch: 4 | Loss: 4.6312\n",
      "Epoch 88 | Batch: 5 | Loss: 15.0108\n",
      "Epoch 88 | Batch: 6 | Loss: 21.9119\n",
      "Epoch 88 | Batch: 7 | Loss: 19.1951\n",
      "Epoch 88 | Batch: 8 | Loss: 12.3737\n",
      "Epoch 88 | Batch: 9 | Loss: 14.3244\n",
      "Epoch 88 | Batch: 10 | Loss: 10.5105\n",
      "Epoch 88 | Batch: 11 | Loss: 2.0194\n",
      "Epoch 88 | Batch: 12 | Loss: 10.1463\n",
      "Epoch 88 | Batch: 13 | Loss: 6.9865\n",
      "Epoch 88 | Batch: 14 | Loss: 10.9263\n",
      "Epoch 88 | Batch: 15 | Loss: 14.0817\n",
      "Epoch 88 | Batch: 16 | Loss: 7.8965\n",
      "Epoch 88 | Batch: 17 | Loss: 10.1522\n",
      "Epoch 88 | Batch: 18 | Loss: 10.5537\n",
      "Epoch 88 | Batch: 19 | Loss: 6.6057\n",
      "Epoch 88 | Batch: 20 | Loss: 10.5500\n",
      "Epoch 88 | Batch: 21 | Loss: 7.2725\n",
      "Epoch 88 | Batch: 22 | Loss: 9.6216\n",
      "Epoch 88 | Batch: 23 | Loss: 9.0470\n",
      "Epoch 88 | Batch: 24 | Loss: 8.3385\n",
      "Epoch 88 | Batch: 25 | Loss: 10.2078\n",
      "Epoch 88 | Batch: 26 | Loss: 8.0480\n",
      "Epoch 88 | Batch: 27 | Loss: 5.8008\n",
      "Epoch 88 | Batch: 28 | Loss: 9.9060\n",
      "Epoch 88 | Batch: 29 | Loss: 11.6029\n",
      "Epoch 88 | Batch: 30 | Loss: 6.9681\n",
      "Epoch 88 | Batch: 31 | Loss: 9.1323\n",
      "Epoch 88 | Batch: 32 | Loss: 10.0694\n",
      "Epoch 88 | Batch: 33 | Loss: 9.4954\n",
      "Epoch 88 | Batch: 34 | Loss: 7.4598\n",
      "Epoch 88 | Batch: 35 | Loss: 14.0744\n",
      "Epoch 88 | Batch: 36 | Loss: 6.2998\n",
      "Epoch 88 | Batch: 37 | Loss: 5.4751\n",
      "Epoch 88 | Batch: 38 | Loss: 2.9642\n",
      "Epoch 88 | Batch: 39 | Loss: 3.4905\n",
      "Epoch 88 | Batch: 40 | Loss: 7.7364\n",
      "Epoch 88 | Batch: 41 | Loss: 5.8062\n",
      "Epoch 88 | Batch: 42 | Loss: 5.9347\n",
      "Epoch 88 | Batch: 43 | Loss: 4.4060\n",
      "Epoch 88 | Batch: 44 | Loss: 5.5785\n",
      "Epoch 88 | Batch: 45 | Loss: 16.1025\n",
      "Epoch 88 | Batch: 46 | Loss: 15.4372\n",
      "Epoch 88 | Batch: 47 | Loss: 6.0181\n",
      "Epoch 88 | Batch: 48 | Loss: 2.3157\n",
      "Mean 9.0502859801054\n",
      "Epoch 89 | Batch: 1 | Loss: 7.6097\n",
      "Epoch 89 | Batch: 2 | Loss: 10.2209\n",
      "Epoch 89 | Batch: 3 | Loss: 7.8690\n",
      "Epoch 89 | Batch: 4 | Loss: 6.1926\n",
      "Epoch 89 | Batch: 5 | Loss: 5.4785\n",
      "Epoch 89 | Batch: 6 | Loss: 9.2222\n",
      "Epoch 89 | Batch: 7 | Loss: 10.5468\n",
      "Epoch 89 | Batch: 8 | Loss: 6.9371\n",
      "Epoch 89 | Batch: 9 | Loss: 8.2659\n",
      "Epoch 89 | Batch: 10 | Loss: 8.4678\n",
      "Epoch 89 | Batch: 11 | Loss: 17.4302\n",
      "Epoch 89 | Batch: 12 | Loss: 16.1521\n",
      "Epoch 89 | Batch: 13 | Loss: 14.6231\n",
      "Epoch 89 | Batch: 14 | Loss: 12.5118\n",
      "Epoch 89 | Batch: 15 | Loss: 16.0361\n",
      "Epoch 89 | Batch: 16 | Loss: 6.2618\n",
      "Epoch 89 | Batch: 17 | Loss: 5.0500\n",
      "Epoch 89 | Batch: 18 | Loss: 13.1036\n",
      "Epoch 89 | Batch: 19 | Loss: 7.8930\n",
      "Epoch 89 | Batch: 20 | Loss: 8.4797\n",
      "Epoch 89 | Batch: 21 | Loss: 19.4202\n",
      "Epoch 89 | Batch: 22 | Loss: 19.3498\n",
      "Epoch 89 | Batch: 23 | Loss: 17.4559\n",
      "Epoch 89 | Batch: 24 | Loss: 10.1293\n",
      "Epoch 89 | Batch: 25 | Loss: 10.0214\n",
      "Epoch 89 | Batch: 26 | Loss: 5.9566\n",
      "Epoch 89 | Batch: 27 | Loss: 11.2264\n",
      "Epoch 89 | Batch: 28 | Loss: 6.1427\n",
      "Epoch 89 | Batch: 29 | Loss: 5.1079\n",
      "Epoch 89 | Batch: 30 | Loss: 5.8024\n",
      "Epoch 89 | Batch: 31 | Loss: 10.2463\n",
      "Epoch 89 | Batch: 32 | Loss: 7.1321\n",
      "Epoch 89 | Batch: 33 | Loss: 4.7459\n",
      "Epoch 89 | Batch: 34 | Loss: 8.7266\n",
      "Epoch 89 | Batch: 35 | Loss: 12.1903\n",
      "Epoch 89 | Batch: 36 | Loss: 9.4015\n",
      "Epoch 89 | Batch: 37 | Loss: 11.3725\n",
      "Epoch 89 | Batch: 38 | Loss: 10.9641\n",
      "Epoch 89 | Batch: 39 | Loss: 5.6161\n",
      "Epoch 89 | Batch: 40 | Loss: 6.7289\n",
      "Epoch 89 | Batch: 41 | Loss: 12.8995\n",
      "Epoch 89 | Batch: 42 | Loss: 13.3093\n",
      "Epoch 89 | Batch: 43 | Loss: 8.8949\n",
      "Epoch 89 | Batch: 44 | Loss: 9.3761\n",
      "Epoch 89 | Batch: 45 | Loss: 7.5039\n",
      "Epoch 89 | Batch: 46 | Loss: 2.9375\n",
      "Epoch 89 | Batch: 47 | Loss: 13.2231\n",
      "Epoch 89 | Batch: 48 | Loss: 2.5440\n",
      "Mean 9.724521224697432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 | Batch: 1 | Loss: 7.7839\n",
      "Epoch 90 | Batch: 2 | Loss: 5.2294\n",
      "Epoch 90 | Batch: 3 | Loss: 10.5730\n",
      "Epoch 90 | Batch: 4 | Loss: 7.1547\n",
      "Epoch 90 | Batch: 5 | Loss: 14.9543\n",
      "Epoch 90 | Batch: 6 | Loss: 13.0669\n",
      "Epoch 90 | Batch: 7 | Loss: 10.1593\n",
      "Epoch 90 | Batch: 8 | Loss: 3.5104\n",
      "Epoch 90 | Batch: 9 | Loss: 10.3413\n",
      "Epoch 90 | Batch: 10 | Loss: 8.3702\n",
      "Epoch 90 | Batch: 11 | Loss: 6.1167\n",
      "Epoch 90 | Batch: 12 | Loss: 7.2849\n",
      "Epoch 90 | Batch: 13 | Loss: 4.2219\n",
      "Epoch 90 | Batch: 14 | Loss: 9.9230\n",
      "Epoch 90 | Batch: 15 | Loss: 17.8621\n",
      "Epoch 90 | Batch: 16 | Loss: 6.7088\n",
      "Epoch 90 | Batch: 17 | Loss: 6.1730\n",
      "Epoch 90 | Batch: 18 | Loss: 18.8800\n",
      "Epoch 90 | Batch: 19 | Loss: 23.0637\n",
      "Epoch 90 | Batch: 20 | Loss: 11.3092\n",
      "Epoch 90 | Batch: 21 | Loss: 7.2944\n",
      "Epoch 90 | Batch: 22 | Loss: 9.0945\n",
      "Epoch 90 | Batch: 23 | Loss: 12.4578\n",
      "Epoch 90 | Batch: 24 | Loss: 3.6350\n",
      "Epoch 90 | Batch: 25 | Loss: 4.6203\n",
      "Epoch 90 | Batch: 26 | Loss: 10.6256\n",
      "Epoch 90 | Batch: 27 | Loss: 9.2513\n",
      "Epoch 90 | Batch: 28 | Loss: 5.0850\n",
      "Epoch 90 | Batch: 29 | Loss: 5.7494\n",
      "Epoch 90 | Batch: 30 | Loss: 8.6036\n",
      "Epoch 90 | Batch: 31 | Loss: 12.2410\n",
      "Epoch 90 | Batch: 32 | Loss: 7.2665\n",
      "Epoch 90 | Batch: 33 | Loss: 9.4474\n",
      "Epoch 90 | Batch: 34 | Loss: 5.0336\n",
      "Epoch 90 | Batch: 35 | Loss: 9.2180\n",
      "Epoch 90 | Batch: 36 | Loss: 5.6905\n",
      "Epoch 90 | Batch: 37 | Loss: 10.9360\n",
      "Epoch 90 | Batch: 38 | Loss: 9.6764\n",
      "Epoch 90 | Batch: 39 | Loss: 3.9914\n",
      "Epoch 90 | Batch: 40 | Loss: 9.2790\n",
      "Epoch 90 | Batch: 41 | Loss: 10.0902\n",
      "Epoch 90 | Batch: 42 | Loss: 12.9839\n",
      "Epoch 90 | Batch: 43 | Loss: 8.7418\n",
      "Epoch 90 | Batch: 44 | Loss: 7.9932\n",
      "Epoch 90 | Batch: 45 | Loss: 9.2828\n",
      "Epoch 90 | Batch: 46 | Loss: 11.0908\n",
      "Epoch 90 | Batch: 47 | Loss: 21.1851\n",
      "Epoch 90 | Batch: 48 | Loss: 27.4483\n",
      "Mean 9.80624420940876\n",
      "Epoch 91 | Batch: 1 | Loss: 9.7240\n",
      "Epoch 91 | Batch: 2 | Loss: 10.5241\n",
      "Epoch 91 | Batch: 3 | Loss: 13.2961\n",
      "Epoch 91 | Batch: 4 | Loss: 13.8243\n",
      "Epoch 91 | Batch: 5 | Loss: 9.6499\n",
      "Epoch 91 | Batch: 6 | Loss: 10.3640\n",
      "Epoch 91 | Batch: 7 | Loss: 12.5825\n",
      "Epoch 91 | Batch: 8 | Loss: 7.0635\n",
      "Epoch 91 | Batch: 9 | Loss: 7.5466\n",
      "Epoch 91 | Batch: 10 | Loss: 5.9958\n",
      "Epoch 91 | Batch: 11 | Loss: 10.5087\n",
      "Epoch 91 | Batch: 12 | Loss: 4.7978\n",
      "Epoch 91 | Batch: 13 | Loss: 6.2520\n",
      "Epoch 91 | Batch: 14 | Loss: 6.0729\n",
      "Epoch 91 | Batch: 15 | Loss: 3.5629\n",
      "Epoch 91 | Batch: 16 | Loss: 10.2180\n",
      "Epoch 91 | Batch: 17 | Loss: 9.9190\n",
      "Epoch 91 | Batch: 18 | Loss: 5.4599\n",
      "Epoch 91 | Batch: 19 | Loss: 11.1780\n",
      "Epoch 91 | Batch: 20 | Loss: 7.1711\n",
      "Epoch 91 | Batch: 21 | Loss: 16.0494\n",
      "Epoch 91 | Batch: 22 | Loss: 8.7333\n",
      "Epoch 91 | Batch: 23 | Loss: 5.4988\n",
      "Epoch 91 | Batch: 24 | Loss: 7.7574\n",
      "Epoch 91 | Batch: 25 | Loss: 7.6710\n",
      "Epoch 91 | Batch: 26 | Loss: 17.1591\n",
      "Epoch 91 | Batch: 27 | Loss: 13.9443\n",
      "Epoch 91 | Batch: 28 | Loss: 9.8074\n",
      "Epoch 91 | Batch: 29 | Loss: 10.1875\n",
      "Epoch 91 | Batch: 30 | Loss: 14.5074\n",
      "Epoch 91 | Batch: 31 | Loss: 11.1543\n",
      "Epoch 91 | Batch: 32 | Loss: 12.0401\n",
      "Epoch 91 | Batch: 33 | Loss: 6.4282\n",
      "Epoch 91 | Batch: 34 | Loss: 6.8179\n",
      "Epoch 91 | Batch: 35 | Loss: 7.2391\n",
      "Epoch 91 | Batch: 36 | Loss: 6.6065\n",
      "Epoch 91 | Batch: 37 | Loss: 7.1516\n",
      "Epoch 91 | Batch: 38 | Loss: 5.5967\n",
      "Epoch 91 | Batch: 39 | Loss: 7.4790\n",
      "Epoch 91 | Batch: 40 | Loss: 21.4734\n",
      "Epoch 91 | Batch: 41 | Loss: 13.3470\n",
      "Epoch 91 | Batch: 42 | Loss: 8.4189\n",
      "Epoch 91 | Batch: 43 | Loss: 8.2141\n",
      "Epoch 91 | Batch: 44 | Loss: 11.1206\n",
      "Epoch 91 | Batch: 45 | Loss: 8.6645\n",
      "Epoch 91 | Batch: 46 | Loss: 16.8416\n",
      "Epoch 91 | Batch: 47 | Loss: 5.9881\n",
      "Epoch 91 | Batch: 48 | Loss: 1.5778\n",
      "Mean 9.441381196180979\n",
      "Epoch 92 | Batch: 1 | Loss: 5.7441\n",
      "Epoch 92 | Batch: 2 | Loss: 5.0246\n",
      "Epoch 92 | Batch: 3 | Loss: 7.6691\n",
      "Epoch 92 | Batch: 4 | Loss: 13.8072\n",
      "Epoch 92 | Batch: 5 | Loss: 27.2229\n",
      "Epoch 92 | Batch: 6 | Loss: 4.9167\n",
      "Epoch 92 | Batch: 7 | Loss: 12.2814\n",
      "Epoch 92 | Batch: 8 | Loss: 3.5363\n",
      "Epoch 92 | Batch: 9 | Loss: 8.6706\n",
      "Epoch 92 | Batch: 10 | Loss: 6.1799\n",
      "Epoch 92 | Batch: 11 | Loss: 18.7136\n",
      "Epoch 92 | Batch: 12 | Loss: 16.4644\n",
      "Epoch 92 | Batch: 13 | Loss: 9.9832\n",
      "Epoch 92 | Batch: 14 | Loss: 6.9702\n",
      "Epoch 92 | Batch: 15 | Loss: 4.8390\n",
      "Epoch 92 | Batch: 16 | Loss: 4.3436\n",
      "Epoch 92 | Batch: 17 | Loss: 6.2327\n",
      "Epoch 92 | Batch: 18 | Loss: 7.3146\n",
      "Epoch 92 | Batch: 19 | Loss: 7.1040\n",
      "Epoch 92 | Batch: 20 | Loss: 2.9127\n",
      "Epoch 92 | Batch: 21 | Loss: 3.3809\n",
      "Epoch 92 | Batch: 22 | Loss: 1.8765\n",
      "Epoch 92 | Batch: 23 | Loss: 7.3035\n",
      "Epoch 92 | Batch: 24 | Loss: 20.2554\n",
      "Epoch 92 | Batch: 25 | Loss: 30.2232\n",
      "Epoch 92 | Batch: 26 | Loss: 9.4619\n",
      "Epoch 92 | Batch: 27 | Loss: 10.1743\n",
      "Epoch 92 | Batch: 28 | Loss: 10.5402\n",
      "Epoch 92 | Batch: 29 | Loss: 9.6168\n",
      "Epoch 92 | Batch: 30 | Loss: 13.0303\n",
      "Epoch 92 | Batch: 31 | Loss: 9.8508\n",
      "Epoch 92 | Batch: 32 | Loss: 6.0320\n",
      "Epoch 92 | Batch: 33 | Loss: 7.0751\n",
      "Epoch 92 | Batch: 34 | Loss: 17.1342\n",
      "Epoch 92 | Batch: 35 | Loss: 23.8386\n",
      "Epoch 92 | Batch: 36 | Loss: 14.1084\n",
      "Epoch 92 | Batch: 37 | Loss: 10.5311\n",
      "Epoch 92 | Batch: 38 | Loss: 15.6587\n",
      "Epoch 92 | Batch: 39 | Loss: 11.3292\n",
      "Epoch 92 | Batch: 40 | Loss: 6.6818\n",
      "Epoch 92 | Batch: 41 | Loss: 5.2778\n",
      "Epoch 92 | Batch: 42 | Loss: 7.4367\n",
      "Epoch 92 | Batch: 43 | Loss: 11.3764\n",
      "Epoch 92 | Batch: 44 | Loss: 6.7786\n",
      "Epoch 92 | Batch: 45 | Loss: 9.7527\n",
      "Epoch 92 | Batch: 46 | Loss: 21.0024\n",
      "Epoch 92 | Batch: 47 | Loss: 17.6543\n",
      "Epoch 92 | Batch: 48 | Loss: 5.9976\n",
      "Mean 10.485634627441565\n",
      "Epoch 93 | Batch: 1 | Loss: 7.1412\n",
      "Epoch 93 | Batch: 2 | Loss: 5.6735\n",
      "Epoch 93 | Batch: 3 | Loss: 8.6119\n",
      "Epoch 93 | Batch: 4 | Loss: 8.1659\n",
      "Epoch 93 | Batch: 5 | Loss: 7.9423\n",
      "Epoch 93 | Batch: 6 | Loss: 6.5583\n",
      "Epoch 93 | Batch: 7 | Loss: 8.0310\n",
      "Epoch 93 | Batch: 8 | Loss: 8.4283\n",
      "Epoch 93 | Batch: 9 | Loss: 8.8459\n",
      "Epoch 93 | Batch: 10 | Loss: 7.7810\n",
      "Epoch 93 | Batch: 11 | Loss: 9.7641\n",
      "Epoch 93 | Batch: 12 | Loss: 10.7016\n",
      "Epoch 93 | Batch: 13 | Loss: 10.0562\n",
      "Epoch 93 | Batch: 14 | Loss: 10.3565\n",
      "Epoch 93 | Batch: 15 | Loss: 10.2437\n",
      "Epoch 93 | Batch: 16 | Loss: 12.3191\n",
      "Epoch 93 | Batch: 17 | Loss: 14.8854\n",
      "Epoch 93 | Batch: 18 | Loss: 3.5461\n",
      "Epoch 93 | Batch: 19 | Loss: 8.5486\n",
      "Epoch 93 | Batch: 20 | Loss: 4.1677\n",
      "Epoch 93 | Batch: 21 | Loss: 11.2827\n",
      "Epoch 93 | Batch: 22 | Loss: 7.6928\n",
      "Epoch 93 | Batch: 23 | Loss: 3.3822\n",
      "Epoch 93 | Batch: 24 | Loss: 18.0159\n",
      "Epoch 93 | Batch: 25 | Loss: 9.6439\n",
      "Epoch 93 | Batch: 26 | Loss: 7.8063\n",
      "Epoch 93 | Batch: 27 | Loss: 2.6988\n",
      "Epoch 93 | Batch: 28 | Loss: 2.9761\n",
      "Epoch 93 | Batch: 29 | Loss: 4.3206\n",
      "Epoch 93 | Batch: 30 | Loss: 4.7606\n",
      "Epoch 93 | Batch: 31 | Loss: 8.8656\n",
      "Epoch 93 | Batch: 32 | Loss: 8.7672\n",
      "Epoch 93 | Batch: 33 | Loss: 12.8669\n",
      "Epoch 93 | Batch: 34 | Loss: 11.4885\n",
      "Epoch 93 | Batch: 35 | Loss: 9.7064\n",
      "Epoch 93 | Batch: 36 | Loss: 9.8920\n",
      "Epoch 93 | Batch: 37 | Loss: 5.6845\n",
      "Epoch 93 | Batch: 38 | Loss: 14.8908\n",
      "Epoch 93 | Batch: 39 | Loss: 15.8681\n",
      "Epoch 93 | Batch: 40 | Loss: 9.7770\n",
      "Epoch 93 | Batch: 41 | Loss: 14.0612\n",
      "Epoch 93 | Batch: 42 | Loss: 10.2287\n",
      "Epoch 93 | Batch: 43 | Loss: 5.5248\n",
      "Epoch 93 | Batch: 44 | Loss: 6.1558\n",
      "Epoch 93 | Batch: 45 | Loss: 9.3838\n",
      "Epoch 93 | Batch: 46 | Loss: 14.6100\n",
      "Epoch 93 | Batch: 47 | Loss: 27.0029\n",
      "Epoch 93 | Batch: 48 | Loss: 5.4254\n",
      "Mean 9.261418908834457\n",
      "Epoch 94 | Batch: 1 | Loss: 7.3377\n",
      "Epoch 94 | Batch: 2 | Loss: 8.7268\n",
      "Epoch 94 | Batch: 3 | Loss: 7.2166\n",
      "Epoch 94 | Batch: 4 | Loss: 4.6819\n",
      "Epoch 94 | Batch: 5 | Loss: 8.1888\n",
      "Epoch 94 | Batch: 6 | Loss: 5.2144\n",
      "Epoch 94 | Batch: 7 | Loss: 12.3091\n",
      "Epoch 94 | Batch: 8 | Loss: 9.3260\n",
      "Epoch 94 | Batch: 9 | Loss: 9.2502\n",
      "Epoch 94 | Batch: 10 | Loss: 7.1335\n",
      "Epoch 94 | Batch: 11 | Loss: 10.2850\n",
      "Epoch 94 | Batch: 12 | Loss: 14.4356\n",
      "Epoch 94 | Batch: 13 | Loss: 12.6831\n",
      "Epoch 94 | Batch: 14 | Loss: 16.2565\n",
      "Epoch 94 | Batch: 15 | Loss: 10.4886\n",
      "Epoch 94 | Batch: 16 | Loss: 9.1518\n",
      "Epoch 94 | Batch: 17 | Loss: 8.4376\n",
      "Epoch 94 | Batch: 18 | Loss: 10.6213\n",
      "Epoch 94 | Batch: 19 | Loss: 16.7801\n",
      "Epoch 94 | Batch: 20 | Loss: 7.3784\n",
      "Epoch 94 | Batch: 21 | Loss: 6.5869\n",
      "Epoch 94 | Batch: 22 | Loss: 3.1392\n",
      "Epoch 94 | Batch: 23 | Loss: 8.5706\n",
      "Epoch 94 | Batch: 24 | Loss: 10.4487\n",
      "Epoch 94 | Batch: 25 | Loss: 5.1094\n",
      "Epoch 94 | Batch: 26 | Loss: 5.8336\n",
      "Epoch 94 | Batch: 27 | Loss: 3.7989\n",
      "Epoch 94 | Batch: 28 | Loss: 14.3568\n",
      "Epoch 94 | Batch: 29 | Loss: 7.5960\n",
      "Epoch 94 | Batch: 30 | Loss: 3.7607\n",
      "Epoch 94 | Batch: 31 | Loss: 2.9283\n",
      "Epoch 94 | Batch: 32 | Loss: 7.8967\n",
      "Epoch 94 | Batch: 33 | Loss: 4.6597\n",
      "Epoch 94 | Batch: 34 | Loss: 5.3006\n",
      "Epoch 94 | Batch: 35 | Loss: 9.2698\n",
      "Epoch 94 | Batch: 36 | Loss: 6.7395\n",
      "Epoch 94 | Batch: 37 | Loss: 9.6515\n",
      "Epoch 94 | Batch: 38 | Loss: 8.6225\n",
      "Epoch 94 | Batch: 39 | Loss: 12.9737\n",
      "Epoch 94 | Batch: 40 | Loss: 4.3306\n",
      "Epoch 94 | Batch: 41 | Loss: 6.7192\n",
      "Epoch 94 | Batch: 42 | Loss: 9.7783\n",
      "Epoch 94 | Batch: 43 | Loss: 10.5358\n",
      "Epoch 94 | Batch: 44 | Loss: 9.2871\n",
      "Epoch 94 | Batch: 45 | Loss: 9.0239\n",
      "Epoch 94 | Batch: 46 | Loss: 13.5686\n",
      "Epoch 94 | Batch: 47 | Loss: 10.5911\n",
      "Epoch 94 | Batch: 48 | Loss: 6.1933\n",
      "Mean 8.607793514927229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95 | Batch: 1 | Loss: 4.1930\n",
      "Epoch 95 | Batch: 2 | Loss: 7.4811\n",
      "Epoch 95 | Batch: 3 | Loss: 13.5758\n",
      "Epoch 95 | Batch: 4 | Loss: 14.7431\n",
      "Epoch 95 | Batch: 5 | Loss: 7.5463\n",
      "Epoch 95 | Batch: 6 | Loss: 11.9212\n",
      "Epoch 95 | Batch: 7 | Loss: 7.8840\n",
      "Epoch 95 | Batch: 8 | Loss: 6.2101\n",
      "Epoch 95 | Batch: 9 | Loss: 5.6529\n",
      "Epoch 95 | Batch: 10 | Loss: 4.3135\n",
      "Epoch 95 | Batch: 11 | Loss: 12.0937\n",
      "Epoch 95 | Batch: 12 | Loss: 7.9428\n",
      "Epoch 95 | Batch: 13 | Loss: 9.6354\n",
      "Epoch 95 | Batch: 14 | Loss: 4.3384\n",
      "Epoch 95 | Batch: 15 | Loss: 4.1902\n",
      "Epoch 95 | Batch: 16 | Loss: 10.5686\n",
      "Epoch 95 | Batch: 17 | Loss: 8.1589\n",
      "Epoch 95 | Batch: 18 | Loss: 6.3021\n",
      "Epoch 95 | Batch: 19 | Loss: 11.2330\n",
      "Epoch 95 | Batch: 20 | Loss: 3.9580\n",
      "Epoch 95 | Batch: 21 | Loss: 11.8786\n",
      "Epoch 95 | Batch: 22 | Loss: 3.8443\n",
      "Epoch 95 | Batch: 23 | Loss: 7.7994\n",
      "Epoch 95 | Batch: 24 | Loss: 10.5598\n",
      "Epoch 95 | Batch: 25 | Loss: 5.5944\n",
      "Epoch 95 | Batch: 26 | Loss: 4.5195\n",
      "Epoch 95 | Batch: 27 | Loss: 6.4914\n",
      "Epoch 95 | Batch: 28 | Loss: 4.3499\n",
      "Epoch 95 | Batch: 29 | Loss: 10.3388\n",
      "Epoch 95 | Batch: 30 | Loss: 10.8302\n",
      "Epoch 95 | Batch: 31 | Loss: 9.3071\n",
      "Epoch 95 | Batch: 32 | Loss: 10.3664\n",
      "Epoch 95 | Batch: 33 | Loss: 4.6825\n",
      "Epoch 95 | Batch: 34 | Loss: 13.7469\n",
      "Epoch 95 | Batch: 35 | Loss: 4.3621\n",
      "Epoch 95 | Batch: 36 | Loss: 15.5550\n",
      "Epoch 95 | Batch: 37 | Loss: 7.6584\n",
      "Epoch 95 | Batch: 38 | Loss: 8.0818\n",
      "Epoch 95 | Batch: 39 | Loss: 14.0282\n",
      "Epoch 95 | Batch: 40 | Loss: 16.9143\n",
      "Epoch 95 | Batch: 41 | Loss: 18.2641\n",
      "Epoch 95 | Batch: 42 | Loss: 41.5977\n",
      "Epoch 95 | Batch: 43 | Loss: 16.0537\n",
      "Epoch 95 | Batch: 44 | Loss: 11.2890\n",
      "Epoch 95 | Batch: 45 | Loss: 8.6120\n",
      "Epoch 95 | Batch: 46 | Loss: 9.2180\n",
      "Epoch 95 | Batch: 47 | Loss: 11.5624\n",
      "Epoch 95 | Batch: 48 | Loss: 9.2970\n",
      "Mean 9.765523105859756\n",
      "Epoch 96 | Batch: 1 | Loss: 5.4104\n",
      "Epoch 96 | Batch: 2 | Loss: 6.2475\n",
      "Epoch 96 | Batch: 3 | Loss: 8.5378\n",
      "Epoch 96 | Batch: 4 | Loss: 8.6842\n",
      "Epoch 96 | Batch: 5 | Loss: 6.7588\n",
      "Epoch 96 | Batch: 6 | Loss: 7.3579\n",
      "Epoch 96 | Batch: 7 | Loss: 11.9067\n",
      "Epoch 96 | Batch: 8 | Loss: 6.2489\n",
      "Epoch 96 | Batch: 9 | Loss: 6.7090\n",
      "Epoch 96 | Batch: 10 | Loss: 11.0167\n",
      "Epoch 96 | Batch: 11 | Loss: 13.5770\n",
      "Epoch 96 | Batch: 12 | Loss: 16.0352\n",
      "Epoch 96 | Batch: 13 | Loss: 14.9223\n",
      "Epoch 96 | Batch: 14 | Loss: 5.7823\n",
      "Epoch 96 | Batch: 15 | Loss: 2.6859\n",
      "Epoch 96 | Batch: 16 | Loss: 12.4208\n",
      "Epoch 96 | Batch: 17 | Loss: 16.9639\n",
      "Epoch 96 | Batch: 18 | Loss: 10.7623\n",
      "Epoch 96 | Batch: 19 | Loss: 8.6407\n",
      "Epoch 96 | Batch: 20 | Loss: 2.8941\n",
      "Epoch 96 | Batch: 21 | Loss: 7.5999\n",
      "Epoch 96 | Batch: 22 | Loss: 14.2665\n",
      "Epoch 96 | Batch: 23 | Loss: 21.9657\n",
      "Epoch 96 | Batch: 24 | Loss: 3.7561\n",
      "Epoch 96 | Batch: 25 | Loss: 9.6206\n",
      "Epoch 96 | Batch: 26 | Loss: 7.1307\n",
      "Epoch 96 | Batch: 27 | Loss: 6.9111\n",
      "Epoch 96 | Batch: 28 | Loss: 5.0850\n",
      "Epoch 96 | Batch: 29 | Loss: 6.1760\n",
      "Epoch 96 | Batch: 30 | Loss: 10.0920\n",
      "Epoch 96 | Batch: 31 | Loss: 13.1990\n",
      "Epoch 96 | Batch: 32 | Loss: 9.1999\n",
      "Epoch 96 | Batch: 33 | Loss: 4.6912\n",
      "Epoch 96 | Batch: 34 | Loss: 5.9976\n",
      "Epoch 96 | Batch: 35 | Loss: 6.9113\n",
      "Epoch 96 | Batch: 36 | Loss: 4.8700\n",
      "Epoch 96 | Batch: 37 | Loss: 7.4132\n",
      "Epoch 96 | Batch: 38 | Loss: 10.2411\n",
      "Epoch 96 | Batch: 39 | Loss: 10.7656\n",
      "Epoch 96 | Batch: 40 | Loss: 5.3201\n",
      "Epoch 96 | Batch: 41 | Loss: 8.8433\n",
      "Epoch 96 | Batch: 42 | Loss: 16.4316\n",
      "Epoch 96 | Batch: 43 | Loss: 10.0444\n",
      "Epoch 96 | Batch: 44 | Loss: 6.1614\n",
      "Epoch 96 | Batch: 45 | Loss: 3.5183\n",
      "Epoch 96 | Batch: 46 | Loss: 11.5577\n",
      "Epoch 96 | Batch: 47 | Loss: 12.6468\n",
      "Epoch 96 | Batch: 48 | Loss: 5.1538\n",
      "Mean 8.940262392163277\n",
      "Epoch 97 | Batch: 1 | Loss: 10.8793\n",
      "Epoch 97 | Batch: 2 | Loss: 9.2539\n",
      "Epoch 97 | Batch: 3 | Loss: 13.1501\n",
      "Epoch 97 | Batch: 4 | Loss: 7.7015\n",
      "Epoch 97 | Batch: 5 | Loss: 7.5658\n",
      "Epoch 97 | Batch: 6 | Loss: 15.0366\n",
      "Epoch 97 | Batch: 7 | Loss: 15.6490\n",
      "Epoch 97 | Batch: 8 | Loss: 5.9942\n",
      "Epoch 97 | Batch: 9 | Loss: 6.0943\n",
      "Epoch 97 | Batch: 10 | Loss: 12.4516\n",
      "Epoch 97 | Batch: 11 | Loss: 7.2701\n",
      "Epoch 97 | Batch: 12 | Loss: 9.6147\n",
      "Epoch 97 | Batch: 13 | Loss: 14.3156\n",
      "Epoch 97 | Batch: 14 | Loss: 5.6374\n",
      "Epoch 97 | Batch: 15 | Loss: 12.8747\n",
      "Epoch 97 | Batch: 16 | Loss: 9.1751\n",
      "Epoch 97 | Batch: 17 | Loss: 8.9100\n",
      "Epoch 97 | Batch: 18 | Loss: 14.7888\n",
      "Epoch 97 | Batch: 19 | Loss: 8.0682\n",
      "Epoch 97 | Batch: 20 | Loss: 10.7405\n",
      "Epoch 97 | Batch: 21 | Loss: 4.1457\n",
      "Epoch 97 | Batch: 22 | Loss: 6.2183\n",
      "Epoch 97 | Batch: 23 | Loss: 2.5593\n",
      "Epoch 97 | Batch: 24 | Loss: 6.0105\n",
      "Epoch 97 | Batch: 25 | Loss: 8.3434\n",
      "Epoch 97 | Batch: 26 | Loss: 13.1103\n",
      "Epoch 97 | Batch: 27 | Loss: 5.0047\n",
      "Epoch 97 | Batch: 28 | Loss: 9.8650\n",
      "Epoch 97 | Batch: 29 | Loss: 9.2592\n",
      "Epoch 97 | Batch: 30 | Loss: 7.2858\n",
      "Epoch 97 | Batch: 31 | Loss: 6.8251\n",
      "Epoch 97 | Batch: 32 | Loss: 10.7705\n",
      "Epoch 97 | Batch: 33 | Loss: 4.5636\n",
      "Epoch 97 | Batch: 34 | Loss: 9.3252\n",
      "Epoch 97 | Batch: 35 | Loss: 9.2035\n",
      "Epoch 97 | Batch: 36 | Loss: 15.5658\n",
      "Epoch 97 | Batch: 37 | Loss: 17.4364\n",
      "Epoch 97 | Batch: 38 | Loss: 15.1859\n",
      "Epoch 97 | Batch: 39 | Loss: 13.4680\n",
      "Epoch 97 | Batch: 40 | Loss: 13.6208\n",
      "Epoch 97 | Batch: 41 | Loss: 8.9359\n",
      "Epoch 97 | Batch: 42 | Loss: 13.3370\n",
      "Epoch 97 | Batch: 43 | Loss: 10.9236\n",
      "Epoch 97 | Batch: 44 | Loss: 4.9529\n",
      "Epoch 97 | Batch: 45 | Loss: 4.9333\n",
      "Epoch 97 | Batch: 46 | Loss: 11.6535\n",
      "Epoch 97 | Batch: 47 | Loss: 11.1218\n",
      "Epoch 97 | Batch: 48 | Loss: 1.2898\n",
      "Mean 9.585129226247469\n",
      "Epoch 98 | Batch: 1 | Loss: 6.0065\n",
      "Epoch 98 | Batch: 2 | Loss: 10.4602\n",
      "Epoch 98 | Batch: 3 | Loss: 12.4151\n",
      "Epoch 98 | Batch: 4 | Loss: 19.5470\n",
      "Epoch 98 | Batch: 5 | Loss: 9.3978\n",
      "Epoch 98 | Batch: 6 | Loss: 9.0359\n",
      "Epoch 98 | Batch: 7 | Loss: 8.7851\n",
      "Epoch 98 | Batch: 8 | Loss: 12.5175\n",
      "Epoch 98 | Batch: 9 | Loss: 14.7001\n",
      "Epoch 98 | Batch: 10 | Loss: 9.4295\n",
      "Epoch 98 | Batch: 11 | Loss: 7.3465\n",
      "Epoch 98 | Batch: 12 | Loss: 13.7012\n",
      "Epoch 98 | Batch: 13 | Loss: 7.3326\n",
      "Epoch 98 | Batch: 14 | Loss: 7.1728\n",
      "Epoch 98 | Batch: 15 | Loss: 8.3195\n",
      "Epoch 98 | Batch: 16 | Loss: 20.3102\n",
      "Epoch 98 | Batch: 17 | Loss: 16.8446\n",
      "Epoch 98 | Batch: 18 | Loss: 12.3907\n",
      "Epoch 98 | Batch: 19 | Loss: 3.1335\n",
      "Epoch 98 | Batch: 20 | Loss: 3.2520\n",
      "Epoch 98 | Batch: 21 | Loss: 7.7951\n",
      "Epoch 98 | Batch: 22 | Loss: 5.3196\n",
      "Epoch 98 | Batch: 23 | Loss: 9.1987\n",
      "Epoch 98 | Batch: 24 | Loss: 5.7810\n",
      "Epoch 98 | Batch: 25 | Loss: 4.9710\n",
      "Epoch 98 | Batch: 26 | Loss: 15.3582\n",
      "Epoch 98 | Batch: 27 | Loss: 4.6778\n",
      "Epoch 98 | Batch: 28 | Loss: 6.1144\n",
      "Epoch 98 | Batch: 29 | Loss: 6.8077\n",
      "Epoch 98 | Batch: 30 | Loss: 7.5788\n",
      "Epoch 98 | Batch: 31 | Loss: 8.9025\n",
      "Epoch 98 | Batch: 32 | Loss: 3.6687\n",
      "Epoch 98 | Batch: 33 | Loss: 4.3013\n",
      "Epoch 98 | Batch: 34 | Loss: 6.8977\n",
      "Epoch 98 | Batch: 35 | Loss: 14.0674\n",
      "Epoch 98 | Batch: 36 | Loss: 10.0675\n",
      "Epoch 98 | Batch: 37 | Loss: 10.8878\n",
      "Epoch 98 | Batch: 38 | Loss: 16.3745\n",
      "Epoch 98 | Batch: 39 | Loss: 6.3625\n",
      "Epoch 98 | Batch: 40 | Loss: 9.7476\n",
      "Epoch 98 | Batch: 41 | Loss: 7.9328\n",
      "Epoch 98 | Batch: 42 | Loss: 16.8224\n",
      "Epoch 98 | Batch: 43 | Loss: 15.8248\n",
      "Epoch 98 | Batch: 44 | Loss: 9.9420\n",
      "Epoch 98 | Batch: 45 | Loss: 23.5127\n",
      "Epoch 98 | Batch: 46 | Loss: 9.8083\n",
      "Epoch 98 | Batch: 47 | Loss: 13.9231\n",
      "Epoch 98 | Batch: 48 | Loss: 4.5649\n",
      "Mean 9.985605145494143\n",
      "Epoch 99 | Batch: 1 | Loss: 8.3003\n",
      "Epoch 99 | Batch: 2 | Loss: 14.4683\n",
      "Epoch 99 | Batch: 3 | Loss: 12.3553\n",
      "Epoch 99 | Batch: 4 | Loss: 7.6577\n",
      "Epoch 99 | Batch: 5 | Loss: 6.0101\n",
      "Epoch 99 | Batch: 6 | Loss: 23.6764\n",
      "Epoch 99 | Batch: 7 | Loss: 14.5081\n",
      "Epoch 99 | Batch: 8 | Loss: 8.5250\n",
      "Epoch 99 | Batch: 9 | Loss: 8.7404\n",
      "Epoch 99 | Batch: 10 | Loss: 14.2165\n",
      "Epoch 99 | Batch: 11 | Loss: 6.1938\n",
      "Epoch 99 | Batch: 12 | Loss: 4.1580\n",
      "Epoch 99 | Batch: 13 | Loss: 9.6966\n",
      "Epoch 99 | Batch: 14 | Loss: 7.7208\n",
      "Epoch 99 | Batch: 15 | Loss: 8.4425\n",
      "Epoch 99 | Batch: 16 | Loss: 5.7600\n",
      "Epoch 99 | Batch: 17 | Loss: 14.8349\n",
      "Epoch 99 | Batch: 18 | Loss: 12.0288\n",
      "Epoch 99 | Batch: 19 | Loss: 6.8668\n",
      "Epoch 99 | Batch: 20 | Loss: 13.5979\n",
      "Epoch 99 | Batch: 21 | Loss: 18.9777\n",
      "Epoch 99 | Batch: 22 | Loss: 8.6566\n",
      "Epoch 99 | Batch: 23 | Loss: 1.1716\n",
      "Epoch 99 | Batch: 24 | Loss: 6.2147\n",
      "Epoch 99 | Batch: 25 | Loss: 10.9121\n",
      "Epoch 99 | Batch: 26 | Loss: 10.9766\n",
      "Epoch 99 | Batch: 27 | Loss: 10.4473\n",
      "Epoch 99 | Batch: 28 | Loss: 7.5775\n",
      "Epoch 99 | Batch: 29 | Loss: 11.8797\n",
      "Epoch 99 | Batch: 30 | Loss: 6.2146\n",
      "Epoch 99 | Batch: 31 | Loss: 4.9574\n",
      "Epoch 99 | Batch: 32 | Loss: 14.0973\n",
      "Epoch 99 | Batch: 33 | Loss: 8.3826\n",
      "Epoch 99 | Batch: 34 | Loss: 16.8035\n",
      "Epoch 99 | Batch: 35 | Loss: 10.6849\n",
      "Epoch 99 | Batch: 36 | Loss: 4.6539\n",
      "Epoch 99 | Batch: 37 | Loss: 11.1154\n",
      "Epoch 99 | Batch: 38 | Loss: 14.5710\n",
      "Epoch 99 | Batch: 39 | Loss: 8.7938\n",
      "Epoch 99 | Batch: 40 | Loss: 18.9167\n",
      "Epoch 99 | Batch: 41 | Loss: 19.2981\n",
      "Epoch 99 | Batch: 42 | Loss: 8.9645\n",
      "Epoch 99 | Batch: 43 | Loss: 11.9601\n",
      "Epoch 99 | Batch: 44 | Loss: 16.1141\n",
      "Epoch 99 | Batch: 45 | Loss: 6.1608\n",
      "Epoch 99 | Batch: 46 | Loss: 9.4098\n",
      "Epoch 99 | Batch: 47 | Loss: 10.5328\n",
      "Epoch 99 | Batch: 48 | Loss: 2.2845\n",
      "Mean 10.385159636537233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 | Batch: 1 | Loss: 7.1287\n",
      "Epoch 100 | Batch: 2 | Loss: 6.4772\n",
      "Epoch 100 | Batch: 3 | Loss: 7.4851\n",
      "Epoch 100 | Batch: 4 | Loss: 3.9885\n",
      "Epoch 100 | Batch: 5 | Loss: 11.2464\n",
      "Epoch 100 | Batch: 6 | Loss: 10.0407\n",
      "Epoch 100 | Batch: 7 | Loss: 10.3250\n",
      "Epoch 100 | Batch: 8 | Loss: 3.6376\n",
      "Epoch 100 | Batch: 9 | Loss: 5.9188\n",
      "Epoch 100 | Batch: 10 | Loss: 6.9432\n",
      "Epoch 100 | Batch: 11 | Loss: 15.1789\n",
      "Epoch 100 | Batch: 12 | Loss: 6.9206\n",
      "Epoch 100 | Batch: 13 | Loss: 6.4749\n",
      "Epoch 100 | Batch: 14 | Loss: 10.8564\n",
      "Epoch 100 | Batch: 15 | Loss: 10.5096\n",
      "Epoch 100 | Batch: 16 | Loss: 9.4987\n",
      "Epoch 100 | Batch: 17 | Loss: 10.0761\n",
      "Epoch 100 | Batch: 18 | Loss: 8.4960\n",
      "Epoch 100 | Batch: 19 | Loss: 2.5967\n",
      "Epoch 100 | Batch: 20 | Loss: 4.6429\n",
      "Epoch 100 | Batch: 21 | Loss: 4.9832\n",
      "Epoch 100 | Batch: 22 | Loss: 5.4966\n",
      "Epoch 100 | Batch: 23 | Loss: 5.9161\n",
      "Epoch 100 | Batch: 24 | Loss: 12.8969\n",
      "Epoch 100 | Batch: 25 | Loss: 17.5944\n",
      "Epoch 100 | Batch: 26 | Loss: 18.4826\n",
      "Epoch 100 | Batch: 27 | Loss: 9.6241\n",
      "Epoch 100 | Batch: 28 | Loss: 12.2090\n",
      "Epoch 100 | Batch: 29 | Loss: 5.9559\n",
      "Epoch 100 | Batch: 30 | Loss: 10.6449\n",
      "Epoch 100 | Batch: 31 | Loss: 13.9831\n",
      "Epoch 100 | Batch: 32 | Loss: 22.9176\n",
      "Epoch 100 | Batch: 33 | Loss: 16.2976\n",
      "Epoch 100 | Batch: 34 | Loss: 15.3910\n",
      "Epoch 100 | Batch: 35 | Loss: 22.0018\n",
      "Epoch 100 | Batch: 36 | Loss: 12.2338\n",
      "Epoch 100 | Batch: 37 | Loss: 10.3023\n",
      "Epoch 100 | Batch: 38 | Loss: 13.6564\n",
      "Epoch 100 | Batch: 39 | Loss: 8.2890\n",
      "Epoch 100 | Batch: 40 | Loss: 17.0085\n",
      "Epoch 100 | Batch: 41 | Loss: 10.3933\n",
      "Epoch 100 | Batch: 42 | Loss: 7.5678\n",
      "Epoch 100 | Batch: 43 | Loss: 3.8298\n",
      "Epoch 100 | Batch: 44 | Loss: 6.7739\n",
      "Epoch 100 | Batch: 45 | Loss: 12.8628\n",
      "Epoch 100 | Batch: 46 | Loss: 5.5915\n",
      "Epoch 100 | Batch: 47 | Loss: 8.3520\n",
      "Epoch 100 | Batch: 48 | Loss: 4.1109\n",
      "Mean 9.871016417940458\n",
      "Epoch 101 | Batch: 1 | Loss: 7.2194\n",
      "Epoch 101 | Batch: 2 | Loss: 7.2270\n",
      "Epoch 101 | Batch: 3 | Loss: 13.6150\n",
      "Epoch 101 | Batch: 4 | Loss: 10.5084\n",
      "Epoch 101 | Batch: 5 | Loss: 7.1882\n",
      "Epoch 101 | Batch: 6 | Loss: 6.2215\n",
      "Epoch 101 | Batch: 7 | Loss: 9.4041\n",
      "Epoch 101 | Batch: 8 | Loss: 8.7363\n",
      "Epoch 101 | Batch: 9 | Loss: 8.4213\n",
      "Epoch 101 | Batch: 10 | Loss: 8.7467\n",
      "Epoch 101 | Batch: 11 | Loss: 7.9064\n",
      "Epoch 101 | Batch: 12 | Loss: 8.3935\n",
      "Epoch 101 | Batch: 13 | Loss: 7.3091\n",
      "Epoch 101 | Batch: 14 | Loss: 5.5989\n",
      "Epoch 101 | Batch: 15 | Loss: 11.1308\n",
      "Epoch 101 | Batch: 16 | Loss: 8.2881\n",
      "Epoch 101 | Batch: 17 | Loss: 8.1336\n",
      "Epoch 101 | Batch: 18 | Loss: 12.3919\n",
      "Epoch 101 | Batch: 19 | Loss: 8.7580\n",
      "Epoch 101 | Batch: 20 | Loss: 9.1030\n",
      "Epoch 101 | Batch: 21 | Loss: 4.1115\n",
      "Epoch 101 | Batch: 22 | Loss: 7.9415\n",
      "Epoch 101 | Batch: 23 | Loss: 9.7189\n",
      "Epoch 101 | Batch: 24 | Loss: 9.6625\n",
      "Epoch 101 | Batch: 25 | Loss: 8.0049\n",
      "Epoch 101 | Batch: 26 | Loss: 5.9762\n",
      "Epoch 101 | Batch: 27 | Loss: 11.3135\n",
      "Epoch 101 | Batch: 28 | Loss: 11.8535\n",
      "Epoch 101 | Batch: 29 | Loss: 7.9776\n",
      "Epoch 101 | Batch: 30 | Loss: 8.4022\n",
      "Epoch 101 | Batch: 31 | Loss: 8.7736\n",
      "Epoch 101 | Batch: 32 | Loss: 8.5473\n",
      "Epoch 101 | Batch: 33 | Loss: 4.5790\n",
      "Epoch 101 | Batch: 34 | Loss: 9.5172\n",
      "Epoch 101 | Batch: 35 | Loss: 10.7618\n",
      "Epoch 101 | Batch: 36 | Loss: 11.3101\n",
      "Epoch 101 | Batch: 37 | Loss: 8.9022\n",
      "Epoch 101 | Batch: 38 | Loss: 8.0597\n",
      "Epoch 101 | Batch: 39 | Loss: 7.2489\n",
      "Epoch 101 | Batch: 40 | Loss: 9.4841\n",
      "Epoch 101 | Batch: 41 | Loss: 10.1373\n",
      "Epoch 101 | Batch: 42 | Loss: 8.3046\n",
      "Epoch 101 | Batch: 43 | Loss: 5.8797\n",
      "Epoch 101 | Batch: 44 | Loss: 7.9305\n",
      "Epoch 101 | Batch: 45 | Loss: 11.7792\n",
      "Epoch 101 | Batch: 46 | Loss: 9.0242\n",
      "Epoch 101 | Batch: 47 | Loss: 5.5194\n",
      "Epoch 101 | Batch: 48 | Loss: 1.7436\n",
      "Mean 8.47429639349381\n",
      "Epoch 102 | Batch: 1 | Loss: 8.1800\n",
      "Epoch 102 | Batch: 2 | Loss: 9.0740\n",
      "Epoch 102 | Batch: 3 | Loss: 11.5065\n",
      "Epoch 102 | Batch: 4 | Loss: 13.0729\n",
      "Epoch 102 | Batch: 5 | Loss: 13.3113\n",
      "Epoch 102 | Batch: 6 | Loss: 6.7623\n",
      "Epoch 102 | Batch: 7 | Loss: 9.6123\n",
      "Epoch 102 | Batch: 8 | Loss: 11.3098\n",
      "Epoch 102 | Batch: 9 | Loss: 5.9538\n",
      "Epoch 102 | Batch: 10 | Loss: 19.9647\n",
      "Epoch 102 | Batch: 11 | Loss: 14.2460\n",
      "Epoch 102 | Batch: 12 | Loss: 9.4382\n",
      "Epoch 102 | Batch: 13 | Loss: 15.2724\n",
      "Epoch 102 | Batch: 14 | Loss: 15.5489\n",
      "Epoch 102 | Batch: 15 | Loss: 6.2794\n",
      "Epoch 102 | Batch: 16 | Loss: 10.4822\n",
      "Epoch 102 | Batch: 17 | Loss: 16.9495\n",
      "Epoch 102 | Batch: 18 | Loss: 7.6536\n",
      "Epoch 102 | Batch: 19 | Loss: 6.9787\n",
      "Epoch 102 | Batch: 20 | Loss: 4.5220\n",
      "Epoch 102 | Batch: 21 | Loss: 5.8233\n",
      "Epoch 102 | Batch: 22 | Loss: 10.8037\n",
      "Epoch 102 | Batch: 23 | Loss: 8.4427\n",
      "Epoch 102 | Batch: 24 | Loss: 5.6348\n",
      "Epoch 102 | Batch: 25 | Loss: 12.2905\n",
      "Epoch 102 | Batch: 26 | Loss: 16.6984\n",
      "Epoch 102 | Batch: 27 | Loss: 10.6073\n",
      "Epoch 102 | Batch: 28 | Loss: 10.2256\n",
      "Epoch 102 | Batch: 29 | Loss: 7.5572\n",
      "Epoch 102 | Batch: 30 | Loss: 7.1038\n",
      "Epoch 102 | Batch: 31 | Loss: 5.0155\n",
      "Epoch 102 | Batch: 32 | Loss: 8.6945\n",
      "Epoch 102 | Batch: 33 | Loss: 7.1003\n",
      "Epoch 102 | Batch: 34 | Loss: 10.3869\n",
      "Epoch 102 | Batch: 35 | Loss: 9.3292\n",
      "Epoch 102 | Batch: 36 | Loss: 4.6360\n",
      "Epoch 102 | Batch: 37 | Loss: 5.5450\n",
      "Epoch 102 | Batch: 38 | Loss: 11.8841\n",
      "Epoch 102 | Batch: 39 | Loss: 12.3636\n",
      "Epoch 102 | Batch: 40 | Loss: 9.4279\n",
      "Epoch 102 | Batch: 41 | Loss: 6.9578\n",
      "Epoch 102 | Batch: 42 | Loss: 8.0706\n",
      "Epoch 102 | Batch: 43 | Loss: 4.8126\n",
      "Epoch 102 | Batch: 44 | Loss: 6.4631\n",
      "Epoch 102 | Batch: 45 | Loss: 8.2306\n",
      "Epoch 102 | Batch: 46 | Loss: 3.4789\n",
      "Epoch 102 | Batch: 47 | Loss: 4.5072\n",
      "Epoch 102 | Batch: 48 | Loss: 5.6027\n",
      "Mean 9.24608131746451\n",
      "Epoch 103 | Batch: 1 | Loss: 5.4041\n",
      "Epoch 103 | Batch: 2 | Loss: 8.7080\n",
      "Epoch 103 | Batch: 3 | Loss: 8.1544\n",
      "Epoch 103 | Batch: 4 | Loss: 7.2317\n",
      "Epoch 103 | Batch: 5 | Loss: 6.9922\n",
      "Epoch 103 | Batch: 6 | Loss: 7.1591\n",
      "Epoch 103 | Batch: 7 | Loss: 7.3462\n",
      "Epoch 103 | Batch: 8 | Loss: 14.0176\n",
      "Epoch 103 | Batch: 9 | Loss: 6.5494\n",
      "Epoch 103 | Batch: 10 | Loss: 12.9797\n",
      "Epoch 103 | Batch: 11 | Loss: 7.3351\n",
      "Epoch 103 | Batch: 12 | Loss: 8.8039\n",
      "Epoch 103 | Batch: 13 | Loss: 6.3379\n",
      "Epoch 103 | Batch: 14 | Loss: 3.9763\n",
      "Epoch 103 | Batch: 15 | Loss: 3.6676\n",
      "Epoch 103 | Batch: 16 | Loss: 4.5743\n",
      "Epoch 103 | Batch: 17 | Loss: 9.2456\n",
      "Epoch 103 | Batch: 18 | Loss: 6.2847\n",
      "Epoch 103 | Batch: 19 | Loss: 15.2426\n",
      "Epoch 103 | Batch: 20 | Loss: 11.5193\n",
      "Epoch 103 | Batch: 21 | Loss: 11.5309\n",
      "Epoch 103 | Batch: 22 | Loss: 6.6578\n",
      "Epoch 103 | Batch: 23 | Loss: 12.2993\n",
      "Epoch 103 | Batch: 24 | Loss: 29.9158\n",
      "Epoch 103 | Batch: 25 | Loss: 16.2355\n",
      "Epoch 103 | Batch: 26 | Loss: 12.9637\n",
      "Epoch 103 | Batch: 27 | Loss: 6.2093\n",
      "Epoch 103 | Batch: 28 | Loss: 8.2213\n",
      "Epoch 103 | Batch: 29 | Loss: 8.1891\n",
      "Epoch 103 | Batch: 30 | Loss: 6.3959\n",
      "Epoch 103 | Batch: 31 | Loss: 8.4135\n",
      "Epoch 103 | Batch: 32 | Loss: 8.8905\n",
      "Epoch 103 | Batch: 33 | Loss: 7.2154\n",
      "Epoch 103 | Batch: 34 | Loss: 8.5134\n",
      "Epoch 103 | Batch: 35 | Loss: 5.2373\n",
      "Epoch 103 | Batch: 36 | Loss: 16.8797\n",
      "Epoch 103 | Batch: 37 | Loss: 9.5172\n",
      "Epoch 103 | Batch: 38 | Loss: 26.0448\n",
      "Epoch 103 | Batch: 39 | Loss: 28.9383\n",
      "Epoch 103 | Batch: 40 | Loss: 5.0646\n",
      "Epoch 103 | Batch: 41 | Loss: 11.4032\n",
      "Epoch 103 | Batch: 42 | Loss: 10.8428\n",
      "Epoch 103 | Batch: 43 | Loss: 8.4772\n",
      "Epoch 103 | Batch: 44 | Loss: 5.3212\n",
      "Epoch 103 | Batch: 45 | Loss: 7.1083\n",
      "Epoch 103 | Batch: 46 | Loss: 5.2088\n",
      "Epoch 103 | Batch: 47 | Loss: 14.1939\n",
      "Epoch 103 | Batch: 48 | Loss: 4.2582\n",
      "Mean 9.826597029964129\n",
      "Epoch 104 | Batch: 1 | Loss: 7.4410\n",
      "Epoch 104 | Batch: 2 | Loss: 3.1393\n",
      "Epoch 104 | Batch: 3 | Loss: 13.4641\n",
      "Epoch 104 | Batch: 4 | Loss: 17.8799\n",
      "Epoch 104 | Batch: 5 | Loss: 24.5440\n",
      "Epoch 104 | Batch: 6 | Loss: 6.1824\n",
      "Epoch 104 | Batch: 7 | Loss: 7.0522\n",
      "Epoch 104 | Batch: 8 | Loss: 9.8778\n",
      "Epoch 104 | Batch: 9 | Loss: 15.8513\n",
      "Epoch 104 | Batch: 10 | Loss: 5.4094\n",
      "Epoch 104 | Batch: 11 | Loss: 8.5075\n",
      "Epoch 104 | Batch: 12 | Loss: 6.7641\n",
      "Epoch 104 | Batch: 13 | Loss: 4.4321\n",
      "Epoch 104 | Batch: 14 | Loss: 7.1608\n",
      "Epoch 104 | Batch: 15 | Loss: 9.8877\n",
      "Epoch 104 | Batch: 16 | Loss: 8.6114\n",
      "Epoch 104 | Batch: 17 | Loss: 7.7505\n",
      "Epoch 104 | Batch: 18 | Loss: 6.6065\n",
      "Epoch 104 | Batch: 19 | Loss: 6.8598\n",
      "Epoch 104 | Batch: 20 | Loss: 12.7425\n",
      "Epoch 104 | Batch: 21 | Loss: 20.6211\n",
      "Epoch 104 | Batch: 22 | Loss: 13.9959\n",
      "Epoch 104 | Batch: 23 | Loss: 8.3790\n",
      "Epoch 104 | Batch: 24 | Loss: 3.0859\n",
      "Epoch 104 | Batch: 25 | Loss: 11.8830\n",
      "Epoch 104 | Batch: 26 | Loss: 13.8866\n",
      "Epoch 104 | Batch: 27 | Loss: 9.6817\n",
      "Epoch 104 | Batch: 28 | Loss: 13.1940\n",
      "Epoch 104 | Batch: 29 | Loss: 4.5976\n",
      "Epoch 104 | Batch: 30 | Loss: 5.6372\n",
      "Epoch 104 | Batch: 31 | Loss: 24.6095\n",
      "Epoch 104 | Batch: 32 | Loss: 17.0879\n",
      "Epoch 104 | Batch: 33 | Loss: 18.6459\n",
      "Epoch 104 | Batch: 34 | Loss: 14.4256\n",
      "Epoch 104 | Batch: 35 | Loss: 7.4239\n",
      "Epoch 104 | Batch: 36 | Loss: 8.7912\n",
      "Epoch 104 | Batch: 37 | Loss: 7.3055\n",
      "Epoch 104 | Batch: 38 | Loss: 5.2244\n",
      "Epoch 104 | Batch: 39 | Loss: 7.8549\n",
      "Epoch 104 | Batch: 40 | Loss: 5.3492\n",
      "Epoch 104 | Batch: 41 | Loss: 8.9633\n",
      "Epoch 104 | Batch: 42 | Loss: 5.0248\n",
      "Epoch 104 | Batch: 43 | Loss: 10.3918\n",
      "Epoch 104 | Batch: 44 | Loss: 6.5489\n",
      "Epoch 104 | Batch: 45 | Loss: 10.2634\n",
      "Epoch 104 | Batch: 46 | Loss: 22.6081\n",
      "Epoch 104 | Batch: 47 | Loss: 10.4545\n",
      "Epoch 104 | Batch: 48 | Loss: 3.8398\n",
      "Mean 10.207058146595955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105 | Batch: 1 | Loss: 8.6954\n",
      "Epoch 105 | Batch: 2 | Loss: 8.6343\n",
      "Epoch 105 | Batch: 3 | Loss: 6.8734\n",
      "Epoch 105 | Batch: 4 | Loss: 8.7073\n",
      "Epoch 105 | Batch: 5 | Loss: 2.5525\n",
      "Epoch 105 | Batch: 6 | Loss: 6.2723\n",
      "Epoch 105 | Batch: 7 | Loss: 9.6469\n",
      "Epoch 105 | Batch: 8 | Loss: 8.4267\n",
      "Epoch 105 | Batch: 9 | Loss: 6.2285\n",
      "Epoch 105 | Batch: 10 | Loss: 5.4004\n",
      "Epoch 105 | Batch: 11 | Loss: 13.2421\n",
      "Epoch 105 | Batch: 12 | Loss: 7.9689\n",
      "Epoch 105 | Batch: 13 | Loss: 4.8737\n",
      "Epoch 105 | Batch: 14 | Loss: 7.1709\n",
      "Epoch 105 | Batch: 15 | Loss: 11.9640\n",
      "Epoch 105 | Batch: 16 | Loss: 7.2025\n",
      "Epoch 105 | Batch: 17 | Loss: 10.0836\n",
      "Epoch 105 | Batch: 18 | Loss: 5.2826\n",
      "Epoch 105 | Batch: 19 | Loss: 8.8150\n",
      "Epoch 105 | Batch: 20 | Loss: 11.6946\n",
      "Epoch 105 | Batch: 21 | Loss: 30.3293\n",
      "Epoch 105 | Batch: 22 | Loss: 12.9823\n",
      "Epoch 105 | Batch: 23 | Loss: 4.9819\n",
      "Epoch 105 | Batch: 24 | Loss: 7.2248\n",
      "Epoch 105 | Batch: 25 | Loss: 8.2323\n",
      "Epoch 105 | Batch: 26 | Loss: 5.3891\n",
      "Epoch 105 | Batch: 27 | Loss: 23.0061\n",
      "Epoch 105 | Batch: 28 | Loss: 14.1223\n",
      "Epoch 105 | Batch: 29 | Loss: 11.9717\n",
      "Epoch 105 | Batch: 30 | Loss: 14.7862\n",
      "Epoch 105 | Batch: 31 | Loss: 11.6191\n",
      "Epoch 105 | Batch: 32 | Loss: 13.4507\n",
      "Epoch 105 | Batch: 33 | Loss: 9.6794\n",
      "Epoch 105 | Batch: 34 | Loss: 7.2724\n",
      "Epoch 105 | Batch: 35 | Loss: 5.9319\n",
      "Epoch 105 | Batch: 36 | Loss: 7.5810\n",
      "Epoch 105 | Batch: 37 | Loss: 4.9261\n",
      "Epoch 105 | Batch: 38 | Loss: 14.2411\n",
      "Epoch 105 | Batch: 39 | Loss: 10.0086\n",
      "Epoch 105 | Batch: 40 | Loss: 3.5472\n",
      "Epoch 105 | Batch: 41 | Loss: 6.2158\n",
      "Epoch 105 | Batch: 42 | Loss: 5.2404\n",
      "Epoch 105 | Batch: 43 | Loss: 6.0728\n",
      "Epoch 105 | Batch: 44 | Loss: 5.8837\n",
      "Epoch 105 | Batch: 45 | Loss: 6.1428\n",
      "Epoch 105 | Batch: 46 | Loss: 6.9073\n",
      "Epoch 105 | Batch: 47 | Loss: 6.4955\n",
      "Epoch 105 | Batch: 48 | Loss: 4.5388\n",
      "Mean 8.927423333128294\n",
      "Epoch 106 | Batch: 1 | Loss: 4.9054\n",
      "Epoch 106 | Batch: 2 | Loss: 19.7766\n",
      "Epoch 106 | Batch: 3 | Loss: 18.3615\n",
      "Epoch 106 | Batch: 4 | Loss: 10.0214\n",
      "Epoch 106 | Batch: 5 | Loss: 6.9475\n",
      "Epoch 106 | Batch: 6 | Loss: 7.8217\n",
      "Epoch 106 | Batch: 7 | Loss: 10.7424\n",
      "Epoch 106 | Batch: 8 | Loss: 18.4706\n",
      "Epoch 106 | Batch: 9 | Loss: 17.7486\n",
      "Epoch 106 | Batch: 10 | Loss: 7.2468\n",
      "Epoch 106 | Batch: 11 | Loss: 6.5649\n",
      "Epoch 106 | Batch: 12 | Loss: 7.6158\n",
      "Epoch 106 | Batch: 13 | Loss: 10.3057\n",
      "Epoch 106 | Batch: 14 | Loss: 14.2392\n",
      "Epoch 106 | Batch: 15 | Loss: 6.2865\n",
      "Epoch 106 | Batch: 16 | Loss: 6.9071\n",
      "Epoch 106 | Batch: 17 | Loss: 5.3449\n",
      "Epoch 106 | Batch: 18 | Loss: 6.7520\n",
      "Epoch 106 | Batch: 19 | Loss: 10.0949\n",
      "Epoch 106 | Batch: 20 | Loss: 10.9618\n",
      "Epoch 106 | Batch: 21 | Loss: 8.4408\n",
      "Epoch 106 | Batch: 22 | Loss: 11.8146\n",
      "Epoch 106 | Batch: 23 | Loss: 16.7730\n",
      "Epoch 106 | Batch: 24 | Loss: 29.0375\n",
      "Epoch 106 | Batch: 25 | Loss: 8.2517\n",
      "Epoch 106 | Batch: 26 | Loss: 8.9927\n",
      "Epoch 106 | Batch: 27 | Loss: 12.8824\n",
      "Epoch 106 | Batch: 28 | Loss: 6.8168\n",
      "Epoch 106 | Batch: 29 | Loss: 7.1716\n",
      "Epoch 106 | Batch: 30 | Loss: 10.1805\n",
      "Epoch 106 | Batch: 31 | Loss: 7.9192\n",
      "Epoch 106 | Batch: 32 | Loss: 10.6518\n",
      "Epoch 106 | Batch: 33 | Loss: 18.4055\n",
      "Epoch 106 | Batch: 34 | Loss: 16.5220\n",
      "Epoch 106 | Batch: 35 | Loss: 2.9285\n",
      "Epoch 106 | Batch: 36 | Loss: 5.7487\n",
      "Epoch 106 | Batch: 37 | Loss: 7.0439\n",
      "Epoch 106 | Batch: 38 | Loss: 13.3035\n",
      "Epoch 106 | Batch: 39 | Loss: 14.5666\n",
      "Epoch 106 | Batch: 40 | Loss: 11.6646\n",
      "Epoch 106 | Batch: 41 | Loss: 16.3427\n",
      "Epoch 106 | Batch: 42 | Loss: 9.6005\n",
      "Epoch 106 | Batch: 43 | Loss: 5.5073\n",
      "Epoch 106 | Batch: 44 | Loss: 5.6302\n",
      "Epoch 106 | Batch: 45 | Loss: 9.0020\n",
      "Epoch 106 | Batch: 46 | Loss: 8.1184\n",
      "Epoch 106 | Batch: 47 | Loss: 21.9110\n",
      "Epoch 106 | Batch: 48 | Loss: 1.9354\n",
      "Mean 10.714140261212984\n",
      "Epoch 107 | Batch: 1 | Loss: 9.6023\n",
      "Epoch 107 | Batch: 2 | Loss: 8.3632\n",
      "Epoch 107 | Batch: 3 | Loss: 9.1104\n",
      "Epoch 107 | Batch: 4 | Loss: 11.8058\n",
      "Epoch 107 | Batch: 5 | Loss: 17.1466\n",
      "Epoch 107 | Batch: 6 | Loss: 11.5924\n",
      "Epoch 107 | Batch: 7 | Loss: 4.9248\n",
      "Epoch 107 | Batch: 8 | Loss: 8.2553\n",
      "Epoch 107 | Batch: 9 | Loss: 11.7855\n",
      "Epoch 107 | Batch: 10 | Loss: 9.8187\n",
      "Epoch 107 | Batch: 11 | Loss: 11.0359\n",
      "Epoch 107 | Batch: 12 | Loss: 20.2764\n",
      "Epoch 107 | Batch: 13 | Loss: 12.5976\n",
      "Epoch 107 | Batch: 14 | Loss: 5.7278\n",
      "Epoch 107 | Batch: 15 | Loss: 6.6642\n",
      "Epoch 107 | Batch: 16 | Loss: 10.9064\n",
      "Epoch 107 | Batch: 17 | Loss: 6.6726\n",
      "Epoch 107 | Batch: 18 | Loss: 10.2748\n",
      "Epoch 107 | Batch: 19 | Loss: 4.7301\n",
      "Epoch 107 | Batch: 20 | Loss: 9.2171\n",
      "Epoch 107 | Batch: 21 | Loss: 8.6645\n",
      "Epoch 107 | Batch: 22 | Loss: 9.9226\n",
      "Epoch 107 | Batch: 23 | Loss: 6.9978\n",
      "Epoch 107 | Batch: 24 | Loss: 6.7221\n",
      "Epoch 107 | Batch: 25 | Loss: 7.5266\n",
      "Epoch 107 | Batch: 26 | Loss: 8.8211\n",
      "Epoch 107 | Batch: 27 | Loss: 8.0908\n",
      "Epoch 107 | Batch: 28 | Loss: 8.1591\n",
      "Epoch 107 | Batch: 29 | Loss: 18.8114\n",
      "Epoch 107 | Batch: 30 | Loss: 31.7294\n",
      "Epoch 107 | Batch: 31 | Loss: 12.6196\n",
      "Epoch 107 | Batch: 32 | Loss: 6.1194\n",
      "Epoch 107 | Batch: 33 | Loss: 5.2083\n",
      "Epoch 107 | Batch: 34 | Loss: 2.0515\n",
      "Epoch 107 | Batch: 35 | Loss: 9.8732\n",
      "Epoch 107 | Batch: 36 | Loss: 2.2814\n",
      "Epoch 107 | Batch: 37 | Loss: 12.3050\n",
      "Epoch 107 | Batch: 38 | Loss: 7.1223\n",
      "Epoch 107 | Batch: 39 | Loss: 4.7831\n",
      "Epoch 107 | Batch: 40 | Loss: 11.7378\n",
      "Epoch 107 | Batch: 41 | Loss: 5.6481\n",
      "Epoch 107 | Batch: 42 | Loss: 4.0952\n",
      "Epoch 107 | Batch: 43 | Loss: 4.6562\n",
      "Epoch 107 | Batch: 44 | Loss: 11.1828\n",
      "Epoch 107 | Batch: 45 | Loss: 31.5179\n",
      "Epoch 107 | Batch: 46 | Loss: 15.3397\n",
      "Epoch 107 | Batch: 47 | Loss: 8.8091\n",
      "Epoch 107 | Batch: 48 | Loss: 1.4289\n",
      "Mean 9.848602026700974\n",
      "Epoch 108 | Batch: 1 | Loss: 8.1046\n",
      "Epoch 108 | Batch: 2 | Loss: 2.1995\n",
      "Epoch 108 | Batch: 3 | Loss: 5.9586\n",
      "Epoch 108 | Batch: 4 | Loss: 10.7918\n",
      "Epoch 108 | Batch: 5 | Loss: 4.5566\n",
      "Epoch 108 | Batch: 6 | Loss: 6.0423\n",
      "Epoch 108 | Batch: 7 | Loss: 11.4557\n",
      "Epoch 108 | Batch: 8 | Loss: 16.0093\n",
      "Epoch 108 | Batch: 9 | Loss: 4.6578\n",
      "Epoch 108 | Batch: 10 | Loss: 9.6485\n",
      "Epoch 108 | Batch: 11 | Loss: 9.2161\n",
      "Epoch 108 | Batch: 12 | Loss: 4.2049\n",
      "Epoch 108 | Batch: 13 | Loss: 4.5777\n",
      "Epoch 108 | Batch: 14 | Loss: 8.3825\n",
      "Epoch 108 | Batch: 15 | Loss: 7.5085\n",
      "Epoch 108 | Batch: 16 | Loss: 6.3424\n",
      "Epoch 108 | Batch: 17 | Loss: 5.3917\n",
      "Epoch 108 | Batch: 18 | Loss: 4.1932\n",
      "Epoch 108 | Batch: 19 | Loss: 7.6057\n",
      "Epoch 108 | Batch: 20 | Loss: 11.9995\n",
      "Epoch 108 | Batch: 21 | Loss: 10.5758\n",
      "Epoch 108 | Batch: 22 | Loss: 11.0948\n",
      "Epoch 108 | Batch: 23 | Loss: 14.6577\n",
      "Epoch 108 | Batch: 24 | Loss: 13.7733\n",
      "Epoch 108 | Batch: 25 | Loss: 13.9032\n",
      "Epoch 108 | Batch: 26 | Loss: 8.2942\n",
      "Epoch 108 | Batch: 27 | Loss: 11.9513\n",
      "Epoch 108 | Batch: 28 | Loss: 10.5673\n",
      "Epoch 108 | Batch: 29 | Loss: 10.0103\n",
      "Epoch 108 | Batch: 30 | Loss: 7.1440\n",
      "Epoch 108 | Batch: 31 | Loss: 9.5078\n",
      "Epoch 108 | Batch: 32 | Loss: 6.5937\n",
      "Epoch 108 | Batch: 33 | Loss: 7.5357\n",
      "Epoch 108 | Batch: 34 | Loss: 9.3728\n",
      "Epoch 108 | Batch: 35 | Loss: 14.3811\n",
      "Epoch 108 | Batch: 36 | Loss: 4.7347\n",
      "Epoch 108 | Batch: 37 | Loss: 8.6093\n",
      "Epoch 108 | Batch: 38 | Loss: 3.8407\n",
      "Epoch 108 | Batch: 39 | Loss: 7.6422\n",
      "Epoch 108 | Batch: 40 | Loss: 7.9967\n",
      "Epoch 108 | Batch: 41 | Loss: 8.7934\n",
      "Epoch 108 | Batch: 42 | Loss: 4.3728\n",
      "Epoch 108 | Batch: 43 | Loss: 5.7058\n",
      "Epoch 108 | Batch: 44 | Loss: 6.2199\n",
      "Epoch 108 | Batch: 45 | Loss: 7.5480\n",
      "Epoch 108 | Batch: 46 | Loss: 5.9268\n",
      "Epoch 108 | Batch: 47 | Loss: 8.1583\n",
      "Epoch 108 | Batch: 48 | Loss: 4.3431\n",
      "Mean 8.168786401549974\n",
      "Epoch 109 | Batch: 1 | Loss: 13.3159\n",
      "Epoch 109 | Batch: 2 | Loss: 11.2239\n",
      "Epoch 109 | Batch: 3 | Loss: 6.9879\n",
      "Epoch 109 | Batch: 4 | Loss: 10.3666\n",
      "Epoch 109 | Batch: 5 | Loss: 20.0436\n",
      "Epoch 109 | Batch: 6 | Loss: 17.2193\n",
      "Epoch 109 | Batch: 7 | Loss: 5.7082\n",
      "Epoch 109 | Batch: 8 | Loss: 8.1482\n",
      "Epoch 109 | Batch: 9 | Loss: 8.3156\n",
      "Epoch 109 | Batch: 10 | Loss: 11.1594\n",
      "Epoch 109 | Batch: 11 | Loss: 8.4927\n",
      "Epoch 109 | Batch: 12 | Loss: 9.3457\n",
      "Epoch 109 | Batch: 13 | Loss: 9.1498\n",
      "Epoch 109 | Batch: 14 | Loss: 10.0259\n",
      "Epoch 109 | Batch: 15 | Loss: 5.2600\n",
      "Epoch 109 | Batch: 16 | Loss: 7.8870\n",
      "Epoch 109 | Batch: 17 | Loss: 9.2055\n",
      "Epoch 109 | Batch: 18 | Loss: 7.6049\n",
      "Epoch 109 | Batch: 19 | Loss: 4.9006\n",
      "Epoch 109 | Batch: 20 | Loss: 7.5854\n",
      "Epoch 109 | Batch: 21 | Loss: 7.6827\n",
      "Epoch 109 | Batch: 22 | Loss: 6.7617\n",
      "Epoch 109 | Batch: 23 | Loss: 5.7048\n",
      "Epoch 109 | Batch: 24 | Loss: 3.3030\n",
      "Epoch 109 | Batch: 25 | Loss: 4.0175\n",
      "Epoch 109 | Batch: 26 | Loss: 10.8987\n",
      "Epoch 109 | Batch: 27 | Loss: 7.3734\n",
      "Epoch 109 | Batch: 28 | Loss: 5.9472\n",
      "Epoch 109 | Batch: 29 | Loss: 11.1190\n",
      "Epoch 109 | Batch: 30 | Loss: 2.4305\n",
      "Epoch 109 | Batch: 31 | Loss: 4.9486\n",
      "Epoch 109 | Batch: 32 | Loss: 14.5823\n",
      "Epoch 109 | Batch: 33 | Loss: 16.0373\n",
      "Epoch 109 | Batch: 34 | Loss: 8.2876\n",
      "Epoch 109 | Batch: 35 | Loss: 4.5977\n",
      "Epoch 109 | Batch: 36 | Loss: 9.4038\n",
      "Epoch 109 | Batch: 37 | Loss: 10.8176\n",
      "Epoch 109 | Batch: 38 | Loss: 12.7154\n",
      "Epoch 109 | Batch: 39 | Loss: 4.3344\n",
      "Epoch 109 | Batch: 40 | Loss: 6.0429\n",
      "Epoch 109 | Batch: 41 | Loss: 18.5084\n",
      "Epoch 109 | Batch: 42 | Loss: 23.9079\n",
      "Epoch 109 | Batch: 43 | Loss: 6.9329\n",
      "Epoch 109 | Batch: 44 | Loss: 12.9966\n",
      "Epoch 109 | Batch: 45 | Loss: 19.5703\n",
      "Epoch 109 | Batch: 46 | Loss: 14.2408\n",
      "Epoch 109 | Batch: 47 | Loss: 10.8888\n",
      "Epoch 109 | Batch: 48 | Loss: 8.1146\n",
      "Mean 9.669011890888214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110 | Batch: 1 | Loss: 5.6246\n",
      "Epoch 110 | Batch: 2 | Loss: 9.3894\n",
      "Epoch 110 | Batch: 3 | Loss: 3.5236\n",
      "Epoch 110 | Batch: 4 | Loss: 7.9858\n",
      "Epoch 110 | Batch: 5 | Loss: 11.8312\n",
      "Epoch 110 | Batch: 6 | Loss: 7.2399\n",
      "Epoch 110 | Batch: 7 | Loss: 8.6397\n",
      "Epoch 110 | Batch: 8 | Loss: 7.5847\n",
      "Epoch 110 | Batch: 9 | Loss: 9.5415\n",
      "Epoch 110 | Batch: 10 | Loss: 5.7861\n",
      "Epoch 110 | Batch: 11 | Loss: 11.0886\n",
      "Epoch 110 | Batch: 12 | Loss: 16.7188\n",
      "Epoch 110 | Batch: 13 | Loss: 12.0608\n",
      "Epoch 110 | Batch: 14 | Loss: 7.8883\n",
      "Epoch 110 | Batch: 15 | Loss: 2.9127\n",
      "Epoch 110 | Batch: 16 | Loss: 4.3020\n",
      "Epoch 110 | Batch: 17 | Loss: 9.9216\n",
      "Epoch 110 | Batch: 18 | Loss: 4.0432\n",
      "Epoch 110 | Batch: 19 | Loss: 11.6007\n",
      "Epoch 110 | Batch: 20 | Loss: 11.7324\n",
      "Epoch 110 | Batch: 21 | Loss: 12.8625\n",
      "Epoch 110 | Batch: 22 | Loss: 9.1746\n",
      "Epoch 110 | Batch: 23 | Loss: 6.8855\n",
      "Epoch 110 | Batch: 24 | Loss: 14.4007\n",
      "Epoch 110 | Batch: 25 | Loss: 19.9302\n",
      "Epoch 110 | Batch: 26 | Loss: 23.6476\n",
      "Epoch 110 | Batch: 27 | Loss: 6.5132\n",
      "Epoch 110 | Batch: 28 | Loss: 7.9994\n",
      "Epoch 110 | Batch: 29 | Loss: 8.8877\n",
      "Epoch 110 | Batch: 30 | Loss: 10.1851\n",
      "Epoch 110 | Batch: 31 | Loss: 10.4099\n",
      "Epoch 110 | Batch: 32 | Loss: 9.6067\n",
      "Epoch 110 | Batch: 33 | Loss: 10.2647\n",
      "Epoch 110 | Batch: 34 | Loss: 10.4055\n",
      "Epoch 110 | Batch: 35 | Loss: 9.1443\n",
      "Epoch 110 | Batch: 36 | Loss: 13.3590\n",
      "Epoch 110 | Batch: 37 | Loss: 7.3043\n",
      "Epoch 110 | Batch: 38 | Loss: 17.3724\n",
      "Epoch 110 | Batch: 39 | Loss: 15.0030\n",
      "Epoch 110 | Batch: 40 | Loss: 10.7318\n",
      "Epoch 110 | Batch: 41 | Loss: 5.4618\n",
      "Epoch 110 | Batch: 42 | Loss: 14.0332\n",
      "Epoch 110 | Batch: 43 | Loss: 12.2077\n",
      "Epoch 110 | Batch: 44 | Loss: 5.5071\n",
      "Epoch 110 | Batch: 45 | Loss: 5.7547\n",
      "Epoch 110 | Batch: 46 | Loss: 6.0247\n",
      "Epoch 110 | Batch: 47 | Loss: 10.8020\n",
      "Epoch 110 | Batch: 48 | Loss: 1.3042\n",
      "Mean 9.679153310755888\n",
      "Epoch 111 | Batch: 1 | Loss: 6.8848\n",
      "Epoch 111 | Batch: 2 | Loss: 4.4838\n",
      "Epoch 111 | Batch: 3 | Loss: 7.8682\n",
      "Epoch 111 | Batch: 4 | Loss: 8.2924\n",
      "Epoch 111 | Batch: 5 | Loss: 5.9700\n",
      "Epoch 111 | Batch: 6 | Loss: 14.7186\n",
      "Epoch 111 | Batch: 7 | Loss: 7.3445\n",
      "Epoch 111 | Batch: 8 | Loss: 15.9576\n",
      "Epoch 111 | Batch: 9 | Loss: 2.9469\n",
      "Epoch 111 | Batch: 10 | Loss: 7.3276\n",
      "Epoch 111 | Batch: 11 | Loss: 11.2594\n",
      "Epoch 111 | Batch: 12 | Loss: 9.7468\n",
      "Epoch 111 | Batch: 13 | Loss: 16.3847\n",
      "Epoch 111 | Batch: 14 | Loss: 13.5592\n",
      "Epoch 111 | Batch: 15 | Loss: 8.2358\n",
      "Epoch 111 | Batch: 16 | Loss: 8.8153\n",
      "Epoch 111 | Batch: 17 | Loss: 5.0392\n",
      "Epoch 111 | Batch: 18 | Loss: 6.1881\n",
      "Epoch 111 | Batch: 19 | Loss: 6.4908\n",
      "Epoch 111 | Batch: 20 | Loss: 6.0730\n",
      "Epoch 111 | Batch: 21 | Loss: 16.8770\n",
      "Epoch 111 | Batch: 22 | Loss: 7.2017\n",
      "Epoch 111 | Batch: 23 | Loss: 8.7503\n",
      "Epoch 111 | Batch: 24 | Loss: 11.7175\n",
      "Epoch 111 | Batch: 25 | Loss: 7.8188\n",
      "Epoch 111 | Batch: 26 | Loss: 5.0067\n",
      "Epoch 111 | Batch: 27 | Loss: 5.4233\n",
      "Epoch 111 | Batch: 28 | Loss: 4.0146\n",
      "Epoch 111 | Batch: 29 | Loss: 8.6370\n",
      "Epoch 111 | Batch: 30 | Loss: 19.0696\n",
      "Epoch 111 | Batch: 31 | Loss: 13.7841\n",
      "Epoch 111 | Batch: 32 | Loss: 20.2209\n",
      "Epoch 111 | Batch: 33 | Loss: 15.4323\n",
      "Epoch 111 | Batch: 34 | Loss: 5.2111\n",
      "Epoch 111 | Batch: 35 | Loss: 11.2783\n",
      "Epoch 111 | Batch: 36 | Loss: 14.9288\n",
      "Epoch 111 | Batch: 37 | Loss: 10.7542\n",
      "Epoch 111 | Batch: 38 | Loss: 4.4146\n",
      "Epoch 111 | Batch: 39 | Loss: 12.4508\n",
      "Epoch 111 | Batch: 40 | Loss: 10.0368\n",
      "Epoch 111 | Batch: 41 | Loss: 5.5338\n",
      "Epoch 111 | Batch: 42 | Loss: 8.0764\n",
      "Epoch 111 | Batch: 43 | Loss: 7.5904\n",
      "Epoch 111 | Batch: 44 | Loss: 6.4221\n",
      "Epoch 111 | Batch: 45 | Loss: 9.9352\n",
      "Epoch 111 | Batch: 46 | Loss: 6.6755\n",
      "Epoch 111 | Batch: 47 | Loss: 7.9452\n",
      "Epoch 111 | Batch: 48 | Loss: 3.0060\n",
      "Mean 9.2041634619236\n",
      "Epoch 112 | Batch: 1 | Loss: 6.6814\n",
      "Epoch 112 | Batch: 2 | Loss: 9.5014\n",
      "Epoch 112 | Batch: 3 | Loss: 4.3806\n",
      "Epoch 112 | Batch: 4 | Loss: 6.1272\n",
      "Epoch 112 | Batch: 5 | Loss: 6.5119\n",
      "Epoch 112 | Batch: 6 | Loss: 6.5434\n",
      "Epoch 112 | Batch: 7 | Loss: 9.9847\n",
      "Epoch 112 | Batch: 8 | Loss: 2.9174\n",
      "Epoch 112 | Batch: 9 | Loss: 12.7674\n",
      "Epoch 112 | Batch: 10 | Loss: 7.6307\n",
      "Epoch 112 | Batch: 11 | Loss: 5.2248\n",
      "Epoch 112 | Batch: 12 | Loss: 4.8569\n",
      "Epoch 112 | Batch: 13 | Loss: 10.0067\n",
      "Epoch 112 | Batch: 14 | Loss: 6.1159\n",
      "Epoch 112 | Batch: 15 | Loss: 3.8938\n",
      "Epoch 112 | Batch: 16 | Loss: 9.8350\n",
      "Epoch 112 | Batch: 17 | Loss: 14.9207\n",
      "Epoch 112 | Batch: 18 | Loss: 6.0802\n",
      "Epoch 112 | Batch: 19 | Loss: 3.7591\n",
      "Epoch 112 | Batch: 20 | Loss: 6.8769\n",
      "Epoch 112 | Batch: 21 | Loss: 12.2183\n",
      "Epoch 112 | Batch: 22 | Loss: 13.1095\n",
      "Epoch 112 | Batch: 23 | Loss: 5.2661\n",
      "Epoch 112 | Batch: 24 | Loss: 10.5514\n",
      "Epoch 112 | Batch: 25 | Loss: 8.2923\n",
      "Epoch 112 | Batch: 26 | Loss: 6.6044\n",
      "Epoch 112 | Batch: 27 | Loss: 4.6359\n",
      "Epoch 112 | Batch: 28 | Loss: 4.7235\n",
      "Epoch 112 | Batch: 29 | Loss: 7.7618\n",
      "Epoch 112 | Batch: 30 | Loss: 11.8028\n",
      "Epoch 112 | Batch: 31 | Loss: 5.6103\n",
      "Epoch 112 | Batch: 32 | Loss: 7.7044\n",
      "Epoch 112 | Batch: 33 | Loss: 9.6908\n",
      "Epoch 112 | Batch: 34 | Loss: 11.6124\n",
      "Epoch 112 | Batch: 35 | Loss: 19.4859\n",
      "Epoch 112 | Batch: 36 | Loss: 9.0711\n",
      "Epoch 112 | Batch: 37 | Loss: 8.6549\n",
      "Epoch 112 | Batch: 38 | Loss: 8.5347\n",
      "Epoch 112 | Batch: 39 | Loss: 11.8002\n",
      "Epoch 112 | Batch: 40 | Loss: 7.0244\n",
      "Epoch 112 | Batch: 41 | Loss: 12.8602\n",
      "Epoch 112 | Batch: 42 | Loss: 17.6983\n",
      "Epoch 112 | Batch: 43 | Loss: 20.4971\n",
      "Epoch 112 | Batch: 44 | Loss: 11.4144\n",
      "Epoch 112 | Batch: 45 | Loss: 6.5831\n",
      "Epoch 112 | Batch: 46 | Loss: 7.6038\n",
      "Epoch 112 | Batch: 47 | Loss: 36.0675\n",
      "Epoch 112 | Batch: 48 | Loss: 9.1879\n",
      "Mean 9.389237801233927\n",
      "Epoch 113 | Batch: 1 | Loss: 21.8445\n",
      "Epoch 113 | Batch: 2 | Loss: 7.3983\n",
      "Epoch 113 | Batch: 3 | Loss: 11.1239\n",
      "Epoch 113 | Batch: 4 | Loss: 11.1129\n",
      "Epoch 113 | Batch: 5 | Loss: 14.0184\n",
      "Epoch 113 | Batch: 6 | Loss: 11.1222\n",
      "Epoch 113 | Batch: 7 | Loss: 15.5640\n",
      "Epoch 113 | Batch: 8 | Loss: 12.4926\n",
      "Epoch 113 | Batch: 9 | Loss: 6.1368\n",
      "Epoch 113 | Batch: 10 | Loss: 5.2404\n",
      "Epoch 113 | Batch: 11 | Loss: 8.4635\n",
      "Epoch 113 | Batch: 12 | Loss: 11.9482\n",
      "Epoch 113 | Batch: 13 | Loss: 10.9513\n",
      "Epoch 113 | Batch: 14 | Loss: 10.5330\n",
      "Epoch 113 | Batch: 15 | Loss: 7.9042\n",
      "Epoch 113 | Batch: 16 | Loss: 8.2762\n",
      "Epoch 113 | Batch: 17 | Loss: 4.5654\n",
      "Epoch 113 | Batch: 18 | Loss: 6.9798\n",
      "Epoch 113 | Batch: 19 | Loss: 4.3143\n",
      "Epoch 113 | Batch: 20 | Loss: 3.0117\n",
      "Epoch 113 | Batch: 21 | Loss: 3.6448\n",
      "Epoch 113 | Batch: 22 | Loss: 9.5250\n",
      "Epoch 113 | Batch: 23 | Loss: 17.8209\n",
      "Epoch 113 | Batch: 24 | Loss: 13.5738\n",
      "Epoch 113 | Batch: 25 | Loss: 7.6085\n",
      "Epoch 113 | Batch: 26 | Loss: 10.2893\n",
      "Epoch 113 | Batch: 27 | Loss: 6.0888\n",
      "Epoch 113 | Batch: 28 | Loss: 5.1173\n",
      "Epoch 113 | Batch: 29 | Loss: 16.0997\n",
      "Epoch 113 | Batch: 30 | Loss: 24.6395\n",
      "Epoch 113 | Batch: 31 | Loss: 14.4863\n",
      "Epoch 113 | Batch: 32 | Loss: 7.5719\n",
      "Epoch 113 | Batch: 33 | Loss: 5.1926\n",
      "Epoch 113 | Batch: 34 | Loss: 4.4365\n",
      "Epoch 113 | Batch: 35 | Loss: 23.8397\n",
      "Epoch 113 | Batch: 36 | Loss: 17.2736\n",
      "Epoch 113 | Batch: 37 | Loss: 24.9444\n",
      "Epoch 113 | Batch: 38 | Loss: 12.3965\n",
      "Epoch 113 | Batch: 39 | Loss: 11.3607\n",
      "Epoch 113 | Batch: 40 | Loss: 11.7173\n",
      "Epoch 113 | Batch: 41 | Loss: 10.9274\n",
      "Epoch 113 | Batch: 42 | Loss: 10.2301\n",
      "Epoch 113 | Batch: 43 | Loss: 7.0206\n",
      "Epoch 113 | Batch: 44 | Loss: 9.4073\n",
      "Epoch 113 | Batch: 45 | Loss: 6.1746\n",
      "Epoch 113 | Batch: 46 | Loss: 4.9647\n",
      "Epoch 113 | Batch: 47 | Loss: 16.4823\n",
      "Epoch 113 | Batch: 48 | Loss: 11.4737\n",
      "Mean 10.777288471659025\n",
      "Epoch 114 | Batch: 1 | Loss: 8.3535\n",
      "Epoch 114 | Batch: 2 | Loss: 10.9591\n",
      "Epoch 114 | Batch: 3 | Loss: 6.3905\n",
      "Epoch 114 | Batch: 4 | Loss: 8.6088\n",
      "Epoch 114 | Batch: 5 | Loss: 5.8506\n",
      "Epoch 114 | Batch: 6 | Loss: 8.1168\n",
      "Epoch 114 | Batch: 7 | Loss: 3.8026\n",
      "Epoch 114 | Batch: 8 | Loss: 9.6196\n",
      "Epoch 114 | Batch: 9 | Loss: 10.9083\n",
      "Epoch 114 | Batch: 10 | Loss: 8.7195\n",
      "Epoch 114 | Batch: 11 | Loss: 16.6641\n",
      "Epoch 114 | Batch: 12 | Loss: 11.4376\n",
      "Epoch 114 | Batch: 13 | Loss: 16.6179\n",
      "Epoch 114 | Batch: 14 | Loss: 7.7266\n",
      "Epoch 114 | Batch: 15 | Loss: 16.7803\n",
      "Epoch 114 | Batch: 16 | Loss: 15.1901\n",
      "Epoch 114 | Batch: 17 | Loss: 31.0062\n",
      "Epoch 114 | Batch: 18 | Loss: 3.8331\n",
      "Epoch 114 | Batch: 19 | Loss: 7.3288\n",
      "Epoch 114 | Batch: 20 | Loss: 7.1978\n",
      "Epoch 114 | Batch: 21 | Loss: 4.2358\n",
      "Epoch 114 | Batch: 22 | Loss: 4.3868\n",
      "Epoch 114 | Batch: 23 | Loss: 6.6568\n",
      "Epoch 114 | Batch: 24 | Loss: 12.0500\n",
      "Epoch 114 | Batch: 25 | Loss: 2.7903\n",
      "Epoch 114 | Batch: 26 | Loss: 11.8889\n",
      "Epoch 114 | Batch: 27 | Loss: 6.8007\n",
      "Epoch 114 | Batch: 28 | Loss: 14.4208\n",
      "Epoch 114 | Batch: 29 | Loss: 6.4931\n",
      "Epoch 114 | Batch: 30 | Loss: 11.8261\n",
      "Epoch 114 | Batch: 31 | Loss: 9.5569\n",
      "Epoch 114 | Batch: 32 | Loss: 8.9955\n",
      "Epoch 114 | Batch: 33 | Loss: 13.1482\n",
      "Epoch 114 | Batch: 34 | Loss: 16.7106\n",
      "Epoch 114 | Batch: 35 | Loss: 24.9764\n",
      "Epoch 114 | Batch: 36 | Loss: 1.7565\n",
      "Epoch 114 | Batch: 37 | Loss: 6.2731\n",
      "Epoch 114 | Batch: 38 | Loss: 6.7155\n",
      "Epoch 114 | Batch: 39 | Loss: 8.3534\n",
      "Epoch 114 | Batch: 40 | Loss: 9.5209\n",
      "Epoch 114 | Batch: 41 | Loss: 8.8097\n",
      "Epoch 114 | Batch: 42 | Loss: 6.4493\n",
      "Epoch 114 | Batch: 43 | Loss: 8.9262\n",
      "Epoch 114 | Batch: 44 | Loss: 10.5223\n",
      "Epoch 114 | Batch: 45 | Loss: 13.1917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114 | Batch: 46 | Loss: 9.0905\n",
      "Epoch 114 | Batch: 47 | Loss: 14.2894\n",
      "Epoch 114 | Batch: 48 | Loss: 2.7529\n",
      "Mean 9.931248265008131\n",
      "Epoch 115 | Batch: 1 | Loss: 9.4204\n",
      "Epoch 115 | Batch: 2 | Loss: 5.9483\n",
      "Epoch 115 | Batch: 3 | Loss: 8.6152\n",
      "Epoch 115 | Batch: 4 | Loss: 11.5263\n",
      "Epoch 115 | Batch: 5 | Loss: 6.7689\n",
      "Epoch 115 | Batch: 6 | Loss: 10.5098\n",
      "Epoch 115 | Batch: 7 | Loss: 6.3515\n",
      "Epoch 115 | Batch: 8 | Loss: 7.7936\n",
      "Epoch 115 | Batch: 9 | Loss: 9.8813\n",
      "Epoch 115 | Batch: 10 | Loss: 11.5667\n",
      "Epoch 115 | Batch: 11 | Loss: 6.2840\n",
      "Epoch 115 | Batch: 12 | Loss: 10.4179\n",
      "Epoch 115 | Batch: 13 | Loss: 13.5179\n",
      "Epoch 115 | Batch: 14 | Loss: 6.8265\n",
      "Epoch 115 | Batch: 15 | Loss: 13.8656\n",
      "Epoch 115 | Batch: 16 | Loss: 12.8845\n",
      "Epoch 115 | Batch: 17 | Loss: 12.3198\n",
      "Epoch 115 | Batch: 18 | Loss: 7.0497\n",
      "Epoch 115 | Batch: 19 | Loss: 7.0738\n",
      "Epoch 115 | Batch: 20 | Loss: 9.1478\n",
      "Epoch 115 | Batch: 21 | Loss: 9.3138\n",
      "Epoch 115 | Batch: 22 | Loss: 8.4678\n",
      "Epoch 115 | Batch: 23 | Loss: 8.0534\n",
      "Epoch 115 | Batch: 24 | Loss: 10.7547\n",
      "Epoch 115 | Batch: 25 | Loss: 11.7065\n",
      "Epoch 115 | Batch: 26 | Loss: 18.7418\n",
      "Epoch 115 | Batch: 27 | Loss: 15.2499\n",
      "Epoch 115 | Batch: 28 | Loss: 8.8161\n",
      "Epoch 115 | Batch: 29 | Loss: 5.3899\n",
      "Epoch 115 | Batch: 30 | Loss: 3.2522\n",
      "Epoch 115 | Batch: 31 | Loss: 5.5556\n",
      "Epoch 115 | Batch: 32 | Loss: 4.1304\n",
      "Epoch 115 | Batch: 33 | Loss: 5.2638\n",
      "Epoch 115 | Batch: 34 | Loss: 8.3239\n",
      "Epoch 115 | Batch: 35 | Loss: 14.0606\n",
      "Epoch 115 | Batch: 36 | Loss: 11.5571\n",
      "Epoch 115 | Batch: 37 | Loss: 4.4600\n",
      "Epoch 115 | Batch: 38 | Loss: 8.7520\n",
      "Epoch 115 | Batch: 39 | Loss: 12.3920\n",
      "Epoch 115 | Batch: 40 | Loss: 4.3545\n",
      "Epoch 115 | Batch: 41 | Loss: 8.0201\n",
      "Epoch 115 | Batch: 42 | Loss: 9.9393\n",
      "Epoch 115 | Batch: 43 | Loss: 4.5826\n",
      "Epoch 115 | Batch: 44 | Loss: 8.8256\n",
      "Epoch 115 | Batch: 45 | Loss: 5.5835\n",
      "Epoch 115 | Batch: 46 | Loss: 8.7259\n",
      "Epoch 115 | Batch: 47 | Loss: 11.7004\n",
      "Epoch 115 | Batch: 48 | Loss: 13.4220\n",
      "Mean 9.107603296637535\n",
      "Epoch 116 | Batch: 1 | Loss: 5.4264\n",
      "Epoch 116 | Batch: 2 | Loss: 9.0148\n",
      "Epoch 116 | Batch: 3 | Loss: 11.8434\n",
      "Epoch 116 | Batch: 4 | Loss: 4.6968\n",
      "Epoch 116 | Batch: 5 | Loss: 6.9449\n",
      "Epoch 116 | Batch: 6 | Loss: 6.9683\n",
      "Epoch 116 | Batch: 7 | Loss: 3.3458\n",
      "Epoch 116 | Batch: 8 | Loss: 7.8813\n",
      "Epoch 116 | Batch: 9 | Loss: 7.4076\n",
      "Epoch 116 | Batch: 10 | Loss: 7.8994\n",
      "Epoch 116 | Batch: 11 | Loss: 5.1489\n",
      "Epoch 116 | Batch: 12 | Loss: 11.1701\n",
      "Epoch 116 | Batch: 13 | Loss: 8.7337\n",
      "Epoch 116 | Batch: 14 | Loss: 7.8776\n",
      "Epoch 116 | Batch: 15 | Loss: 14.6129\n",
      "Epoch 116 | Batch: 16 | Loss: 8.9250\n",
      "Epoch 116 | Batch: 17 | Loss: 4.9768\n",
      "Epoch 116 | Batch: 18 | Loss: 14.0971\n",
      "Epoch 116 | Batch: 19 | Loss: 4.3786\n",
      "Epoch 116 | Batch: 20 | Loss: 7.8179\n",
      "Epoch 116 | Batch: 21 | Loss: 5.2492\n",
      "Epoch 116 | Batch: 22 | Loss: 10.6098\n",
      "Epoch 116 | Batch: 23 | Loss: 6.6750\n",
      "Epoch 116 | Batch: 24 | Loss: 10.3146\n",
      "Epoch 116 | Batch: 25 | Loss: 3.1695\n",
      "Epoch 116 | Batch: 26 | Loss: 6.0652\n",
      "Epoch 116 | Batch: 27 | Loss: 11.6551\n",
      "Epoch 116 | Batch: 28 | Loss: 19.2134\n",
      "Epoch 116 | Batch: 29 | Loss: 10.6906\n",
      "Epoch 116 | Batch: 30 | Loss: 7.8971\n",
      "Epoch 116 | Batch: 31 | Loss: 5.4902\n",
      "Epoch 116 | Batch: 32 | Loss: 3.6526\n",
      "Epoch 116 | Batch: 33 | Loss: 8.0341\n",
      "Epoch 116 | Batch: 34 | Loss: 7.2287\n",
      "Epoch 116 | Batch: 35 | Loss: 6.4863\n",
      "Epoch 116 | Batch: 36 | Loss: 4.0561\n",
      "Epoch 116 | Batch: 37 | Loss: 8.6937\n",
      "Epoch 116 | Batch: 38 | Loss: 14.9883\n",
      "Epoch 116 | Batch: 39 | Loss: 15.5617\n",
      "Epoch 116 | Batch: 40 | Loss: 27.6586\n",
      "Epoch 116 | Batch: 41 | Loss: 18.7142\n",
      "Epoch 116 | Batch: 42 | Loss: 9.7101\n",
      "Epoch 116 | Batch: 43 | Loss: 8.5746\n",
      "Epoch 116 | Batch: 44 | Loss: 7.8352\n",
      "Epoch 116 | Batch: 45 | Loss: 7.8923\n",
      "Epoch 116 | Batch: 46 | Loss: 5.5592\n",
      "Epoch 116 | Batch: 47 | Loss: 9.8450\n",
      "Epoch 116 | Batch: 48 | Loss: 8.8313\n",
      "Mean 8.94831054409345\n",
      "Epoch 117 | Batch: 1 | Loss: 7.8720\n",
      "Epoch 117 | Batch: 2 | Loss: 10.9182\n",
      "Epoch 117 | Batch: 3 | Loss: 8.2430\n",
      "Epoch 117 | Batch: 4 | Loss: 10.8778\n",
      "Epoch 117 | Batch: 5 | Loss: 3.9788\n",
      "Epoch 117 | Batch: 6 | Loss: 8.6314\n",
      "Epoch 117 | Batch: 7 | Loss: 14.5577\n",
      "Epoch 117 | Batch: 8 | Loss: 11.1715\n",
      "Epoch 117 | Batch: 9 | Loss: 7.9387\n",
      "Epoch 117 | Batch: 10 | Loss: 9.0697\n",
      "Epoch 117 | Batch: 11 | Loss: 9.3055\n",
      "Epoch 117 | Batch: 12 | Loss: 7.9084\n",
      "Epoch 117 | Batch: 13 | Loss: 6.0643\n",
      "Epoch 117 | Batch: 14 | Loss: 4.8887\n",
      "Epoch 117 | Batch: 15 | Loss: 9.6281\n",
      "Epoch 117 | Batch: 16 | Loss: 9.4516\n",
      "Epoch 117 | Batch: 17 | Loss: 4.7973\n",
      "Epoch 117 | Batch: 18 | Loss: 8.0922\n",
      "Epoch 117 | Batch: 19 | Loss: 7.5956\n",
      "Epoch 117 | Batch: 20 | Loss: 7.3601\n",
      "Epoch 117 | Batch: 21 | Loss: 8.1585\n",
      "Epoch 117 | Batch: 22 | Loss: 14.2314\n",
      "Epoch 117 | Batch: 23 | Loss: 11.0927\n",
      "Epoch 117 | Batch: 24 | Loss: 13.7840\n",
      "Epoch 117 | Batch: 25 | Loss: 8.3198\n",
      "Epoch 117 | Batch: 26 | Loss: 6.4317\n",
      "Epoch 117 | Batch: 27 | Loss: 4.2684\n",
      "Epoch 117 | Batch: 28 | Loss: 5.4331\n",
      "Epoch 117 | Batch: 29 | Loss: 6.2997\n",
      "Epoch 117 | Batch: 30 | Loss: 6.3288\n",
      "Epoch 117 | Batch: 31 | Loss: 8.0774\n",
      "Epoch 117 | Batch: 32 | Loss: 5.9620\n",
      "Epoch 117 | Batch: 33 | Loss: 8.6346\n",
      "Epoch 117 | Batch: 34 | Loss: 8.3774\n",
      "Epoch 117 | Batch: 35 | Loss: 8.0207\n",
      "Epoch 117 | Batch: 36 | Loss: 9.0076\n",
      "Epoch 117 | Batch: 37 | Loss: 6.6726\n",
      "Epoch 117 | Batch: 38 | Loss: 8.7139\n",
      "Epoch 117 | Batch: 39 | Loss: 3.7130\n",
      "Epoch 117 | Batch: 40 | Loss: 4.2783\n",
      "Epoch 117 | Batch: 41 | Loss: 4.7918\n",
      "Epoch 117 | Batch: 42 | Loss: 15.5867\n",
      "Epoch 117 | Batch: 43 | Loss: 9.7946\n",
      "Epoch 117 | Batch: 44 | Loss: 4.8791\n",
      "Epoch 117 | Batch: 45 | Loss: 5.5918\n",
      "Epoch 117 | Batch: 46 | Loss: 3.8257\n",
      "Epoch 117 | Batch: 47 | Loss: 6.0180\n",
      "Epoch 117 | Batch: 48 | Loss: 2.3712\n",
      "Mean 7.854479079445203\n",
      "Epoch 118 | Batch: 1 | Loss: 2.3988\n",
      "Epoch 118 | Batch: 2 | Loss: 5.3209\n",
      "Epoch 118 | Batch: 3 | Loss: 5.6883\n",
      "Epoch 118 | Batch: 4 | Loss: 5.9721\n",
      "Epoch 118 | Batch: 5 | Loss: 6.5806\n",
      "Epoch 118 | Batch: 6 | Loss: 5.7919\n",
      "Epoch 118 | Batch: 7 | Loss: 8.4626\n",
      "Epoch 118 | Batch: 8 | Loss: 7.9855\n",
      "Epoch 118 | Batch: 9 | Loss: 6.5511\n",
      "Epoch 118 | Batch: 10 | Loss: 9.5791\n",
      "Epoch 118 | Batch: 11 | Loss: 10.6658\n",
      "Epoch 118 | Batch: 12 | Loss: 6.5180\n",
      "Epoch 118 | Batch: 13 | Loss: 8.4124\n",
      "Epoch 118 | Batch: 14 | Loss: 7.9291\n",
      "Epoch 118 | Batch: 15 | Loss: 4.9408\n",
      "Epoch 118 | Batch: 16 | Loss: 7.5739\n",
      "Epoch 118 | Batch: 17 | Loss: 9.6935\n",
      "Epoch 118 | Batch: 18 | Loss: 15.8625\n",
      "Epoch 118 | Batch: 19 | Loss: 21.1874\n",
      "Epoch 118 | Batch: 20 | Loss: 9.8078\n",
      "Epoch 118 | Batch: 21 | Loss: 6.3065\n",
      "Epoch 118 | Batch: 22 | Loss: 9.5712\n",
      "Epoch 118 | Batch: 23 | Loss: 9.0647\n",
      "Epoch 118 | Batch: 24 | Loss: 7.8048\n",
      "Epoch 118 | Batch: 25 | Loss: 7.8143\n",
      "Epoch 118 | Batch: 26 | Loss: 5.1343\n",
      "Epoch 118 | Batch: 27 | Loss: 13.5467\n",
      "Epoch 118 | Batch: 28 | Loss: 5.4413\n",
      "Epoch 118 | Batch: 29 | Loss: 15.1663\n",
      "Epoch 118 | Batch: 30 | Loss: 15.8728\n",
      "Epoch 118 | Batch: 31 | Loss: 5.3266\n",
      "Epoch 118 | Batch: 32 | Loss: 10.4959\n",
      "Epoch 118 | Batch: 33 | Loss: 8.3961\n",
      "Epoch 118 | Batch: 34 | Loss: 11.2165\n",
      "Epoch 118 | Batch: 35 | Loss: 6.9567\n",
      "Epoch 118 | Batch: 36 | Loss: 9.9343\n",
      "Epoch 118 | Batch: 37 | Loss: 4.9624\n",
      "Epoch 118 | Batch: 38 | Loss: 4.7758\n",
      "Epoch 118 | Batch: 39 | Loss: 10.5907\n",
      "Epoch 118 | Batch: 40 | Loss: 10.7158\n",
      "Epoch 118 | Batch: 41 | Loss: 8.5223\n",
      "Epoch 118 | Batch: 42 | Loss: 13.9194\n",
      "Epoch 118 | Batch: 43 | Loss: 13.5681\n",
      "Epoch 118 | Batch: 44 | Loss: 7.9617\n",
      "Epoch 118 | Batch: 45 | Loss: 5.4142\n",
      "Epoch 118 | Batch: 46 | Loss: 7.5179\n",
      "Epoch 118 | Batch: 47 | Loss: 8.6281\n",
      "Epoch 118 | Batch: 48 | Loss: 5.1776\n",
      "Mean 8.68176736931006\n",
      "Epoch 119 | Batch: 1 | Loss: 6.8340\n",
      "Epoch 119 | Batch: 2 | Loss: 10.4621\n",
      "Epoch 119 | Batch: 3 | Loss: 5.8892\n",
      "Epoch 119 | Batch: 4 | Loss: 12.9714\n",
      "Epoch 119 | Batch: 5 | Loss: 5.4238\n",
      "Epoch 119 | Batch: 6 | Loss: 14.5373\n",
      "Epoch 119 | Batch: 7 | Loss: 12.3188\n",
      "Epoch 119 | Batch: 8 | Loss: 8.5838\n",
      "Epoch 119 | Batch: 9 | Loss: 13.3305\n",
      "Epoch 119 | Batch: 10 | Loss: 8.2572\n",
      "Epoch 119 | Batch: 11 | Loss: 14.0663\n",
      "Epoch 119 | Batch: 12 | Loss: 15.8668\n",
      "Epoch 119 | Batch: 13 | Loss: 7.8284\n",
      "Epoch 119 | Batch: 14 | Loss: 16.8854\n",
      "Epoch 119 | Batch: 15 | Loss: 32.0374\n",
      "Epoch 119 | Batch: 16 | Loss: 37.5650\n",
      "Epoch 119 | Batch: 17 | Loss: 7.5444\n",
      "Epoch 119 | Batch: 18 | Loss: 5.2033\n",
      "Epoch 119 | Batch: 19 | Loss: 14.2144\n",
      "Epoch 119 | Batch: 20 | Loss: 8.1494\n",
      "Epoch 119 | Batch: 21 | Loss: 10.5053\n",
      "Epoch 119 | Batch: 22 | Loss: 12.5938\n",
      "Epoch 119 | Batch: 23 | Loss: 7.1493\n",
      "Epoch 119 | Batch: 24 | Loss: 3.8044\n",
      "Epoch 119 | Batch: 25 | Loss: 7.1837\n",
      "Epoch 119 | Batch: 26 | Loss: 8.2150\n",
      "Epoch 119 | Batch: 27 | Loss: 6.2967\n",
      "Epoch 119 | Batch: 28 | Loss: 15.5532\n",
      "Epoch 119 | Batch: 29 | Loss: 12.8493\n",
      "Epoch 119 | Batch: 30 | Loss: 13.9114\n",
      "Epoch 119 | Batch: 31 | Loss: 9.4029\n",
      "Epoch 119 | Batch: 32 | Loss: 12.6986\n",
      "Epoch 119 | Batch: 33 | Loss: 20.2196\n",
      "Epoch 119 | Batch: 34 | Loss: 8.0060\n",
      "Epoch 119 | Batch: 35 | Loss: 7.0852\n",
      "Epoch 119 | Batch: 36 | Loss: 6.1236\n",
      "Epoch 119 | Batch: 37 | Loss: 9.1600\n",
      "Epoch 119 | Batch: 38 | Loss: 6.5691\n",
      "Epoch 119 | Batch: 39 | Loss: 6.6047\n",
      "Epoch 119 | Batch: 40 | Loss: 5.0955\n",
      "Epoch 119 | Batch: 41 | Loss: 10.6021\n",
      "Epoch 119 | Batch: 42 | Loss: 9.3252\n",
      "Epoch 119 | Batch: 43 | Loss: 6.8471\n",
      "Epoch 119 | Batch: 44 | Loss: 10.0591\n",
      "Epoch 119 | Batch: 45 | Loss: 11.8242\n",
      "Epoch 119 | Batch: 46 | Loss: 23.0508\n",
      "Epoch 119 | Batch: 47 | Loss: 13.3396\n",
      "Epoch 119 | Batch: 48 | Loss: 1.9270\n",
      "Mean 11.12439894179503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120 | Batch: 1 | Loss: 5.0693\n",
      "Epoch 120 | Batch: 2 | Loss: 8.9182\n",
      "Epoch 120 | Batch: 3 | Loss: 8.7648\n",
      "Epoch 120 | Batch: 4 | Loss: 26.6926\n",
      "Epoch 120 | Batch: 5 | Loss: 33.4816\n",
      "Epoch 120 | Batch: 6 | Loss: 10.3915\n",
      "Epoch 120 | Batch: 7 | Loss: 10.0946\n",
      "Epoch 120 | Batch: 8 | Loss: 8.1977\n",
      "Epoch 120 | Batch: 9 | Loss: 5.4242\n",
      "Epoch 120 | Batch: 10 | Loss: 6.2823\n",
      "Epoch 120 | Batch: 11 | Loss: 9.1257\n",
      "Epoch 120 | Batch: 12 | Loss: 8.3400\n",
      "Epoch 120 | Batch: 13 | Loss: 6.2910\n",
      "Epoch 120 | Batch: 14 | Loss: 5.7362\n",
      "Epoch 120 | Batch: 15 | Loss: 13.6740\n",
      "Epoch 120 | Batch: 16 | Loss: 14.5117\n",
      "Epoch 120 | Batch: 17 | Loss: 8.0424\n",
      "Epoch 120 | Batch: 18 | Loss: 4.0581\n",
      "Epoch 120 | Batch: 19 | Loss: 11.0345\n",
      "Epoch 120 | Batch: 20 | Loss: 21.2312\n",
      "Epoch 120 | Batch: 21 | Loss: 8.0982\n",
      "Epoch 120 | Batch: 22 | Loss: 4.9125\n",
      "Epoch 120 | Batch: 23 | Loss: 10.8034\n",
      "Epoch 120 | Batch: 24 | Loss: 9.6703\n",
      "Epoch 120 | Batch: 25 | Loss: 3.8758\n",
      "Epoch 120 | Batch: 26 | Loss: 11.9144\n",
      "Epoch 120 | Batch: 27 | Loss: 7.1217\n",
      "Epoch 120 | Batch: 28 | Loss: 2.8435\n",
      "Epoch 120 | Batch: 29 | Loss: 7.1243\n",
      "Epoch 120 | Batch: 30 | Loss: 5.1152\n",
      "Epoch 120 | Batch: 31 | Loss: 4.7848\n",
      "Epoch 120 | Batch: 32 | Loss: 9.3751\n",
      "Epoch 120 | Batch: 33 | Loss: 2.8059\n",
      "Epoch 120 | Batch: 34 | Loss: 9.3916\n",
      "Epoch 120 | Batch: 35 | Loss: 8.1825\n",
      "Epoch 120 | Batch: 36 | Loss: 10.5021\n",
      "Epoch 120 | Batch: 37 | Loss: 29.4790\n",
      "Epoch 120 | Batch: 38 | Loss: 18.8444\n",
      "Epoch 120 | Batch: 39 | Loss: 20.3096\n",
      "Epoch 120 | Batch: 40 | Loss: 11.0906\n",
      "Epoch 120 | Batch: 41 | Loss: 6.8801\n",
      "Epoch 120 | Batch: 42 | Loss: 10.8549\n",
      "Epoch 120 | Batch: 43 | Loss: 4.2082\n",
      "Epoch 120 | Batch: 44 | Loss: 12.0254\n",
      "Epoch 120 | Batch: 45 | Loss: 9.1818\n",
      "Epoch 120 | Batch: 46 | Loss: 6.8967\n",
      "Epoch 120 | Batch: 47 | Loss: 9.1602\n",
      "Epoch 120 | Batch: 48 | Loss: 6.2087\n",
      "Mean 10.146303787827492\n",
      "Epoch 121 | Batch: 1 | Loss: 5.8167\n",
      "Epoch 121 | Batch: 2 | Loss: 10.9886\n",
      "Epoch 121 | Batch: 3 | Loss: 10.0579\n",
      "Epoch 121 | Batch: 4 | Loss: 12.6009\n",
      "Epoch 121 | Batch: 5 | Loss: 17.3009\n",
      "Epoch 121 | Batch: 6 | Loss: 10.5769\n",
      "Epoch 121 | Batch: 7 | Loss: 8.0774\n",
      "Epoch 121 | Batch: 8 | Loss: 7.1864\n",
      "Epoch 121 | Batch: 9 | Loss: 9.6873\n",
      "Epoch 121 | Batch: 10 | Loss: 13.6802\n",
      "Epoch 121 | Batch: 11 | Loss: 10.0847\n",
      "Epoch 121 | Batch: 12 | Loss: 10.0033\n",
      "Epoch 121 | Batch: 13 | Loss: 7.4421\n",
      "Epoch 121 | Batch: 14 | Loss: 4.8496\n",
      "Epoch 121 | Batch: 15 | Loss: 5.4798\n",
      "Epoch 121 | Batch: 16 | Loss: 8.1398\n",
      "Epoch 121 | Batch: 17 | Loss: 2.7061\n",
      "Epoch 121 | Batch: 18 | Loss: 4.4969\n",
      "Epoch 121 | Batch: 19 | Loss: 5.4885\n",
      "Epoch 121 | Batch: 20 | Loss: 9.6404\n",
      "Epoch 121 | Batch: 21 | Loss: 11.8520\n",
      "Epoch 121 | Batch: 22 | Loss: 5.6432\n",
      "Epoch 121 | Batch: 23 | Loss: 3.6837\n",
      "Epoch 121 | Batch: 24 | Loss: 10.7337\n",
      "Epoch 121 | Batch: 25 | Loss: 5.9008\n",
      "Epoch 121 | Batch: 26 | Loss: 9.9083\n",
      "Epoch 121 | Batch: 27 | Loss: 11.7429\n",
      "Epoch 121 | Batch: 28 | Loss: 11.1008\n",
      "Epoch 121 | Batch: 29 | Loss: 9.6011\n",
      "Epoch 121 | Batch: 30 | Loss: 11.0487\n",
      "Epoch 121 | Batch: 31 | Loss: 12.9519\n",
      "Epoch 121 | Batch: 32 | Loss: 7.9701\n",
      "Epoch 121 | Batch: 33 | Loss: 8.1464\n",
      "Epoch 121 | Batch: 34 | Loss: 5.9092\n",
      "Epoch 121 | Batch: 35 | Loss: 8.5874\n",
      "Epoch 121 | Batch: 36 | Loss: 8.4678\n",
      "Epoch 121 | Batch: 37 | Loss: 11.7666\n",
      "Epoch 121 | Batch: 38 | Loss: 12.5098\n",
      "Epoch 121 | Batch: 39 | Loss: 5.9127\n",
      "Epoch 121 | Batch: 40 | Loss: 15.4664\n",
      "Epoch 121 | Batch: 41 | Loss: 8.0906\n",
      "Epoch 121 | Batch: 42 | Loss: 10.5455\n",
      "Epoch 121 | Batch: 43 | Loss: 6.2760\n",
      "Epoch 121 | Batch: 44 | Loss: 10.9641\n",
      "Epoch 121 | Batch: 45 | Loss: 6.9251\n",
      "Epoch 121 | Batch: 46 | Loss: 7.8445\n",
      "Epoch 121 | Batch: 47 | Loss: 11.5974\n",
      "Epoch 121 | Batch: 48 | Loss: 5.6162\n",
      "Mean 8.980574205517769\n",
      "Epoch 122 | Batch: 1 | Loss: 12.7404\n",
      "Epoch 122 | Batch: 2 | Loss: 9.0096\n",
      "Epoch 122 | Batch: 3 | Loss: 6.9755\n",
      "Epoch 122 | Batch: 4 | Loss: 5.8022\n",
      "Epoch 122 | Batch: 5 | Loss: 8.6874\n",
      "Epoch 122 | Batch: 6 | Loss: 10.9114\n",
      "Epoch 122 | Batch: 7 | Loss: 6.0415\n",
      "Epoch 122 | Batch: 8 | Loss: 7.5962\n",
      "Epoch 122 | Batch: 9 | Loss: 21.6442\n",
      "Epoch 122 | Batch: 10 | Loss: 13.0532\n",
      "Epoch 122 | Batch: 11 | Loss: 7.5529\n",
      "Epoch 122 | Batch: 12 | Loss: 2.7318\n",
      "Epoch 122 | Batch: 13 | Loss: 9.8672\n",
      "Epoch 122 | Batch: 14 | Loss: 11.1737\n",
      "Epoch 122 | Batch: 15 | Loss: 15.5657\n",
      "Epoch 122 | Batch: 16 | Loss: 11.2480\n",
      "Epoch 122 | Batch: 17 | Loss: 3.9342\n",
      "Epoch 122 | Batch: 18 | Loss: 5.7571\n",
      "Epoch 122 | Batch: 19 | Loss: 8.7100\n",
      "Epoch 122 | Batch: 20 | Loss: 8.7659\n",
      "Epoch 122 | Batch: 21 | Loss: 7.6891\n",
      "Epoch 122 | Batch: 22 | Loss: 5.4942\n",
      "Epoch 122 | Batch: 23 | Loss: 5.6483\n",
      "Epoch 122 | Batch: 24 | Loss: 5.4839\n",
      "Epoch 122 | Batch: 25 | Loss: 11.8820\n",
      "Epoch 122 | Batch: 26 | Loss: 7.0669\n",
      "Epoch 122 | Batch: 27 | Loss: 4.2015\n",
      "Epoch 122 | Batch: 28 | Loss: 11.0432\n",
      "Epoch 122 | Batch: 29 | Loss: 15.3857\n",
      "Epoch 122 | Batch: 30 | Loss: 15.9136\n",
      "Epoch 122 | Batch: 31 | Loss: 11.9328\n",
      "Epoch 122 | Batch: 32 | Loss: 3.0759\n",
      "Epoch 122 | Batch: 33 | Loss: 8.4829\n",
      "Epoch 122 | Batch: 34 | Loss: 9.8590\n",
      "Epoch 122 | Batch: 35 | Loss: 4.2608\n",
      "Epoch 122 | Batch: 36 | Loss: 4.7312\n",
      "Epoch 122 | Batch: 37 | Loss: 11.8826\n",
      "Epoch 122 | Batch: 38 | Loss: 11.8979\n",
      "Epoch 122 | Batch: 39 | Loss: 4.8471\n",
      "Epoch 122 | Batch: 40 | Loss: 4.0882\n",
      "Epoch 122 | Batch: 41 | Loss: 18.3447\n",
      "Epoch 122 | Batch: 42 | Loss: 10.2667\n",
      "Epoch 122 | Batch: 43 | Loss: 7.0392\n",
      "Epoch 122 | Batch: 44 | Loss: 7.1686\n",
      "Epoch 122 | Batch: 45 | Loss: 6.5694\n",
      "Epoch 122 | Batch: 46 | Loss: 5.0105\n",
      "Epoch 122 | Batch: 47 | Loss: 3.6487\n",
      "Epoch 122 | Batch: 48 | Loss: 1.7538\n",
      "Mean 8.59242839117845\n",
      "Epoch 123 | Batch: 1 | Loss: 3.0716\n",
      "Epoch 123 | Batch: 2 | Loss: 10.6508\n",
      "Epoch 123 | Batch: 3 | Loss: 6.8508\n",
      "Epoch 123 | Batch: 4 | Loss: 8.4667\n",
      "Epoch 123 | Batch: 5 | Loss: 9.5976\n",
      "Epoch 123 | Batch: 6 | Loss: 4.4787\n",
      "Epoch 123 | Batch: 7 | Loss: 6.5351\n",
      "Epoch 123 | Batch: 8 | Loss: 6.1220\n",
      "Epoch 123 | Batch: 9 | Loss: 2.9416\n",
      "Epoch 123 | Batch: 10 | Loss: 11.3460\n",
      "Epoch 123 | Batch: 11 | Loss: 4.3569\n",
      "Epoch 123 | Batch: 12 | Loss: 10.7250\n",
      "Epoch 123 | Batch: 13 | Loss: 2.8326\n",
      "Epoch 123 | Batch: 14 | Loss: 6.2341\n",
      "Epoch 123 | Batch: 15 | Loss: 10.9629\n",
      "Epoch 123 | Batch: 16 | Loss: 8.0809\n",
      "Epoch 123 | Batch: 17 | Loss: 8.7059\n",
      "Epoch 123 | Batch: 18 | Loss: 9.1940\n",
      "Epoch 123 | Batch: 19 | Loss: 6.4968\n",
      "Epoch 123 | Batch: 20 | Loss: 13.1021\n",
      "Epoch 123 | Batch: 21 | Loss: 9.9880\n",
      "Epoch 123 | Batch: 22 | Loss: 10.9943\n",
      "Epoch 123 | Batch: 23 | Loss: 7.6963\n",
      "Epoch 123 | Batch: 24 | Loss: 12.2094\n",
      "Epoch 123 | Batch: 25 | Loss: 4.5365\n",
      "Epoch 123 | Batch: 26 | Loss: 8.4753\n",
      "Epoch 123 | Batch: 27 | Loss: 9.6608\n",
      "Epoch 123 | Batch: 28 | Loss: 8.8491\n",
      "Epoch 123 | Batch: 29 | Loss: 8.1029\n",
      "Epoch 123 | Batch: 30 | Loss: 17.4464\n",
      "Epoch 123 | Batch: 31 | Loss: 17.5406\n",
      "Epoch 123 | Batch: 32 | Loss: 7.4383\n",
      "Epoch 123 | Batch: 33 | Loss: 5.6268\n",
      "Epoch 123 | Batch: 34 | Loss: 12.8333\n",
      "Epoch 123 | Batch: 35 | Loss: 10.8432\n",
      "Epoch 123 | Batch: 36 | Loss: 11.8488\n",
      "Epoch 123 | Batch: 37 | Loss: 11.3565\n",
      "Epoch 123 | Batch: 38 | Loss: 2.0924\n",
      "Epoch 123 | Batch: 39 | Loss: 4.7894\n",
      "Epoch 123 | Batch: 40 | Loss: 5.6090\n",
      "Epoch 123 | Batch: 41 | Loss: 13.2830\n",
      "Epoch 123 | Batch: 42 | Loss: 11.5348\n",
      "Epoch 123 | Batch: 43 | Loss: 10.1004\n",
      "Epoch 123 | Batch: 44 | Loss: 5.2074\n",
      "Epoch 123 | Batch: 45 | Loss: 7.1188\n",
      "Epoch 123 | Batch: 46 | Loss: 7.3007\n",
      "Epoch 123 | Batch: 47 | Loss: 4.0987\n",
      "Epoch 123 | Batch: 48 | Loss: 1.0781\n",
      "Mean 8.30024042725563\n",
      "Epoch 124 | Batch: 1 | Loss: 4.0772\n",
      "Epoch 124 | Batch: 2 | Loss: 13.7623\n",
      "Epoch 124 | Batch: 3 | Loss: 15.2681\n",
      "Epoch 124 | Batch: 4 | Loss: 12.8339\n",
      "Epoch 124 | Batch: 5 | Loss: 7.7586\n",
      "Epoch 124 | Batch: 6 | Loss: 12.4723\n",
      "Epoch 124 | Batch: 7 | Loss: 9.8142\n",
      "Epoch 124 | Batch: 8 | Loss: 7.5449\n",
      "Epoch 124 | Batch: 9 | Loss: 7.5172\n",
      "Epoch 124 | Batch: 10 | Loss: 6.3776\n",
      "Epoch 124 | Batch: 11 | Loss: 10.4939\n",
      "Epoch 124 | Batch: 12 | Loss: 9.9544\n",
      "Epoch 124 | Batch: 13 | Loss: 13.1892\n",
      "Epoch 124 | Batch: 14 | Loss: 14.2256\n",
      "Epoch 124 | Batch: 15 | Loss: 7.1425\n",
      "Epoch 124 | Batch: 16 | Loss: 4.0962\n",
      "Epoch 124 | Batch: 17 | Loss: 7.8327\n",
      "Epoch 124 | Batch: 18 | Loss: 15.1805\n",
      "Epoch 124 | Batch: 19 | Loss: 8.0245\n",
      "Epoch 124 | Batch: 20 | Loss: 6.4485\n",
      "Epoch 124 | Batch: 21 | Loss: 18.1159\n",
      "Epoch 124 | Batch: 22 | Loss: 16.7191\n",
      "Epoch 124 | Batch: 23 | Loss: 6.7569\n",
      "Epoch 124 | Batch: 24 | Loss: 2.4433\n",
      "Epoch 124 | Batch: 25 | Loss: 7.6101\n",
      "Epoch 124 | Batch: 26 | Loss: 6.1018\n",
      "Epoch 124 | Batch: 27 | Loss: 9.4701\n",
      "Epoch 124 | Batch: 28 | Loss: 16.9511\n",
      "Epoch 124 | Batch: 29 | Loss: 16.2355\n",
      "Epoch 124 | Batch: 30 | Loss: 5.3773\n",
      "Epoch 124 | Batch: 31 | Loss: 11.0879\n",
      "Epoch 124 | Batch: 32 | Loss: 8.9826\n",
      "Epoch 124 | Batch: 33 | Loss: 7.0399\n",
      "Epoch 124 | Batch: 34 | Loss: 10.7556\n",
      "Epoch 124 | Batch: 35 | Loss: 5.0766\n",
      "Epoch 124 | Batch: 36 | Loss: 4.0519\n",
      "Epoch 124 | Batch: 37 | Loss: 6.4796\n",
      "Epoch 124 | Batch: 38 | Loss: 10.7110\n",
      "Epoch 124 | Batch: 39 | Loss: 14.6844\n",
      "Epoch 124 | Batch: 40 | Loss: 12.6176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124 | Batch: 41 | Loss: 14.0162\n",
      "Epoch 124 | Batch: 42 | Loss: 9.1571\n",
      "Epoch 124 | Batch: 43 | Loss: 17.7244\n",
      "Epoch 124 | Batch: 44 | Loss: 7.4894\n",
      "Epoch 124 | Batch: 45 | Loss: 13.2919\n",
      "Epoch 124 | Batch: 46 | Loss: 5.5692\n",
      "Epoch 124 | Batch: 47 | Loss: 4.9365\n",
      "Epoch 124 | Batch: 48 | Loss: 5.3986\n",
      "Mean 9.768039951721827\n",
      "Epoch 125 | Batch: 1 | Loss: 6.6676\n",
      "Epoch 125 | Batch: 2 | Loss: 9.2488\n",
      "Epoch 125 | Batch: 3 | Loss: 11.3709\n",
      "Epoch 125 | Batch: 4 | Loss: 5.1911\n",
      "Epoch 125 | Batch: 5 | Loss: 9.3815\n",
      "Epoch 125 | Batch: 6 | Loss: 6.7278\n",
      "Epoch 125 | Batch: 7 | Loss: 11.8803\n",
      "Epoch 125 | Batch: 8 | Loss: 3.2261\n",
      "Epoch 125 | Batch: 9 | Loss: 5.1649\n",
      "Epoch 125 | Batch: 10 | Loss: 3.3416\n",
      "Epoch 125 | Batch: 11 | Loss: 10.0624\n",
      "Epoch 125 | Batch: 12 | Loss: 9.5172\n",
      "Epoch 125 | Batch: 13 | Loss: 15.8624\n",
      "Epoch 125 | Batch: 14 | Loss: 20.4587\n",
      "Epoch 125 | Batch: 15 | Loss: 11.4119\n",
      "Epoch 125 | Batch: 16 | Loss: 13.0546\n",
      "Epoch 125 | Batch: 17 | Loss: 6.7689\n",
      "Epoch 125 | Batch: 18 | Loss: 11.4302\n",
      "Epoch 125 | Batch: 19 | Loss: 10.3975\n",
      "Epoch 125 | Batch: 20 | Loss: 10.3747\n",
      "Epoch 125 | Batch: 21 | Loss: 17.3157\n",
      "Epoch 125 | Batch: 22 | Loss: 6.1780\n",
      "Epoch 125 | Batch: 23 | Loss: 6.4450\n",
      "Epoch 125 | Batch: 24 | Loss: 14.9596\n",
      "Epoch 125 | Batch: 25 | Loss: 17.6266\n",
      "Epoch 125 | Batch: 26 | Loss: 14.4168\n",
      "Epoch 125 | Batch: 27 | Loss: 13.3926\n",
      "Epoch 125 | Batch: 28 | Loss: 25.1346\n",
      "Epoch 125 | Batch: 29 | Loss: 6.0046\n",
      "Epoch 125 | Batch: 30 | Loss: 17.6488\n",
      "Epoch 125 | Batch: 31 | Loss: 15.5284\n",
      "Epoch 125 | Batch: 32 | Loss: 23.4319\n",
      "Epoch 125 | Batch: 33 | Loss: 21.2854\n",
      "Epoch 125 | Batch: 34 | Loss: 11.7913\n",
      "Epoch 125 | Batch: 35 | Loss: 12.1369\n",
      "Epoch 125 | Batch: 36 | Loss: 7.6327\n",
      "Epoch 125 | Batch: 37 | Loss: 4.8562\n",
      "Epoch 125 | Batch: 38 | Loss: 1.2721\n",
      "Epoch 125 | Batch: 39 | Loss: 11.0429\n",
      "Epoch 125 | Batch: 40 | Loss: 16.3855\n",
      "Epoch 125 | Batch: 41 | Loss: 32.5144\n",
      "Epoch 125 | Batch: 42 | Loss: 17.7057\n",
      "Epoch 125 | Batch: 43 | Loss: 2.9314\n",
      "Epoch 125 | Batch: 44 | Loss: 5.8013\n",
      "Epoch 125 | Batch: 45 | Loss: 10.6944\n",
      "Epoch 125 | Batch: 46 | Loss: 13.0901\n",
      "Epoch 125 | Batch: 47 | Loss: 5.5960\n",
      "Epoch 125 | Batch: 48 | Loss: 3.1226\n",
      "Mean 11.405847748120626\n",
      "Epoch 126 | Batch: 1 | Loss: 6.4196\n",
      "Epoch 126 | Batch: 2 | Loss: 5.2366\n",
      "Epoch 126 | Batch: 3 | Loss: 6.2266\n",
      "Epoch 126 | Batch: 4 | Loss: 4.4032\n",
      "Epoch 126 | Batch: 5 | Loss: 8.1597\n",
      "Epoch 126 | Batch: 6 | Loss: 12.4925\n",
      "Epoch 126 | Batch: 7 | Loss: 7.6989\n",
      "Epoch 126 | Batch: 8 | Loss: 2.9832\n",
      "Epoch 126 | Batch: 9 | Loss: 8.6398\n",
      "Epoch 126 | Batch: 10 | Loss: 9.2143\n",
      "Epoch 126 | Batch: 11 | Loss: 6.5335\n",
      "Epoch 126 | Batch: 12 | Loss: 13.3891\n",
      "Epoch 126 | Batch: 13 | Loss: 6.3605\n",
      "Epoch 126 | Batch: 14 | Loss: 7.2375\n",
      "Epoch 126 | Batch: 15 | Loss: 8.7931\n",
      "Epoch 126 | Batch: 16 | Loss: 8.6861\n",
      "Epoch 126 | Batch: 17 | Loss: 11.5888\n",
      "Epoch 126 | Batch: 18 | Loss: 6.8559\n",
      "Epoch 126 | Batch: 19 | Loss: 7.8042\n",
      "Epoch 126 | Batch: 20 | Loss: 13.5105\n",
      "Epoch 126 | Batch: 21 | Loss: 11.0336\n",
      "Epoch 126 | Batch: 22 | Loss: 9.0425\n",
      "Epoch 126 | Batch: 23 | Loss: 5.2867\n",
      "Epoch 126 | Batch: 24 | Loss: 5.0743\n",
      "Epoch 126 | Batch: 25 | Loss: 6.6491\n",
      "Epoch 126 | Batch: 26 | Loss: 5.9304\n",
      "Epoch 126 | Batch: 27 | Loss: 4.2746\n",
      "Epoch 126 | Batch: 28 | Loss: 10.8841\n",
      "Epoch 126 | Batch: 29 | Loss: 13.7162\n",
      "Epoch 126 | Batch: 30 | Loss: 10.5161\n",
      "Epoch 126 | Batch: 31 | Loss: 7.5641\n",
      "Epoch 126 | Batch: 32 | Loss: 10.1160\n",
      "Epoch 126 | Batch: 33 | Loss: 6.8124\n",
      "Epoch 126 | Batch: 34 | Loss: 10.1061\n",
      "Epoch 126 | Batch: 35 | Loss: 9.0586\n",
      "Epoch 126 | Batch: 36 | Loss: 15.9170\n",
      "Epoch 126 | Batch: 37 | Loss: 10.2411\n",
      "Epoch 126 | Batch: 38 | Loss: 9.1359\n",
      "Epoch 126 | Batch: 39 | Loss: 7.0231\n",
      "Epoch 126 | Batch: 40 | Loss: 5.9965\n",
      "Epoch 126 | Batch: 41 | Loss: 10.4857\n",
      "Epoch 126 | Batch: 42 | Loss: 7.0736\n",
      "Epoch 126 | Batch: 43 | Loss: 10.3324\n",
      "Epoch 126 | Batch: 44 | Loss: 12.3906\n",
      "Epoch 126 | Batch: 45 | Loss: 13.4138\n",
      "Epoch 126 | Batch: 46 | Loss: 13.5096\n",
      "Epoch 126 | Batch: 47 | Loss: 7.5265\n",
      "Epoch 126 | Batch: 48 | Loss: 4.3194\n",
      "Mean 8.659659604231516\n",
      "Epoch 127 | Batch: 1 | Loss: 4.2885\n",
      "Epoch 127 | Batch: 2 | Loss: 10.6729\n",
      "Epoch 127 | Batch: 3 | Loss: 13.6716\n",
      "Epoch 127 | Batch: 4 | Loss: 17.0147\n",
      "Epoch 127 | Batch: 5 | Loss: 3.1287\n",
      "Epoch 127 | Batch: 6 | Loss: 10.0277\n",
      "Epoch 127 | Batch: 7 | Loss: 4.5718\n",
      "Epoch 127 | Batch: 8 | Loss: 12.4982\n",
      "Epoch 127 | Batch: 9 | Loss: 6.6271\n",
      "Epoch 127 | Batch: 10 | Loss: 9.9741\n",
      "Epoch 127 | Batch: 11 | Loss: 13.7786\n",
      "Epoch 127 | Batch: 12 | Loss: 10.9519\n",
      "Epoch 127 | Batch: 13 | Loss: 7.8703\n",
      "Epoch 127 | Batch: 14 | Loss: 9.8217\n",
      "Epoch 127 | Batch: 15 | Loss: 7.8267\n",
      "Epoch 127 | Batch: 16 | Loss: 15.4738\n",
      "Epoch 127 | Batch: 17 | Loss: 6.9559\n",
      "Epoch 127 | Batch: 18 | Loss: 6.9375\n",
      "Epoch 127 | Batch: 19 | Loss: 28.0065\n",
      "Epoch 127 | Batch: 20 | Loss: 8.7648\n",
      "Epoch 127 | Batch: 21 | Loss: 11.6969\n",
      "Epoch 127 | Batch: 22 | Loss: 6.1228\n",
      "Epoch 127 | Batch: 23 | Loss: 12.6905\n",
      "Epoch 127 | Batch: 24 | Loss: 3.2879\n",
      "Epoch 127 | Batch: 25 | Loss: 4.7056\n",
      "Epoch 127 | Batch: 26 | Loss: 5.6153\n",
      "Epoch 127 | Batch: 27 | Loss: 7.6532\n",
      "Epoch 127 | Batch: 28 | Loss: 11.2379\n",
      "Epoch 127 | Batch: 29 | Loss: 3.2698\n",
      "Epoch 127 | Batch: 30 | Loss: 7.5168\n",
      "Epoch 127 | Batch: 31 | Loss: 8.2325\n",
      "Epoch 127 | Batch: 32 | Loss: 8.9320\n",
      "Epoch 127 | Batch: 33 | Loss: 6.0349\n",
      "Epoch 127 | Batch: 34 | Loss: 7.1842\n",
      "Epoch 127 | Batch: 35 | Loss: 5.9222\n",
      "Epoch 127 | Batch: 36 | Loss: 9.9520\n",
      "Epoch 127 | Batch: 37 | Loss: 14.8599\n",
      "Epoch 127 | Batch: 38 | Loss: 20.0139\n",
      "Epoch 127 | Batch: 39 | Loss: 11.8002\n",
      "Epoch 127 | Batch: 40 | Loss: 14.1563\n",
      "Epoch 127 | Batch: 41 | Loss: 19.4977\n",
      "Epoch 127 | Batch: 42 | Loss: 8.5053\n",
      "Epoch 127 | Batch: 43 | Loss: 4.0400\n",
      "Epoch 127 | Batch: 44 | Loss: 6.3089\n",
      "Epoch 127 | Batch: 45 | Loss: 5.3985\n",
      "Epoch 127 | Batch: 46 | Loss: 9.3297\n",
      "Epoch 127 | Batch: 47 | Loss: 5.8142\n",
      "Epoch 127 | Batch: 48 | Loss: 1.0243\n",
      "Mean 9.368047282099724\n",
      "Epoch 128 | Batch: 1 | Loss: 6.4892\n",
      "Epoch 128 | Batch: 2 | Loss: 9.6326\n",
      "Epoch 128 | Batch: 3 | Loss: 12.0167\n",
      "Epoch 128 | Batch: 4 | Loss: 5.7452\n",
      "Epoch 128 | Batch: 5 | Loss: 8.2116\n",
      "Epoch 128 | Batch: 6 | Loss: 13.0468\n",
      "Epoch 128 | Batch: 7 | Loss: 5.8864\n",
      "Epoch 128 | Batch: 8 | Loss: 6.6380\n",
      "Epoch 128 | Batch: 9 | Loss: 6.3785\n",
      "Epoch 128 | Batch: 10 | Loss: 1.4047\n",
      "Epoch 128 | Batch: 11 | Loss: 1.8164\n",
      "Epoch 128 | Batch: 12 | Loss: 6.3549\n",
      "Epoch 128 | Batch: 13 | Loss: 14.8466\n",
      "Epoch 128 | Batch: 14 | Loss: 18.0270\n",
      "Epoch 128 | Batch: 15 | Loss: 14.1254\n",
      "Epoch 128 | Batch: 16 | Loss: 6.0316\n",
      "Epoch 128 | Batch: 17 | Loss: 8.5936\n",
      "Epoch 128 | Batch: 18 | Loss: 11.1606\n",
      "Epoch 128 | Batch: 19 | Loss: 4.1087\n",
      "Epoch 128 | Batch: 20 | Loss: 5.9060\n",
      "Epoch 128 | Batch: 21 | Loss: 10.1958\n",
      "Epoch 128 | Batch: 22 | Loss: 8.2266\n",
      "Epoch 128 | Batch: 23 | Loss: 7.1759\n",
      "Epoch 128 | Batch: 24 | Loss: 6.2237\n",
      "Epoch 128 | Batch: 25 | Loss: 12.9898\n",
      "Epoch 128 | Batch: 26 | Loss: 6.6264\n",
      "Epoch 128 | Batch: 27 | Loss: 7.9837\n",
      "Epoch 128 | Batch: 28 | Loss: 7.4815\n",
      "Epoch 128 | Batch: 29 | Loss: 13.8431\n",
      "Epoch 128 | Batch: 30 | Loss: 27.0303\n",
      "Epoch 128 | Batch: 31 | Loss: 28.2941\n",
      "Epoch 128 | Batch: 32 | Loss: 8.5390\n",
      "Epoch 128 | Batch: 33 | Loss: 7.5187\n",
      "Epoch 128 | Batch: 34 | Loss: 6.1972\n",
      "Epoch 128 | Batch: 35 | Loss: 4.5247\n",
      "Epoch 128 | Batch: 36 | Loss: 10.6369\n",
      "Epoch 128 | Batch: 37 | Loss: 8.4158\n",
      "Epoch 128 | Batch: 38 | Loss: 5.3474\n",
      "Epoch 128 | Batch: 39 | Loss: 4.6487\n",
      "Epoch 128 | Batch: 40 | Loss: 9.0075\n",
      "Epoch 128 | Batch: 41 | Loss: 13.7229\n",
      "Epoch 128 | Batch: 42 | Loss: 8.4093\n",
      "Epoch 128 | Batch: 43 | Loss: 13.7164\n",
      "Epoch 128 | Batch: 44 | Loss: 6.7884\n",
      "Epoch 128 | Batch: 45 | Loss: 7.1950\n",
      "Epoch 128 | Batch: 46 | Loss: 7.9321\n",
      "Epoch 128 | Batch: 47 | Loss: 7.4768\n",
      "Epoch 128 | Batch: 48 | Loss: 4.4031\n",
      "Mean 9.103567026555538\n",
      "Epoch 129 | Batch: 1 | Loss: 7.8981\n",
      "Epoch 129 | Batch: 2 | Loss: 3.6006\n",
      "Epoch 129 | Batch: 3 | Loss: 6.1266\n",
      "Epoch 129 | Batch: 4 | Loss: 12.0888\n",
      "Epoch 129 | Batch: 5 | Loss: 7.3531\n",
      "Epoch 129 | Batch: 6 | Loss: 6.7482\n",
      "Epoch 129 | Batch: 7 | Loss: 14.0638\n",
      "Epoch 129 | Batch: 8 | Loss: 22.4813\n",
      "Epoch 129 | Batch: 9 | Loss: 7.2918\n",
      "Epoch 129 | Batch: 10 | Loss: 5.7329\n",
      "Epoch 129 | Batch: 11 | Loss: 3.0596\n",
      "Epoch 129 | Batch: 12 | Loss: 5.4412\n",
      "Epoch 129 | Batch: 13 | Loss: 3.9671\n",
      "Epoch 129 | Batch: 14 | Loss: 9.9611\n",
      "Epoch 129 | Batch: 15 | Loss: 8.3614\n",
      "Epoch 129 | Batch: 16 | Loss: 7.0441\n",
      "Epoch 129 | Batch: 17 | Loss: 7.4250\n",
      "Epoch 129 | Batch: 18 | Loss: 9.2219\n",
      "Epoch 129 | Batch: 19 | Loss: 6.2671\n",
      "Epoch 129 | Batch: 20 | Loss: 10.3311\n",
      "Epoch 129 | Batch: 21 | Loss: 5.1672\n",
      "Epoch 129 | Batch: 22 | Loss: 11.9946\n",
      "Epoch 129 | Batch: 23 | Loss: 12.2588\n",
      "Epoch 129 | Batch: 24 | Loss: 14.7126\n",
      "Epoch 129 | Batch: 25 | Loss: 9.5513\n",
      "Epoch 129 | Batch: 26 | Loss: 6.3761\n",
      "Epoch 129 | Batch: 27 | Loss: 9.1489\n",
      "Epoch 129 | Batch: 28 | Loss: 8.8754\n",
      "Epoch 129 | Batch: 29 | Loss: 16.5205\n",
      "Epoch 129 | Batch: 30 | Loss: 5.6956\n",
      "Epoch 129 | Batch: 31 | Loss: 10.8609\n",
      "Epoch 129 | Batch: 32 | Loss: 7.6011\n",
      "Epoch 129 | Batch: 33 | Loss: 8.6653\n",
      "Epoch 129 | Batch: 34 | Loss: 7.1970\n",
      "Epoch 129 | Batch: 35 | Loss: 10.3339\n",
      "Epoch 129 | Batch: 36 | Loss: 5.3522\n",
      "Epoch 129 | Batch: 37 | Loss: 8.6808\n",
      "Epoch 129 | Batch: 38 | Loss: 10.4764\n",
      "Epoch 129 | Batch: 39 | Loss: 9.4122\n",
      "Epoch 129 | Batch: 40 | Loss: 13.1295\n",
      "Epoch 129 | Batch: 41 | Loss: 17.0042\n",
      "Epoch 129 | Batch: 42 | Loss: 8.8525\n",
      "Epoch 129 | Batch: 43 | Loss: 6.4110\n",
      "Epoch 129 | Batch: 44 | Loss: 9.9081\n",
      "Epoch 129 | Batch: 45 | Loss: 4.5873\n",
      "Epoch 129 | Batch: 46 | Loss: 4.3158\n",
      "Epoch 129 | Batch: 47 | Loss: 6.0374\n",
      "Epoch 129 | Batch: 48 | Loss: 0.8390\n",
      "Mean 8.633968519667784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130 | Batch: 1 | Loss: 3.2868\n",
      "Epoch 130 | Batch: 2 | Loss: 10.3983\n",
      "Epoch 130 | Batch: 3 | Loss: 8.9322\n",
      "Epoch 130 | Batch: 4 | Loss: 19.9212\n",
      "Epoch 130 | Batch: 5 | Loss: 6.6589\n",
      "Epoch 130 | Batch: 6 | Loss: 12.5115\n",
      "Epoch 130 | Batch: 7 | Loss: 7.7533\n",
      "Epoch 130 | Batch: 8 | Loss: 6.2853\n",
      "Epoch 130 | Batch: 9 | Loss: 3.5015\n",
      "Epoch 130 | Batch: 10 | Loss: 3.7215\n",
      "Epoch 130 | Batch: 11 | Loss: 7.5312\n",
      "Epoch 130 | Batch: 12 | Loss: 17.6277\n",
      "Epoch 130 | Batch: 13 | Loss: 28.9239\n",
      "Epoch 130 | Batch: 14 | Loss: 9.9648\n",
      "Epoch 130 | Batch: 15 | Loss: 8.4884\n",
      "Epoch 130 | Batch: 16 | Loss: 10.9943\n",
      "Epoch 130 | Batch: 17 | Loss: 8.1475\n",
      "Epoch 130 | Batch: 18 | Loss: 10.2170\n",
      "Epoch 130 | Batch: 19 | Loss: 11.0707\n",
      "Epoch 130 | Batch: 20 | Loss: 8.6944\n",
      "Epoch 130 | Batch: 21 | Loss: 11.0301\n",
      "Epoch 130 | Batch: 22 | Loss: 16.7606\n",
      "Epoch 130 | Batch: 23 | Loss: 6.3103\n",
      "Epoch 130 | Batch: 24 | Loss: 5.6377\n",
      "Epoch 130 | Batch: 25 | Loss: 6.8031\n",
      "Epoch 130 | Batch: 26 | Loss: 4.5805\n",
      "Epoch 130 | Batch: 27 | Loss: 7.4977\n",
      "Epoch 130 | Batch: 28 | Loss: 1.7068\n",
      "Epoch 130 | Batch: 29 | Loss: 5.1100\n",
      "Epoch 130 | Batch: 30 | Loss: 4.8945\n",
      "Epoch 130 | Batch: 31 | Loss: 7.8349\n",
      "Epoch 130 | Batch: 32 | Loss: 3.3669\n",
      "Epoch 130 | Batch: 33 | Loss: 18.0190\n",
      "Epoch 130 | Batch: 34 | Loss: 3.0106\n",
      "Epoch 130 | Batch: 35 | Loss: 12.3503\n",
      "Epoch 130 | Batch: 36 | Loss: 5.1429\n",
      "Epoch 130 | Batch: 37 | Loss: 5.5188\n",
      "Epoch 130 | Batch: 38 | Loss: 10.8951\n",
      "Epoch 130 | Batch: 39 | Loss: 8.5907\n",
      "Epoch 130 | Batch: 40 | Loss: 7.7266\n",
      "Epoch 130 | Batch: 41 | Loss: 4.2633\n",
      "Epoch 130 | Batch: 42 | Loss: 13.5600\n",
      "Epoch 130 | Batch: 43 | Loss: 10.3880\n",
      "Epoch 130 | Batch: 44 | Loss: 19.0205\n",
      "Epoch 130 | Batch: 45 | Loss: 15.8074\n",
      "Epoch 130 | Batch: 46 | Loss: 12.2867\n",
      "Epoch 130 | Batch: 47 | Loss: 14.7558\n",
      "Epoch 130 | Batch: 48 | Loss: 4.9851\n",
      "Mean 9.426759260396162\n",
      "Epoch 131 | Batch: 1 | Loss: 8.0051\n",
      "Epoch 131 | Batch: 2 | Loss: 6.1974\n",
      "Epoch 131 | Batch: 3 | Loss: 9.8929\n",
      "Epoch 131 | Batch: 4 | Loss: 8.1926\n",
      "Epoch 131 | Batch: 5 | Loss: 10.5226\n",
      "Epoch 131 | Batch: 6 | Loss: 6.5383\n",
      "Epoch 131 | Batch: 7 | Loss: 11.5379\n",
      "Epoch 131 | Batch: 8 | Loss: 13.4993\n",
      "Epoch 131 | Batch: 9 | Loss: 15.9352\n",
      "Epoch 131 | Batch: 10 | Loss: 17.8872\n",
      "Epoch 131 | Batch: 11 | Loss: 3.9202\n",
      "Epoch 131 | Batch: 12 | Loss: 6.8052\n",
      "Epoch 131 | Batch: 13 | Loss: 5.3865\n",
      "Epoch 131 | Batch: 14 | Loss: 5.3555\n",
      "Epoch 131 | Batch: 15 | Loss: 4.1779\n",
      "Epoch 131 | Batch: 16 | Loss: 7.3412\n",
      "Epoch 131 | Batch: 17 | Loss: 5.0913\n",
      "Epoch 131 | Batch: 18 | Loss: 3.8983\n",
      "Epoch 131 | Batch: 19 | Loss: 7.1027\n",
      "Epoch 131 | Batch: 20 | Loss: 4.8309\n",
      "Epoch 131 | Batch: 21 | Loss: 9.3877\n",
      "Epoch 131 | Batch: 22 | Loss: 7.4881\n",
      "Epoch 131 | Batch: 23 | Loss: 6.2862\n",
      "Epoch 131 | Batch: 24 | Loss: 4.0490\n",
      "Epoch 131 | Batch: 25 | Loss: 4.4923\n",
      "Epoch 131 | Batch: 26 | Loss: 7.9174\n",
      "Epoch 131 | Batch: 27 | Loss: 7.7577\n",
      "Epoch 131 | Batch: 28 | Loss: 19.2702\n",
      "Epoch 131 | Batch: 29 | Loss: 10.3098\n",
      "Epoch 131 | Batch: 30 | Loss: 2.0513\n",
      "Epoch 131 | Batch: 31 | Loss: 12.9411\n",
      "Epoch 131 | Batch: 32 | Loss: 20.0942\n",
      "Epoch 131 | Batch: 33 | Loss: 12.6817\n",
      "Epoch 131 | Batch: 34 | Loss: 6.8965\n",
      "Epoch 131 | Batch: 35 | Loss: 5.0850\n",
      "Epoch 131 | Batch: 36 | Loss: 8.2671\n",
      "Epoch 131 | Batch: 37 | Loss: 8.8239\n",
      "Epoch 131 | Batch: 38 | Loss: 4.5985\n",
      "Epoch 131 | Batch: 39 | Loss: 10.9052\n",
      "Epoch 131 | Batch: 40 | Loss: 9.5699\n",
      "Epoch 131 | Batch: 41 | Loss: 10.3841\n",
      "Epoch 131 | Batch: 42 | Loss: 6.3360\n",
      "Epoch 131 | Batch: 43 | Loss: 8.0472\n",
      "Epoch 131 | Batch: 44 | Loss: 5.5637\n",
      "Epoch 131 | Batch: 45 | Loss: 8.7057\n",
      "Epoch 131 | Batch: 46 | Loss: 7.0242\n",
      "Epoch 131 | Batch: 47 | Loss: 7.5083\n",
      "Epoch 131 | Batch: 48 | Loss: 1.5326\n",
      "Mean 8.251935842136541\n",
      "Epoch 132 | Batch: 1 | Loss: 5.6616\n",
      "Epoch 132 | Batch: 2 | Loss: 6.2034\n",
      "Epoch 132 | Batch: 3 | Loss: 1.4919\n",
      "Epoch 132 | Batch: 4 | Loss: 9.9504\n",
      "Epoch 132 | Batch: 5 | Loss: 19.4932\n",
      "Epoch 132 | Batch: 6 | Loss: 2.3326\n",
      "Epoch 132 | Batch: 7 | Loss: 3.2856\n",
      "Epoch 132 | Batch: 8 | Loss: 3.9691\n",
      "Epoch 132 | Batch: 9 | Loss: 10.3550\n",
      "Epoch 132 | Batch: 10 | Loss: 10.6801\n",
      "Epoch 132 | Batch: 11 | Loss: 4.5147\n",
      "Epoch 132 | Batch: 12 | Loss: 4.7987\n",
      "Epoch 132 | Batch: 13 | Loss: 8.8540\n",
      "Epoch 132 | Batch: 14 | Loss: 7.0209\n",
      "Epoch 132 | Batch: 15 | Loss: 3.3947\n",
      "Epoch 132 | Batch: 16 | Loss: 6.6591\n",
      "Epoch 132 | Batch: 17 | Loss: 9.1346\n",
      "Epoch 132 | Batch: 18 | Loss: 7.8085\n",
      "Epoch 132 | Batch: 19 | Loss: 11.1310\n",
      "Epoch 132 | Batch: 20 | Loss: 5.0742\n",
      "Epoch 132 | Batch: 21 | Loss: 15.2361\n",
      "Epoch 132 | Batch: 22 | Loss: 4.9797\n",
      "Epoch 132 | Batch: 23 | Loss: 7.7421\n",
      "Epoch 132 | Batch: 24 | Loss: 15.1754\n",
      "Epoch 132 | Batch: 25 | Loss: 12.5074\n",
      "Epoch 132 | Batch: 26 | Loss: 13.3472\n",
      "Epoch 132 | Batch: 27 | Loss: 11.8524\n",
      "Epoch 132 | Batch: 28 | Loss: 12.4246\n",
      "Epoch 132 | Batch: 29 | Loss: 8.6298\n",
      "Epoch 132 | Batch: 30 | Loss: 7.9022\n",
      "Epoch 132 | Batch: 31 | Loss: 6.2000\n",
      "Epoch 132 | Batch: 32 | Loss: 7.1610\n",
      "Epoch 132 | Batch: 33 | Loss: 7.3375\n",
      "Epoch 132 | Batch: 34 | Loss: 14.2956\n",
      "Epoch 132 | Batch: 35 | Loss: 7.1901\n",
      "Epoch 132 | Batch: 36 | Loss: 8.6545\n",
      "Epoch 132 | Batch: 37 | Loss: 16.1905\n",
      "Epoch 132 | Batch: 38 | Loss: 14.8805\n",
      "Epoch 132 | Batch: 39 | Loss: 8.9783\n",
      "Epoch 132 | Batch: 40 | Loss: 6.9324\n",
      "Epoch 132 | Batch: 41 | Loss: 4.9946\n",
      "Epoch 132 | Batch: 42 | Loss: 13.6620\n",
      "Epoch 132 | Batch: 43 | Loss: 25.6484\n",
      "Epoch 132 | Batch: 44 | Loss: 19.8569\n",
      "Epoch 132 | Batch: 45 | Loss: 7.1972\n",
      "Epoch 132 | Batch: 46 | Loss: 8.8046\n",
      "Epoch 132 | Batch: 47 | Loss: 10.7734\n",
      "Epoch 132 | Batch: 48 | Loss: 3.8934\n",
      "Mean 9.255446463823318\n",
      "Epoch 133 | Batch: 1 | Loss: 5.4557\n",
      "Epoch 133 | Batch: 2 | Loss: 8.5533\n",
      "Epoch 133 | Batch: 3 | Loss: 7.0474\n",
      "Epoch 133 | Batch: 4 | Loss: 4.2870\n",
      "Epoch 133 | Batch: 5 | Loss: 9.4362\n",
      "Epoch 133 | Batch: 6 | Loss: 6.7337\n",
      "Epoch 133 | Batch: 7 | Loss: 7.4456\n",
      "Epoch 133 | Batch: 8 | Loss: 8.3633\n",
      "Epoch 133 | Batch: 9 | Loss: 10.5120\n",
      "Epoch 133 | Batch: 10 | Loss: 4.5650\n",
      "Epoch 133 | Batch: 11 | Loss: 15.8745\n",
      "Epoch 133 | Batch: 12 | Loss: 4.9204\n",
      "Epoch 133 | Batch: 13 | Loss: 5.6970\n",
      "Epoch 133 | Batch: 14 | Loss: 12.4626\n",
      "Epoch 133 | Batch: 15 | Loss: 10.7218\n",
      "Epoch 133 | Batch: 16 | Loss: 4.1931\n",
      "Epoch 133 | Batch: 17 | Loss: 9.4196\n",
      "Epoch 133 | Batch: 18 | Loss: 20.0424\n",
      "Epoch 133 | Batch: 19 | Loss: 11.2315\n",
      "Epoch 133 | Batch: 20 | Loss: 5.3799\n",
      "Epoch 133 | Batch: 21 | Loss: 7.3281\n",
      "Epoch 133 | Batch: 22 | Loss: 8.5298\n",
      "Epoch 133 | Batch: 23 | Loss: 8.0290\n",
      "Epoch 133 | Batch: 24 | Loss: 7.3476\n",
      "Epoch 133 | Batch: 25 | Loss: 4.8538\n",
      "Epoch 133 | Batch: 26 | Loss: 2.1989\n",
      "Epoch 133 | Batch: 27 | Loss: 9.9843\n",
      "Epoch 133 | Batch: 28 | Loss: 4.3777\n",
      "Epoch 133 | Batch: 29 | Loss: 4.0855\n",
      "Epoch 133 | Batch: 30 | Loss: 7.8363\n",
      "Epoch 133 | Batch: 31 | Loss: 12.8906\n",
      "Epoch 133 | Batch: 32 | Loss: 21.3884\n",
      "Epoch 133 | Batch: 33 | Loss: 10.1925\n",
      "Epoch 133 | Batch: 34 | Loss: 9.7059\n",
      "Epoch 133 | Batch: 35 | Loss: 4.3774\n",
      "Epoch 133 | Batch: 36 | Loss: 9.1499\n",
      "Epoch 133 | Batch: 37 | Loss: 5.0616\n",
      "Epoch 133 | Batch: 38 | Loss: 7.1543\n",
      "Epoch 133 | Batch: 39 | Loss: 13.6552\n",
      "Epoch 133 | Batch: 40 | Loss: 12.1497\n",
      "Epoch 133 | Batch: 41 | Loss: 24.0716\n",
      "Epoch 133 | Batch: 42 | Loss: 29.3822\n",
      "Epoch 133 | Batch: 43 | Loss: 17.0409\n",
      "Epoch 133 | Batch: 44 | Loss: 6.6334\n",
      "Epoch 133 | Batch: 45 | Loss: 7.2594\n",
      "Epoch 133 | Batch: 46 | Loss: 13.1667\n",
      "Epoch 133 | Batch: 47 | Loss: 21.1311\n",
      "Epoch 133 | Batch: 48 | Loss: 4.2891\n",
      "Mean 9.700269028544426\n",
      "Epoch 134 | Batch: 1 | Loss: 6.3021\n",
      "Epoch 134 | Batch: 2 | Loss: 5.5200\n",
      "Epoch 134 | Batch: 3 | Loss: 12.5669\n",
      "Epoch 134 | Batch: 4 | Loss: 12.2637\n",
      "Epoch 134 | Batch: 5 | Loss: 8.7160\n",
      "Epoch 134 | Batch: 6 | Loss: 7.7697\n",
      "Epoch 134 | Batch: 7 | Loss: 6.7794\n",
      "Epoch 134 | Batch: 8 | Loss: 8.2005\n",
      "Epoch 134 | Batch: 9 | Loss: 5.2273\n",
      "Epoch 134 | Batch: 10 | Loss: 7.1567\n",
      "Epoch 134 | Batch: 11 | Loss: 13.1255\n",
      "Epoch 134 | Batch: 12 | Loss: 11.4798\n",
      "Epoch 134 | Batch: 13 | Loss: 7.0867\n",
      "Epoch 134 | Batch: 14 | Loss: 5.3253\n",
      "Epoch 134 | Batch: 15 | Loss: 5.7287\n",
      "Epoch 134 | Batch: 16 | Loss: 4.9475\n",
      "Epoch 134 | Batch: 17 | Loss: 2.0649\n",
      "Epoch 134 | Batch: 18 | Loss: 6.9372\n",
      "Epoch 134 | Batch: 19 | Loss: 21.5984\n",
      "Epoch 134 | Batch: 20 | Loss: 15.3789\n",
      "Epoch 134 | Batch: 21 | Loss: 9.8627\n",
      "Epoch 134 | Batch: 22 | Loss: 12.1272\n",
      "Epoch 134 | Batch: 23 | Loss: 12.3384\n",
      "Epoch 134 | Batch: 24 | Loss: 7.0609\n",
      "Epoch 134 | Batch: 25 | Loss: 8.1684\n",
      "Epoch 134 | Batch: 26 | Loss: 5.4548\n",
      "Epoch 134 | Batch: 27 | Loss: 5.0162\n",
      "Epoch 134 | Batch: 28 | Loss: 5.3775\n",
      "Epoch 134 | Batch: 29 | Loss: 7.1398\n",
      "Epoch 134 | Batch: 30 | Loss: 5.3010\n",
      "Epoch 134 | Batch: 31 | Loss: 7.6692\n",
      "Epoch 134 | Batch: 32 | Loss: 7.1616\n",
      "Epoch 134 | Batch: 33 | Loss: 13.1224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134 | Batch: 34 | Loss: 13.3080\n",
      "Epoch 134 | Batch: 35 | Loss: 8.5249\n",
      "Epoch 134 | Batch: 36 | Loss: 5.8422\n",
      "Epoch 134 | Batch: 37 | Loss: 7.3398\n",
      "Epoch 134 | Batch: 38 | Loss: 15.5473\n",
      "Epoch 134 | Batch: 39 | Loss: 8.8175\n",
      "Epoch 134 | Batch: 40 | Loss: 12.8289\n",
      "Epoch 134 | Batch: 41 | Loss: 10.9274\n",
      "Epoch 134 | Batch: 42 | Loss: 8.9011\n",
      "Epoch 134 | Batch: 43 | Loss: 6.4616\n",
      "Epoch 134 | Batch: 44 | Loss: 6.3472\n",
      "Epoch 134 | Batch: 45 | Loss: 8.0540\n",
      "Epoch 134 | Batch: 46 | Loss: 5.9994\n",
      "Epoch 134 | Batch: 47 | Loss: 9.0032\n",
      "Epoch 134 | Batch: 48 | Loss: 1.6970\n",
      "Mean 8.53280857205391\n",
      "Epoch 135 | Batch: 1 | Loss: 11.2011\n",
      "Epoch 135 | Batch: 2 | Loss: 8.8500\n",
      "Epoch 135 | Batch: 3 | Loss: 4.5582\n",
      "Epoch 135 | Batch: 4 | Loss: 9.6925\n",
      "Epoch 135 | Batch: 5 | Loss: 7.6762\n",
      "Epoch 135 | Batch: 6 | Loss: 6.7114\n",
      "Epoch 135 | Batch: 7 | Loss: 7.6856\n",
      "Epoch 135 | Batch: 8 | Loss: 3.7505\n",
      "Epoch 135 | Batch: 9 | Loss: 5.7120\n",
      "Epoch 135 | Batch: 10 | Loss: 10.4643\n",
      "Epoch 135 | Batch: 11 | Loss: 17.3905\n",
      "Epoch 135 | Batch: 12 | Loss: 2.8941\n",
      "Epoch 135 | Batch: 13 | Loss: 17.6876\n",
      "Epoch 135 | Batch: 14 | Loss: 5.9592\n",
      "Epoch 135 | Batch: 15 | Loss: 12.0750\n",
      "Epoch 135 | Batch: 16 | Loss: 9.1616\n",
      "Epoch 135 | Batch: 17 | Loss: 7.6958\n",
      "Epoch 135 | Batch: 18 | Loss: 6.6944\n",
      "Epoch 135 | Batch: 19 | Loss: 6.7920\n",
      "Epoch 135 | Batch: 20 | Loss: 2.7370\n",
      "Epoch 135 | Batch: 21 | Loss: 11.5857\n",
      "Epoch 135 | Batch: 22 | Loss: 9.9083\n",
      "Epoch 135 | Batch: 23 | Loss: 12.6490\n",
      "Epoch 135 | Batch: 24 | Loss: 10.1828\n",
      "Epoch 135 | Batch: 25 | Loss: 8.8591\n",
      "Epoch 135 | Batch: 26 | Loss: 10.5503\n",
      "Epoch 135 | Batch: 27 | Loss: 17.9967\n",
      "Epoch 135 | Batch: 28 | Loss: 22.1038\n",
      "Epoch 135 | Batch: 29 | Loss: 15.4473\n",
      "Epoch 135 | Batch: 30 | Loss: 3.8749\n",
      "Epoch 135 | Batch: 31 | Loss: 6.3776\n",
      "Epoch 135 | Batch: 32 | Loss: 9.1324\n",
      "Epoch 135 | Batch: 33 | Loss: 18.9107\n",
      "Epoch 135 | Batch: 34 | Loss: 12.1248\n",
      "Epoch 135 | Batch: 35 | Loss: 17.3171\n",
      "Epoch 135 | Batch: 36 | Loss: 8.1327\n",
      "Epoch 135 | Batch: 37 | Loss: 9.9171\n",
      "Epoch 135 | Batch: 38 | Loss: 7.4381\n",
      "Epoch 135 | Batch: 39 | Loss: 9.9503\n",
      "Epoch 135 | Batch: 40 | Loss: 6.1142\n",
      "Epoch 135 | Batch: 41 | Loss: 4.8142\n",
      "Epoch 135 | Batch: 42 | Loss: 2.6279\n",
      "Epoch 135 | Batch: 43 | Loss: 11.9417\n",
      "Epoch 135 | Batch: 44 | Loss: 11.0983\n",
      "Epoch 135 | Batch: 45 | Loss: 6.0558\n",
      "Epoch 135 | Batch: 46 | Loss: 8.5074\n",
      "Epoch 135 | Batch: 47 | Loss: 4.7959\n",
      "Epoch 135 | Batch: 48 | Loss: 9.1550\n",
      "Mean 9.43663008014361\n",
      "Epoch 136 | Batch: 1 | Loss: 11.3126\n",
      "Epoch 136 | Batch: 2 | Loss: 11.7230\n",
      "Epoch 136 | Batch: 3 | Loss: 10.3394\n",
      "Epoch 136 | Batch: 4 | Loss: 11.9357\n",
      "Epoch 136 | Batch: 5 | Loss: 5.9945\n",
      "Epoch 136 | Batch: 6 | Loss: 6.4398\n",
      "Epoch 136 | Batch: 7 | Loss: 5.7517\n",
      "Epoch 136 | Batch: 8 | Loss: 6.3177\n",
      "Epoch 136 | Batch: 9 | Loss: 10.1218\n",
      "Epoch 136 | Batch: 10 | Loss: 6.4840\n",
      "Epoch 136 | Batch: 11 | Loss: 11.0124\n",
      "Epoch 136 | Batch: 12 | Loss: 14.6548\n",
      "Epoch 136 | Batch: 13 | Loss: 12.8482\n",
      "Epoch 136 | Batch: 14 | Loss: 7.1748\n",
      "Epoch 136 | Batch: 15 | Loss: 9.0432\n",
      "Epoch 136 | Batch: 16 | Loss: 14.7716\n",
      "Epoch 136 | Batch: 17 | Loss: 11.5512\n",
      "Epoch 136 | Batch: 18 | Loss: 7.6774\n",
      "Epoch 136 | Batch: 19 | Loss: 5.7212\n",
      "Epoch 136 | Batch: 20 | Loss: 6.8933\n",
      "Epoch 136 | Batch: 21 | Loss: 10.5705\n",
      "Epoch 136 | Batch: 22 | Loss: 7.8187\n",
      "Epoch 136 | Batch: 23 | Loss: 12.1942\n",
      "Epoch 136 | Batch: 24 | Loss: 12.4231\n",
      "Epoch 136 | Batch: 25 | Loss: 5.1479\n",
      "Epoch 136 | Batch: 26 | Loss: 14.3740\n",
      "Epoch 136 | Batch: 27 | Loss: 8.5274\n",
      "Epoch 136 | Batch: 28 | Loss: 6.3059\n",
      "Epoch 136 | Batch: 29 | Loss: 5.5457\n",
      "Epoch 136 | Batch: 30 | Loss: 9.4485\n",
      "Epoch 136 | Batch: 31 | Loss: 7.9972\n",
      "Epoch 136 | Batch: 32 | Loss: 4.3020\n",
      "Epoch 136 | Batch: 33 | Loss: 1.4926\n",
      "Epoch 136 | Batch: 34 | Loss: 13.2543\n",
      "Epoch 136 | Batch: 35 | Loss: 15.6144\n",
      "Epoch 136 | Batch: 36 | Loss: 28.8618\n",
      "Epoch 136 | Batch: 37 | Loss: 21.5098\n",
      "Epoch 136 | Batch: 38 | Loss: 22.6660\n",
      "Epoch 136 | Batch: 39 | Loss: 13.6685\n",
      "Epoch 136 | Batch: 40 | Loss: 21.1305\n",
      "Epoch 136 | Batch: 41 | Loss: 22.9487\n",
      "Epoch 136 | Batch: 42 | Loss: 11.2834\n",
      "Epoch 136 | Batch: 43 | Loss: 10.4809\n",
      "Epoch 136 | Batch: 44 | Loss: 3.7610\n",
      "Epoch 136 | Batch: 45 | Loss: 5.6973\n",
      "Epoch 136 | Batch: 46 | Loss: 3.4382\n",
      "Epoch 136 | Batch: 47 | Loss: 10.4490\n",
      "Epoch 136 | Batch: 48 | Loss: 11.2189\n",
      "Mean 10.622886118789514\n",
      "Epoch 137 | Batch: 1 | Loss: 8.0405\n",
      "Epoch 137 | Batch: 2 | Loss: 3.0796\n",
      "Epoch 137 | Batch: 3 | Loss: 2.2643\n",
      "Epoch 137 | Batch: 4 | Loss: 11.4025\n",
      "Epoch 137 | Batch: 5 | Loss: 5.9240\n",
      "Epoch 137 | Batch: 6 | Loss: 14.4646\n",
      "Epoch 137 | Batch: 7 | Loss: 10.4763\n",
      "Epoch 137 | Batch: 8 | Loss: 9.4386\n",
      "Epoch 137 | Batch: 9 | Loss: 14.5362\n",
      "Epoch 137 | Batch: 10 | Loss: 9.8462\n",
      "Epoch 137 | Batch: 11 | Loss: 8.0305\n",
      "Epoch 137 | Batch: 12 | Loss: 9.1218\n",
      "Epoch 137 | Batch: 13 | Loss: 5.6476\n",
      "Epoch 137 | Batch: 14 | Loss: 5.3440\n",
      "Epoch 137 | Batch: 15 | Loss: 5.1420\n",
      "Epoch 137 | Batch: 16 | Loss: 17.4427\n",
      "Epoch 137 | Batch: 17 | Loss: 9.9460\n",
      "Epoch 137 | Batch: 18 | Loss: 6.3942\n",
      "Epoch 137 | Batch: 19 | Loss: 8.9794\n",
      "Epoch 137 | Batch: 20 | Loss: 9.6485\n",
      "Epoch 137 | Batch: 21 | Loss: 8.6900\n",
      "Epoch 137 | Batch: 22 | Loss: 6.4119\n",
      "Epoch 137 | Batch: 23 | Loss: 4.9916\n",
      "Epoch 137 | Batch: 24 | Loss: 9.6494\n",
      "Epoch 137 | Batch: 25 | Loss: 8.5876\n",
      "Epoch 137 | Batch: 26 | Loss: 8.1663\n",
      "Epoch 137 | Batch: 27 | Loss: 8.5759\n",
      "Epoch 137 | Batch: 28 | Loss: 4.8277\n",
      "Epoch 137 | Batch: 29 | Loss: 13.0805\n",
      "Epoch 137 | Batch: 30 | Loss: 8.0711\n",
      "Epoch 137 | Batch: 31 | Loss: 10.0767\n",
      "Epoch 137 | Batch: 32 | Loss: 6.9228\n",
      "Epoch 137 | Batch: 33 | Loss: 7.7057\n",
      "Epoch 137 | Batch: 34 | Loss: 5.8889\n",
      "Epoch 137 | Batch: 35 | Loss: 7.6691\n",
      "Epoch 137 | Batch: 36 | Loss: 10.4267\n",
      "Epoch 137 | Batch: 37 | Loss: 14.3833\n",
      "Epoch 137 | Batch: 38 | Loss: 11.2220\n",
      "Epoch 137 | Batch: 39 | Loss: 5.1932\n",
      "Epoch 137 | Batch: 40 | Loss: 4.2760\n",
      "Epoch 137 | Batch: 41 | Loss: 9.1670\n",
      "Epoch 137 | Batch: 42 | Loss: 6.4100\n",
      "Epoch 137 | Batch: 43 | Loss: 10.6745\n",
      "Epoch 137 | Batch: 44 | Loss: 16.6460\n",
      "Epoch 137 | Batch: 45 | Loss: 8.4034\n",
      "Epoch 137 | Batch: 46 | Loss: 10.0150\n",
      "Epoch 137 | Batch: 47 | Loss: 9.2040\n",
      "Epoch 137 | Batch: 48 | Loss: 1.2602\n",
      "Mean 8.578462320069471\n",
      "Epoch 138 | Batch: 1 | Loss: 8.9667\n",
      "Epoch 138 | Batch: 2 | Loss: 3.9453\n",
      "Epoch 138 | Batch: 3 | Loss: 10.1793\n",
      "Epoch 138 | Batch: 4 | Loss: 5.9560\n",
      "Epoch 138 | Batch: 5 | Loss: 5.7471\n",
      "Epoch 138 | Batch: 6 | Loss: 7.4184\n",
      "Epoch 138 | Batch: 7 | Loss: 5.8923\n",
      "Epoch 138 | Batch: 8 | Loss: 6.0918\n",
      "Epoch 138 | Batch: 9 | Loss: 9.5418\n",
      "Epoch 138 | Batch: 10 | Loss: 7.4867\n",
      "Epoch 138 | Batch: 11 | Loss: 12.7749\n",
      "Epoch 138 | Batch: 12 | Loss: 15.3998\n",
      "Epoch 138 | Batch: 13 | Loss: 10.2004\n",
      "Epoch 138 | Batch: 14 | Loss: 8.7932\n",
      "Epoch 138 | Batch: 15 | Loss: 10.3118\n",
      "Epoch 138 | Batch: 16 | Loss: 7.3825\n",
      "Epoch 138 | Batch: 17 | Loss: 6.8723\n",
      "Epoch 138 | Batch: 18 | Loss: 5.9810\n",
      "Epoch 138 | Batch: 19 | Loss: 7.2784\n",
      "Epoch 138 | Batch: 20 | Loss: 6.3224\n",
      "Epoch 138 | Batch: 21 | Loss: 5.7816\n",
      "Epoch 138 | Batch: 22 | Loss: 10.4062\n",
      "Epoch 138 | Batch: 23 | Loss: 6.5773\n",
      "Epoch 138 | Batch: 24 | Loss: 3.9942\n",
      "Epoch 138 | Batch: 25 | Loss: 4.8507\n",
      "Epoch 138 | Batch: 26 | Loss: 12.7626\n",
      "Epoch 138 | Batch: 27 | Loss: 5.6904\n",
      "Epoch 138 | Batch: 28 | Loss: 9.5913\n",
      "Epoch 138 | Batch: 29 | Loss: 12.6506\n",
      "Epoch 138 | Batch: 30 | Loss: 13.5716\n",
      "Epoch 138 | Batch: 31 | Loss: 12.1793\n",
      "Epoch 138 | Batch: 32 | Loss: 9.6702\n",
      "Epoch 138 | Batch: 33 | Loss: 10.5788\n",
      "Epoch 138 | Batch: 34 | Loss: 4.5185\n",
      "Epoch 138 | Batch: 35 | Loss: 8.1444\n",
      "Epoch 138 | Batch: 36 | Loss: 5.2889\n",
      "Epoch 138 | Batch: 37 | Loss: 5.0694\n",
      "Epoch 138 | Batch: 38 | Loss: 6.5624\n",
      "Epoch 138 | Batch: 39 | Loss: 19.2663\n",
      "Epoch 138 | Batch: 40 | Loss: 6.6669\n",
      "Epoch 138 | Batch: 41 | Loss: 9.3187\n",
      "Epoch 138 | Batch: 42 | Loss: 5.7977\n",
      "Epoch 138 | Batch: 43 | Loss: 9.3740\n",
      "Epoch 138 | Batch: 44 | Loss: 6.7282\n",
      "Epoch 138 | Batch: 45 | Loss: 9.5372\n",
      "Epoch 138 | Batch: 46 | Loss: 2.8563\n",
      "Epoch 138 | Batch: 47 | Loss: 10.8321\n",
      "Epoch 138 | Batch: 48 | Loss: 4.4698\n",
      "Mean 8.234947184721628\n",
      "Epoch 139 | Batch: 1 | Loss: 15.5283\n",
      "Epoch 139 | Batch: 2 | Loss: 15.3268\n",
      "Epoch 139 | Batch: 3 | Loss: 10.2936\n",
      "Epoch 139 | Batch: 4 | Loss: 5.7157\n",
      "Epoch 139 | Batch: 5 | Loss: 5.0348\n",
      "Epoch 139 | Batch: 6 | Loss: 3.5706\n",
      "Epoch 139 | Batch: 7 | Loss: 7.7986\n",
      "Epoch 139 | Batch: 8 | Loss: 8.4481\n",
      "Epoch 139 | Batch: 9 | Loss: 8.6176\n",
      "Epoch 139 | Batch: 10 | Loss: 4.1503\n",
      "Epoch 139 | Batch: 11 | Loss: 6.3297\n",
      "Epoch 139 | Batch: 12 | Loss: 7.2186\n",
      "Epoch 139 | Batch: 13 | Loss: 3.0556\n",
      "Epoch 139 | Batch: 14 | Loss: 15.8745\n",
      "Epoch 139 | Batch: 15 | Loss: 13.1984\n",
      "Epoch 139 | Batch: 16 | Loss: 4.3926\n",
      "Epoch 139 | Batch: 17 | Loss: 10.1900\n",
      "Epoch 139 | Batch: 18 | Loss: 10.9628\n",
      "Epoch 139 | Batch: 19 | Loss: 14.5829\n",
      "Epoch 139 | Batch: 20 | Loss: 7.3239\n",
      "Epoch 139 | Batch: 21 | Loss: 5.0643\n",
      "Epoch 139 | Batch: 22 | Loss: 18.2814\n",
      "Epoch 139 | Batch: 23 | Loss: 15.2550\n",
      "Epoch 139 | Batch: 24 | Loss: 9.5001\n",
      "Epoch 139 | Batch: 25 | Loss: 5.3933\n",
      "Epoch 139 | Batch: 26 | Loss: 4.6198\n",
      "Epoch 139 | Batch: 27 | Loss: 5.6391\n",
      "Epoch 139 | Batch: 28 | Loss: 34.5517\n",
      "Epoch 139 | Batch: 29 | Loss: 9.1889\n",
      "Epoch 139 | Batch: 30 | Loss: 10.6495\n",
      "Epoch 139 | Batch: 31 | Loss: 10.0178\n",
      "Epoch 139 | Batch: 32 | Loss: 8.4422\n",
      "Epoch 139 | Batch: 33 | Loss: 9.4283\n",
      "Epoch 139 | Batch: 34 | Loss: 6.8580\n",
      "Epoch 139 | Batch: 35 | Loss: 7.1128\n",
      "Epoch 139 | Batch: 36 | Loss: 8.4756\n",
      "Epoch 139 | Batch: 37 | Loss: 8.5958\n",
      "Epoch 139 | Batch: 38 | Loss: 9.9178\n",
      "Epoch 139 | Batch: 39 | Loss: 13.7412\n",
      "Epoch 139 | Batch: 40 | Loss: 19.6567\n",
      "Epoch 139 | Batch: 41 | Loss: 24.9579\n",
      "Epoch 139 | Batch: 42 | Loss: 11.7857\n",
      "Epoch 139 | Batch: 43 | Loss: 4.2056\n",
      "Epoch 139 | Batch: 44 | Loss: 9.8840\n",
      "Epoch 139 | Batch: 45 | Loss: 9.3596\n",
      "Epoch 139 | Batch: 46 | Loss: 9.1263\n",
      "Epoch 139 | Batch: 47 | Loss: 13.8354\n",
      "Epoch 139 | Batch: 48 | Loss: 2.4974\n",
      "Mean 10.076131780942282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140 | Batch: 1 | Loss: 4.1733\n",
      "Epoch 140 | Batch: 2 | Loss: 5.6895\n",
      "Epoch 140 | Batch: 3 | Loss: 15.1130\n",
      "Epoch 140 | Batch: 4 | Loss: 4.4163\n",
      "Epoch 140 | Batch: 5 | Loss: 7.0685\n",
      "Epoch 140 | Batch: 6 | Loss: 9.8642\n",
      "Epoch 140 | Batch: 7 | Loss: 7.1301\n",
      "Epoch 140 | Batch: 8 | Loss: 5.5322\n",
      "Epoch 140 | Batch: 9 | Loss: 3.7349\n",
      "Epoch 140 | Batch: 10 | Loss: 5.6808\n",
      "Epoch 140 | Batch: 11 | Loss: 5.6381\n",
      "Epoch 140 | Batch: 12 | Loss: 11.2133\n",
      "Epoch 140 | Batch: 13 | Loss: 16.5550\n",
      "Epoch 140 | Batch: 14 | Loss: 5.9520\n",
      "Epoch 140 | Batch: 15 | Loss: 5.2496\n",
      "Epoch 140 | Batch: 16 | Loss: 10.6627\n",
      "Epoch 140 | Batch: 17 | Loss: 4.5242\n",
      "Epoch 140 | Batch: 18 | Loss: 7.5906\n",
      "Epoch 140 | Batch: 19 | Loss: 8.9694\n",
      "Epoch 140 | Batch: 20 | Loss: 5.5787\n",
      "Epoch 140 | Batch: 21 | Loss: 10.9530\n",
      "Epoch 140 | Batch: 22 | Loss: 5.9991\n",
      "Epoch 140 | Batch: 23 | Loss: 12.1501\n",
      "Epoch 140 | Batch: 24 | Loss: 9.6727\n",
      "Epoch 140 | Batch: 25 | Loss: 7.5386\n",
      "Epoch 140 | Batch: 26 | Loss: 8.8839\n",
      "Epoch 140 | Batch: 27 | Loss: 9.9456\n",
      "Epoch 140 | Batch: 28 | Loss: 6.5492\n",
      "Epoch 140 | Batch: 29 | Loss: 5.5414\n",
      "Epoch 140 | Batch: 30 | Loss: 9.8732\n",
      "Epoch 140 | Batch: 31 | Loss: 16.4576\n",
      "Epoch 140 | Batch: 32 | Loss: 17.8631\n",
      "Epoch 140 | Batch: 33 | Loss: 18.9968\n",
      "Epoch 140 | Batch: 34 | Loss: 2.0501\n",
      "Epoch 140 | Batch: 35 | Loss: 8.5346\n",
      "Epoch 140 | Batch: 36 | Loss: 6.4743\n",
      "Epoch 140 | Batch: 37 | Loss: 10.4812\n",
      "Epoch 140 | Batch: 38 | Loss: 10.7164\n",
      "Epoch 140 | Batch: 39 | Loss: 9.5377\n",
      "Epoch 140 | Batch: 40 | Loss: 9.5016\n",
      "Epoch 140 | Batch: 41 | Loss: 9.4084\n",
      "Epoch 140 | Batch: 42 | Loss: 5.8527\n",
      "Epoch 140 | Batch: 43 | Loss: 3.4865\n",
      "Epoch 140 | Batch: 44 | Loss: 4.5866\n",
      "Epoch 140 | Batch: 45 | Loss: 7.5200\n",
      "Epoch 140 | Batch: 46 | Loss: 7.3367\n",
      "Epoch 140 | Batch: 47 | Loss: 27.6841\n",
      "Epoch 140 | Batch: 48 | Loss: 5.6781\n",
      "Mean 8.741876194874445\n",
      "Epoch 141 | Batch: 1 | Loss: 13.0582\n",
      "Epoch 141 | Batch: 2 | Loss: 7.3554\n",
      "Epoch 141 | Batch: 3 | Loss: 8.9908\n",
      "Epoch 141 | Batch: 4 | Loss: 7.0032\n",
      "Epoch 141 | Batch: 5 | Loss: 8.4801\n",
      "Epoch 141 | Batch: 6 | Loss: 4.0918\n",
      "Epoch 141 | Batch: 7 | Loss: 11.8658\n",
      "Epoch 141 | Batch: 8 | Loss: 8.2680\n",
      "Epoch 141 | Batch: 9 | Loss: 10.4851\n",
      "Epoch 141 | Batch: 10 | Loss: 11.6657\n",
      "Epoch 141 | Batch: 11 | Loss: 13.5764\n",
      "Epoch 141 | Batch: 12 | Loss: 5.5370\n",
      "Epoch 141 | Batch: 13 | Loss: 12.4615\n",
      "Epoch 141 | Batch: 14 | Loss: 23.8269\n",
      "Epoch 141 | Batch: 15 | Loss: 9.4406\n",
      "Epoch 141 | Batch: 16 | Loss: 9.9205\n",
      "Epoch 141 | Batch: 17 | Loss: 9.1734\n",
      "Epoch 141 | Batch: 18 | Loss: 4.2837\n",
      "Epoch 141 | Batch: 19 | Loss: 6.1401\n",
      "Epoch 141 | Batch: 20 | Loss: 7.9129\n",
      "Epoch 141 | Batch: 21 | Loss: 9.1902\n",
      "Epoch 141 | Batch: 22 | Loss: 9.4705\n",
      "Epoch 141 | Batch: 23 | Loss: 8.6111\n",
      "Epoch 141 | Batch: 24 | Loss: 10.6406\n",
      "Epoch 141 | Batch: 25 | Loss: 6.2062\n",
      "Epoch 141 | Batch: 26 | Loss: 7.1646\n",
      "Epoch 141 | Batch: 27 | Loss: 7.2111\n",
      "Epoch 141 | Batch: 28 | Loss: 5.3628\n",
      "Epoch 141 | Batch: 29 | Loss: 4.8562\n",
      "Epoch 141 | Batch: 30 | Loss: 8.2136\n",
      "Epoch 141 | Batch: 31 | Loss: 4.9052\n",
      "Epoch 141 | Batch: 32 | Loss: 10.4365\n",
      "Epoch 141 | Batch: 33 | Loss: 3.4816\n",
      "Epoch 141 | Batch: 34 | Loss: 5.7212\n",
      "Epoch 141 | Batch: 35 | Loss: 10.1305\n",
      "Epoch 141 | Batch: 36 | Loss: 2.9060\n",
      "Epoch 141 | Batch: 37 | Loss: 7.0556\n",
      "Epoch 141 | Batch: 38 | Loss: 13.4253\n",
      "Epoch 141 | Batch: 39 | Loss: 14.9296\n",
      "Epoch 141 | Batch: 40 | Loss: 12.4480\n",
      "Epoch 141 | Batch: 41 | Loss: 5.0690\n",
      "Epoch 141 | Batch: 42 | Loss: 7.9911\n",
      "Epoch 141 | Batch: 43 | Loss: 5.1061\n",
      "Epoch 141 | Batch: 44 | Loss: 9.5312\n",
      "Epoch 141 | Batch: 45 | Loss: 4.3191\n",
      "Epoch 141 | Batch: 46 | Loss: 12.9002\n",
      "Epoch 141 | Batch: 47 | Loss: 11.2272\n",
      "Epoch 141 | Batch: 48 | Loss: 2.6680\n",
      "Mean 8.639900863170624\n",
      "Epoch 142 | Batch: 1 | Loss: 5.1232\n",
      "Epoch 142 | Batch: 2 | Loss: 7.1221\n",
      "Epoch 142 | Batch: 3 | Loss: 8.0955\n",
      "Epoch 142 | Batch: 4 | Loss: 8.4464\n",
      "Epoch 142 | Batch: 5 | Loss: 6.7191\n",
      "Epoch 142 | Batch: 6 | Loss: 7.4194\n",
      "Epoch 142 | Batch: 7 | Loss: 7.7492\n",
      "Epoch 142 | Batch: 8 | Loss: 7.4927\n",
      "Epoch 142 | Batch: 9 | Loss: 10.6720\n",
      "Epoch 142 | Batch: 10 | Loss: 2.8272\n",
      "Epoch 142 | Batch: 11 | Loss: 12.6245\n",
      "Epoch 142 | Batch: 12 | Loss: 36.6851\n",
      "Epoch 142 | Batch: 13 | Loss: 32.7813\n",
      "Epoch 142 | Batch: 14 | Loss: 10.0179\n",
      "Epoch 142 | Batch: 15 | Loss: 7.3354\n",
      "Epoch 142 | Batch: 16 | Loss: 7.4104\n",
      "Epoch 142 | Batch: 17 | Loss: 6.7013\n",
      "Epoch 142 | Batch: 18 | Loss: 10.2901\n",
      "Epoch 142 | Batch: 19 | Loss: 13.3322\n",
      "Epoch 142 | Batch: 20 | Loss: 18.8462\n",
      "Epoch 142 | Batch: 21 | Loss: 18.3410\n",
      "Epoch 142 | Batch: 22 | Loss: 15.0352\n",
      "Epoch 142 | Batch: 23 | Loss: 11.2445\n",
      "Epoch 142 | Batch: 24 | Loss: 12.8410\n",
      "Epoch 142 | Batch: 25 | Loss: 11.8769\n",
      "Epoch 142 | Batch: 26 | Loss: 10.4567\n",
      "Epoch 142 | Batch: 27 | Loss: 9.1673\n",
      "Epoch 142 | Batch: 28 | Loss: 12.3888\n",
      "Epoch 142 | Batch: 29 | Loss: 13.2529\n",
      "Epoch 142 | Batch: 30 | Loss: 5.9225\n",
      "Epoch 142 | Batch: 31 | Loss: 9.4986\n",
      "Epoch 142 | Batch: 32 | Loss: 24.0611\n",
      "Epoch 142 | Batch: 33 | Loss: 12.1704\n",
      "Epoch 142 | Batch: 34 | Loss: 8.7103\n",
      "Epoch 142 | Batch: 35 | Loss: 5.6558\n",
      "Epoch 142 | Batch: 36 | Loss: 3.3128\n",
      "Epoch 142 | Batch: 37 | Loss: 8.8148\n",
      "Epoch 142 | Batch: 38 | Loss: 6.8211\n",
      "Epoch 142 | Batch: 39 | Loss: 6.0026\n",
      "Epoch 142 | Batch: 40 | Loss: 12.3567\n",
      "Epoch 142 | Batch: 41 | Loss: 8.4495\n",
      "Epoch 142 | Batch: 42 | Loss: 9.7178\n",
      "Epoch 142 | Batch: 43 | Loss: 6.3538\n",
      "Epoch 142 | Batch: 44 | Loss: 6.7932\n",
      "Epoch 142 | Batch: 45 | Loss: 10.2677\n",
      "Epoch 142 | Batch: 46 | Loss: 11.1798\n",
      "Epoch 142 | Batch: 47 | Loss: 7.7008\n",
      "Epoch 142 | Batch: 48 | Loss: 2.7193\n",
      "Mean 10.600085213780403\n",
      "Epoch 143 | Batch: 1 | Loss: 5.1025\n",
      "Epoch 143 | Batch: 2 | Loss: 3.2526\n",
      "Epoch 143 | Batch: 3 | Loss: 4.9719\n",
      "Epoch 143 | Batch: 4 | Loss: 12.3395\n",
      "Epoch 143 | Batch: 5 | Loss: 16.6875\n",
      "Epoch 143 | Batch: 6 | Loss: 8.1557\n",
      "Epoch 143 | Batch: 7 | Loss: 5.8299\n",
      "Epoch 143 | Batch: 8 | Loss: 10.5863\n",
      "Epoch 143 | Batch: 9 | Loss: 6.0844\n",
      "Epoch 143 | Batch: 10 | Loss: 2.8671\n",
      "Epoch 143 | Batch: 11 | Loss: 11.5044\n",
      "Epoch 143 | Batch: 12 | Loss: 4.5827\n",
      "Epoch 143 | Batch: 13 | Loss: 14.9171\n",
      "Epoch 143 | Batch: 14 | Loss: 14.0705\n",
      "Epoch 143 | Batch: 15 | Loss: 12.8098\n",
      "Epoch 143 | Batch: 16 | Loss: 11.4544\n",
      "Epoch 143 | Batch: 17 | Loss: 10.0723\n",
      "Epoch 143 | Batch: 18 | Loss: 6.0049\n",
      "Epoch 143 | Batch: 19 | Loss: 10.4120\n",
      "Epoch 143 | Batch: 20 | Loss: 14.7901\n",
      "Epoch 143 | Batch: 21 | Loss: 22.8556\n",
      "Epoch 143 | Batch: 22 | Loss: 12.1124\n",
      "Epoch 143 | Batch: 23 | Loss: 8.2967\n",
      "Epoch 143 | Batch: 24 | Loss: 8.0287\n",
      "Epoch 143 | Batch: 25 | Loss: 8.6637\n",
      "Epoch 143 | Batch: 26 | Loss: 13.0916\n",
      "Epoch 143 | Batch: 27 | Loss: 13.0334\n",
      "Epoch 143 | Batch: 28 | Loss: 12.2204\n",
      "Epoch 143 | Batch: 29 | Loss: 6.6566\n",
      "Epoch 143 | Batch: 30 | Loss: 12.2713\n",
      "Epoch 143 | Batch: 31 | Loss: 5.9057\n",
      "Epoch 143 | Batch: 32 | Loss: 10.9892\n",
      "Epoch 143 | Batch: 33 | Loss: 14.3414\n",
      "Epoch 143 | Batch: 34 | Loss: 13.0224\n",
      "Epoch 143 | Batch: 35 | Loss: 2.8589\n",
      "Epoch 143 | Batch: 36 | Loss: 8.3763\n",
      "Epoch 143 | Batch: 37 | Loss: 12.2330\n",
      "Epoch 143 | Batch: 38 | Loss: 6.2353\n",
      "Epoch 143 | Batch: 39 | Loss: 8.6848\n",
      "Epoch 143 | Batch: 40 | Loss: 6.5478\n",
      "Epoch 143 | Batch: 41 | Loss: 15.3632\n",
      "Epoch 143 | Batch: 42 | Loss: 15.4972\n",
      "Epoch 143 | Batch: 43 | Loss: 12.4389\n",
      "Epoch 143 | Batch: 44 | Loss: 3.9993\n",
      "Epoch 143 | Batch: 45 | Loss: 9.2852\n",
      "Epoch 143 | Batch: 46 | Loss: 8.1133\n",
      "Epoch 143 | Batch: 47 | Loss: 2.8237\n",
      "Epoch 143 | Batch: 48 | Loss: 3.6554\n",
      "Mean 9.668694774309794\n",
      "Epoch 144 | Batch: 1 | Loss: 8.5919\n",
      "Epoch 144 | Batch: 2 | Loss: 6.4308\n",
      "Epoch 144 | Batch: 3 | Loss: 8.2648\n",
      "Epoch 144 | Batch: 4 | Loss: 7.5379\n",
      "Epoch 144 | Batch: 5 | Loss: 13.9819\n",
      "Epoch 144 | Batch: 6 | Loss: 6.0390\n",
      "Epoch 144 | Batch: 7 | Loss: 6.3678\n",
      "Epoch 144 | Batch: 8 | Loss: 5.8721\n",
      "Epoch 144 | Batch: 9 | Loss: 6.0342\n",
      "Epoch 144 | Batch: 10 | Loss: 5.8671\n",
      "Epoch 144 | Batch: 11 | Loss: 4.8460\n",
      "Epoch 144 | Batch: 12 | Loss: 4.1213\n",
      "Epoch 144 | Batch: 13 | Loss: 4.5553\n",
      "Epoch 144 | Batch: 14 | Loss: 9.1539\n",
      "Epoch 144 | Batch: 15 | Loss: 22.7547\n",
      "Epoch 144 | Batch: 16 | Loss: 7.5350\n",
      "Epoch 144 | Batch: 17 | Loss: 19.3652\n",
      "Epoch 144 | Batch: 18 | Loss: 32.1451\n",
      "Epoch 144 | Batch: 19 | Loss: 7.7913\n",
      "Epoch 144 | Batch: 20 | Loss: 7.2563\n",
      "Epoch 144 | Batch: 21 | Loss: 10.2898\n",
      "Epoch 144 | Batch: 22 | Loss: 6.4665\n",
      "Epoch 144 | Batch: 23 | Loss: 21.3924\n",
      "Epoch 144 | Batch: 24 | Loss: 7.4842\n",
      "Epoch 144 | Batch: 25 | Loss: 10.2915\n",
      "Epoch 144 | Batch: 26 | Loss: 4.9954\n",
      "Epoch 144 | Batch: 27 | Loss: 15.7411\n",
      "Epoch 144 | Batch: 28 | Loss: 12.2931\n",
      "Epoch 144 | Batch: 29 | Loss: 5.2261\n",
      "Epoch 144 | Batch: 30 | Loss: 8.9494\n",
      "Epoch 144 | Batch: 31 | Loss: 12.3458\n",
      "Epoch 144 | Batch: 32 | Loss: 2.8461\n",
      "Epoch 144 | Batch: 33 | Loss: 15.5868\n",
      "Epoch 144 | Batch: 34 | Loss: 13.5659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144 | Batch: 35 | Loss: 6.5183\n",
      "Epoch 144 | Batch: 36 | Loss: 3.9795\n",
      "Epoch 144 | Batch: 37 | Loss: 6.0352\n",
      "Epoch 144 | Batch: 38 | Loss: 7.7581\n",
      "Epoch 144 | Batch: 39 | Loss: 7.1568\n",
      "Epoch 144 | Batch: 40 | Loss: 18.3720\n",
      "Epoch 144 | Batch: 41 | Loss: 22.5131\n",
      "Epoch 144 | Batch: 42 | Loss: 15.4592\n",
      "Epoch 144 | Batch: 43 | Loss: 11.0907\n",
      "Epoch 144 | Batch: 44 | Loss: 25.1413\n",
      "Epoch 144 | Batch: 45 | Loss: 11.8507\n",
      "Epoch 144 | Batch: 46 | Loss: 5.1141\n",
      "Epoch 144 | Batch: 47 | Loss: 7.4621\n",
      "Epoch 144 | Batch: 48 | Loss: 17.3141\n",
      "Mean 10.578144257267317\n",
      "Epoch 145 | Batch: 1 | Loss: 16.6464\n",
      "Epoch 145 | Batch: 2 | Loss: 6.3936\n",
      "Epoch 145 | Batch: 3 | Loss: 8.7019\n",
      "Epoch 145 | Batch: 4 | Loss: 11.3348\n",
      "Epoch 145 | Batch: 5 | Loss: 30.3559\n",
      "Epoch 145 | Batch: 6 | Loss: 12.3224\n",
      "Epoch 145 | Batch: 7 | Loss: 8.8254\n",
      "Epoch 145 | Batch: 8 | Loss: 5.0735\n",
      "Epoch 145 | Batch: 9 | Loss: 5.9898\n",
      "Epoch 145 | Batch: 10 | Loss: 8.7031\n",
      "Epoch 145 | Batch: 11 | Loss: 3.4714\n",
      "Epoch 145 | Batch: 12 | Loss: 11.8462\n",
      "Epoch 145 | Batch: 13 | Loss: 8.1509\n",
      "Epoch 145 | Batch: 14 | Loss: 4.6751\n",
      "Epoch 145 | Batch: 15 | Loss: 5.3465\n",
      "Epoch 145 | Batch: 16 | Loss: 5.3902\n",
      "Epoch 145 | Batch: 17 | Loss: 6.7842\n",
      "Epoch 145 | Batch: 18 | Loss: 10.1339\n",
      "Epoch 145 | Batch: 19 | Loss: 10.0444\n",
      "Epoch 145 | Batch: 20 | Loss: 5.0217\n",
      "Epoch 145 | Batch: 21 | Loss: 5.8745\n",
      "Epoch 145 | Batch: 22 | Loss: 8.8747\n",
      "Epoch 145 | Batch: 23 | Loss: 5.4779\n",
      "Epoch 145 | Batch: 24 | Loss: 8.3283\n",
      "Epoch 145 | Batch: 25 | Loss: 11.6666\n",
      "Epoch 145 | Batch: 26 | Loss: 5.6503\n",
      "Epoch 145 | Batch: 27 | Loss: 9.1526\n",
      "Epoch 145 | Batch: 28 | Loss: 5.8353\n",
      "Epoch 145 | Batch: 29 | Loss: 7.5875\n",
      "Epoch 145 | Batch: 30 | Loss: 3.5270\n",
      "Epoch 145 | Batch: 31 | Loss: 12.5329\n",
      "Epoch 145 | Batch: 32 | Loss: 4.5563\n",
      "Epoch 145 | Batch: 33 | Loss: 3.7693\n",
      "Epoch 145 | Batch: 34 | Loss: 4.2404\n",
      "Epoch 145 | Batch: 35 | Loss: 7.8945\n",
      "Epoch 145 | Batch: 36 | Loss: 10.9442\n",
      "Epoch 145 | Batch: 37 | Loss: 12.9982\n",
      "Epoch 145 | Batch: 38 | Loss: 7.2982\n",
      "Epoch 145 | Batch: 39 | Loss: 5.2557\n",
      "Epoch 145 | Batch: 40 | Loss: 20.7959\n",
      "Epoch 145 | Batch: 41 | Loss: 32.8969\n",
      "Epoch 145 | Batch: 42 | Loss: 20.2838\n",
      "Epoch 145 | Batch: 43 | Loss: 7.8065\n",
      "Epoch 145 | Batch: 44 | Loss: 7.2860\n",
      "Epoch 145 | Batch: 45 | Loss: 7.0304\n",
      "Epoch 145 | Batch: 46 | Loss: 8.6980\n",
      "Epoch 145 | Batch: 47 | Loss: 17.4371\n",
      "Epoch 145 | Batch: 48 | Loss: 1.6337\n",
      "Mean 9.386335964004198\n",
      "Epoch 146 | Batch: 1 | Loss: 5.7296\n",
      "Epoch 146 | Batch: 2 | Loss: 4.3089\n",
      "Epoch 146 | Batch: 3 | Loss: 5.4076\n",
      "Epoch 146 | Batch: 4 | Loss: 6.3715\n",
      "Epoch 146 | Batch: 5 | Loss: 7.4764\n",
      "Epoch 146 | Batch: 6 | Loss: 8.5514\n",
      "Epoch 146 | Batch: 7 | Loss: 7.9310\n",
      "Epoch 146 | Batch: 8 | Loss: 8.6686\n",
      "Epoch 146 | Batch: 9 | Loss: 5.5353\n",
      "Epoch 146 | Batch: 10 | Loss: 0.9854\n",
      "Epoch 146 | Batch: 11 | Loss: 21.4184\n",
      "Epoch 146 | Batch: 12 | Loss: 28.2332\n",
      "Epoch 146 | Batch: 13 | Loss: 22.0037\n",
      "Epoch 146 | Batch: 14 | Loss: 6.2632\n",
      "Epoch 146 | Batch: 15 | Loss: 4.6233\n",
      "Epoch 146 | Batch: 16 | Loss: 6.0866\n",
      "Epoch 146 | Batch: 17 | Loss: 10.7586\n",
      "Epoch 146 | Batch: 18 | Loss: 15.6562\n",
      "Epoch 146 | Batch: 19 | Loss: 10.4372\n",
      "Epoch 146 | Batch: 20 | Loss: 8.4155\n",
      "Epoch 146 | Batch: 21 | Loss: 14.0856\n",
      "Epoch 146 | Batch: 22 | Loss: 9.0487\n",
      "Epoch 146 | Batch: 23 | Loss: 9.3140\n",
      "Epoch 146 | Batch: 24 | Loss: 7.2318\n",
      "Epoch 146 | Batch: 25 | Loss: 9.3913\n",
      "Epoch 146 | Batch: 26 | Loss: 8.7905\n",
      "Epoch 146 | Batch: 27 | Loss: 6.7176\n",
      "Epoch 146 | Batch: 28 | Loss: 4.3790\n",
      "Epoch 146 | Batch: 29 | Loss: 7.5453\n",
      "Epoch 146 | Batch: 30 | Loss: 14.6964\n",
      "Epoch 146 | Batch: 31 | Loss: 12.6483\n",
      "Epoch 146 | Batch: 32 | Loss: 7.2386\n",
      "Epoch 146 | Batch: 33 | Loss: 6.8916\n",
      "Epoch 146 | Batch: 34 | Loss: 3.8621\n",
      "Epoch 146 | Batch: 35 | Loss: 8.9521\n",
      "Epoch 146 | Batch: 36 | Loss: 5.2562\n",
      "Epoch 146 | Batch: 37 | Loss: 8.1792\n",
      "Epoch 146 | Batch: 38 | Loss: 7.0255\n",
      "Epoch 146 | Batch: 39 | Loss: 9.1210\n",
      "Epoch 146 | Batch: 40 | Loss: 5.5806\n",
      "Epoch 146 | Batch: 41 | Loss: 6.1701\n",
      "Epoch 146 | Batch: 42 | Loss: 14.6633\n",
      "Epoch 146 | Batch: 43 | Loss: 4.2545\n",
      "Epoch 146 | Batch: 44 | Loss: 7.9752\n",
      "Epoch 146 | Batch: 45 | Loss: 7.2313\n",
      "Epoch 146 | Batch: 46 | Loss: 10.8300\n",
      "Epoch 146 | Batch: 47 | Loss: 6.4116\n",
      "Epoch 146 | Batch: 48 | Loss: 4.0749\n",
      "Mean 8.800582802544037\n",
      "Epoch 147 | Batch: 1 | Loss: 5.8940\n",
      "Epoch 147 | Batch: 2 | Loss: 9.4506\n",
      "Epoch 147 | Batch: 3 | Loss: 17.5194\n",
      "Epoch 147 | Batch: 4 | Loss: 18.6290\n",
      "Epoch 147 | Batch: 5 | Loss: 29.4491\n",
      "Epoch 147 | Batch: 6 | Loss: 12.6244\n",
      "Epoch 147 | Batch: 7 | Loss: 10.7426\n",
      "Epoch 147 | Batch: 8 | Loss: 6.0652\n",
      "Epoch 147 | Batch: 9 | Loss: 7.5167\n",
      "Epoch 147 | Batch: 10 | Loss: 8.5968\n",
      "Epoch 147 | Batch: 11 | Loss: 11.0892\n",
      "Epoch 147 | Batch: 12 | Loss: 6.9893\n",
      "Epoch 147 | Batch: 13 | Loss: 14.4645\n",
      "Epoch 147 | Batch: 14 | Loss: 16.1212\n",
      "Epoch 147 | Batch: 15 | Loss: 10.6045\n",
      "Epoch 147 | Batch: 16 | Loss: 10.0874\n",
      "Epoch 147 | Batch: 17 | Loss: 6.7480\n",
      "Epoch 147 | Batch: 18 | Loss: 8.3315\n",
      "Epoch 147 | Batch: 19 | Loss: 8.3517\n",
      "Epoch 147 | Batch: 20 | Loss: 12.9114\n",
      "Epoch 147 | Batch: 21 | Loss: 28.5947\n",
      "Epoch 147 | Batch: 22 | Loss: 28.1455\n",
      "Epoch 147 | Batch: 23 | Loss: 6.6659\n",
      "Epoch 147 | Batch: 24 | Loss: 9.3228\n",
      "Epoch 147 | Batch: 25 | Loss: 7.2135\n",
      "Epoch 147 | Batch: 26 | Loss: 9.4893\n",
      "Epoch 147 | Batch: 27 | Loss: 5.8067\n",
      "Epoch 147 | Batch: 28 | Loss: 7.4742\n",
      "Epoch 147 | Batch: 29 | Loss: 2.4214\n",
      "Epoch 147 | Batch: 30 | Loss: 1.8845\n",
      "Epoch 147 | Batch: 31 | Loss: 3.2472\n",
      "Epoch 147 | Batch: 32 | Loss: 10.0139\n",
      "Epoch 147 | Batch: 33 | Loss: 17.7706\n",
      "Epoch 147 | Batch: 34 | Loss: 3.5470\n",
      "Epoch 147 | Batch: 35 | Loss: 5.1405\n",
      "Epoch 147 | Batch: 36 | Loss: 5.9633\n",
      "Epoch 147 | Batch: 37 | Loss: 9.1837\n",
      "Epoch 147 | Batch: 38 | Loss: 9.5883\n",
      "Epoch 147 | Batch: 39 | Loss: 4.7693\n",
      "Epoch 147 | Batch: 40 | Loss: 7.6657\n",
      "Epoch 147 | Batch: 41 | Loss: 6.7466\n",
      "Epoch 147 | Batch: 42 | Loss: 6.5602\n",
      "Epoch 147 | Batch: 43 | Loss: 5.3918\n",
      "Epoch 147 | Batch: 44 | Loss: 4.8792\n",
      "Epoch 147 | Batch: 45 | Loss: 3.1545\n",
      "Epoch 147 | Batch: 46 | Loss: 14.1400\n",
      "Epoch 147 | Batch: 47 | Loss: 3.7204\n",
      "Epoch 147 | Batch: 48 | Loss: 4.4609\n",
      "Mean 9.69058232754469\n",
      "Epoch 148 | Batch: 1 | Loss: 10.6109\n",
      "Epoch 148 | Batch: 2 | Loss: 4.8106\n",
      "Epoch 148 | Batch: 3 | Loss: 7.1485\n",
      "Epoch 148 | Batch: 4 | Loss: 7.0957\n",
      "Epoch 148 | Batch: 5 | Loss: 13.1028\n",
      "Epoch 148 | Batch: 6 | Loss: 7.2570\n",
      "Epoch 148 | Batch: 7 | Loss: 4.4261\n",
      "Epoch 148 | Batch: 8 | Loss: 9.2418\n",
      "Epoch 148 | Batch: 9 | Loss: 9.8760\n",
      "Epoch 148 | Batch: 10 | Loss: 4.7845\n",
      "Epoch 148 | Batch: 11 | Loss: 6.3389\n",
      "Epoch 148 | Batch: 12 | Loss: 8.2628\n",
      "Epoch 148 | Batch: 13 | Loss: 11.1829\n",
      "Epoch 148 | Batch: 14 | Loss: 13.4158\n",
      "Epoch 148 | Batch: 15 | Loss: 9.5997\n",
      "Epoch 148 | Batch: 16 | Loss: 7.7113\n",
      "Epoch 148 | Batch: 17 | Loss: 4.0944\n",
      "Epoch 148 | Batch: 18 | Loss: 7.7914\n",
      "Epoch 148 | Batch: 19 | Loss: 6.3742\n",
      "Epoch 148 | Batch: 20 | Loss: 6.0456\n",
      "Epoch 148 | Batch: 21 | Loss: 5.2417\n",
      "Epoch 148 | Batch: 22 | Loss: 5.3833\n",
      "Epoch 148 | Batch: 23 | Loss: 5.1506\n",
      "Epoch 148 | Batch: 24 | Loss: 4.3324\n",
      "Epoch 148 | Batch: 25 | Loss: 7.5255\n",
      "Epoch 148 | Batch: 26 | Loss: 7.0196\n",
      "Epoch 148 | Batch: 27 | Loss: 7.9803\n",
      "Epoch 148 | Batch: 28 | Loss: 6.0316\n",
      "Epoch 148 | Batch: 29 | Loss: 6.7293\n",
      "Epoch 148 | Batch: 30 | Loss: 12.6043\n",
      "Epoch 148 | Batch: 31 | Loss: 1.7012\n",
      "Epoch 148 | Batch: 32 | Loss: 15.2170\n",
      "Epoch 148 | Batch: 33 | Loss: 18.5418\n",
      "Epoch 148 | Batch: 34 | Loss: 6.2600\n",
      "Epoch 148 | Batch: 35 | Loss: 11.5263\n",
      "Epoch 148 | Batch: 36 | Loss: 7.3796\n",
      "Epoch 148 | Batch: 37 | Loss: 8.0344\n",
      "Epoch 148 | Batch: 38 | Loss: 5.3836\n",
      "Epoch 148 | Batch: 39 | Loss: 12.9793\n",
      "Epoch 148 | Batch: 40 | Loss: 8.4836\n",
      "Epoch 148 | Batch: 41 | Loss: 18.5557\n",
      "Epoch 148 | Batch: 42 | Loss: 13.6570\n",
      "Epoch 148 | Batch: 43 | Loss: 5.6793\n",
      "Epoch 148 | Batch: 44 | Loss: 9.1978\n",
      "Epoch 148 | Batch: 45 | Loss: 7.4112\n",
      "Epoch 148 | Batch: 46 | Loss: 9.3535\n",
      "Epoch 148 | Batch: 47 | Loss: 6.3576\n",
      "Epoch 148 | Batch: 48 | Loss: 2.0647\n",
      "Mean 8.228181210656961\n",
      "Epoch 149 | Batch: 1 | Loss: 6.2300\n",
      "Epoch 149 | Batch: 2 | Loss: 11.3665\n",
      "Epoch 149 | Batch: 3 | Loss: 10.6226\n",
      "Epoch 149 | Batch: 4 | Loss: 10.5033\n",
      "Epoch 149 | Batch: 5 | Loss: 6.2605\n",
      "Epoch 149 | Batch: 6 | Loss: 9.3303\n",
      "Epoch 149 | Batch: 7 | Loss: 6.2638\n",
      "Epoch 149 | Batch: 8 | Loss: 9.3604\n",
      "Epoch 149 | Batch: 9 | Loss: 28.3294\n",
      "Epoch 149 | Batch: 10 | Loss: 22.6472\n",
      "Epoch 149 | Batch: 11 | Loss: 7.6670\n",
      "Epoch 149 | Batch: 12 | Loss: 8.6900\n",
      "Epoch 149 | Batch: 13 | Loss: 7.5738\n",
      "Epoch 149 | Batch: 14 | Loss: 6.5747\n",
      "Epoch 149 | Batch: 15 | Loss: 8.7033\n",
      "Epoch 149 | Batch: 16 | Loss: 12.2432\n",
      "Epoch 149 | Batch: 17 | Loss: 7.9894\n",
      "Epoch 149 | Batch: 18 | Loss: 7.7075\n",
      "Epoch 149 | Batch: 19 | Loss: 9.6986\n",
      "Epoch 149 | Batch: 20 | Loss: 9.3108\n",
      "Epoch 149 | Batch: 21 | Loss: 5.2614\n",
      "Epoch 149 | Batch: 22 | Loss: 7.9729\n",
      "Epoch 149 | Batch: 23 | Loss: 7.9328\n",
      "Epoch 149 | Batch: 24 | Loss: 6.9354\n",
      "Epoch 149 | Batch: 25 | Loss: 15.2442\n",
      "Epoch 149 | Batch: 26 | Loss: 12.2176\n",
      "Epoch 149 | Batch: 27 | Loss: 5.5804\n",
      "Epoch 149 | Batch: 28 | Loss: 6.4626\n",
      "Epoch 149 | Batch: 29 | Loss: 9.0734\n",
      "Epoch 149 | Batch: 30 | Loss: 6.1377\n",
      "Epoch 149 | Batch: 31 | Loss: 9.2669\n",
      "Epoch 149 | Batch: 32 | Loss: 9.0585\n",
      "Epoch 149 | Batch: 33 | Loss: 5.6272\n",
      "Epoch 149 | Batch: 34 | Loss: 15.1045\n",
      "Epoch 149 | Batch: 35 | Loss: 12.0667\n",
      "Epoch 149 | Batch: 36 | Loss: 8.5997\n",
      "Epoch 149 | Batch: 37 | Loss: 7.2361\n",
      "Epoch 149 | Batch: 38 | Loss: 7.2942\n",
      "Epoch 149 | Batch: 39 | Loss: 5.8894\n",
      "Epoch 149 | Batch: 40 | Loss: 5.2129\n",
      "Epoch 149 | Batch: 41 | Loss: 5.3765\n",
      "Epoch 149 | Batch: 42 | Loss: 7.7343\n",
      "Epoch 149 | Batch: 43 | Loss: 6.7774\n",
      "Epoch 149 | Batch: 44 | Loss: 1.6790\n",
      "Epoch 149 | Batch: 45 | Loss: 11.3213\n",
      "Epoch 149 | Batch: 46 | Loss: 5.2592\n",
      "Epoch 149 | Batch: 47 | Loss: 7.2448\n",
      "Epoch 149 | Batch: 48 | Loss: 1.2654\n",
      "Mean 8.789679748316606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150 | Batch: 1 | Loss: 5.5093\n",
      "Epoch 150 | Batch: 2 | Loss: 5.1674\n",
      "Epoch 150 | Batch: 3 | Loss: 7.9866\n",
      "Epoch 150 | Batch: 4 | Loss: 5.3713\n",
      "Epoch 150 | Batch: 5 | Loss: 13.4313\n",
      "Epoch 150 | Batch: 6 | Loss: 9.8432\n",
      "Epoch 150 | Batch: 7 | Loss: 13.8875\n",
      "Epoch 150 | Batch: 8 | Loss: 11.7498\n",
      "Epoch 150 | Batch: 9 | Loss: 9.8852\n",
      "Epoch 150 | Batch: 10 | Loss: 8.0400\n",
      "Epoch 150 | Batch: 11 | Loss: 7.3365\n",
      "Epoch 150 | Batch: 12 | Loss: 10.9903\n",
      "Epoch 150 | Batch: 13 | Loss: 16.5926\n",
      "Epoch 150 | Batch: 14 | Loss: 19.1740\n",
      "Epoch 150 | Batch: 15 | Loss: 7.8212\n",
      "Epoch 150 | Batch: 16 | Loss: 7.8906\n",
      "Epoch 150 | Batch: 17 | Loss: 9.0661\n",
      "Epoch 150 | Batch: 18 | Loss: 8.3106\n",
      "Epoch 150 | Batch: 19 | Loss: 8.8218\n",
      "Epoch 150 | Batch: 20 | Loss: 9.2765\n",
      "Epoch 150 | Batch: 21 | Loss: 18.0626\n",
      "Epoch 150 | Batch: 22 | Loss: 6.8115\n",
      "Epoch 150 | Batch: 23 | Loss: 5.6371\n",
      "Epoch 150 | Batch: 24 | Loss: 8.7247\n",
      "Epoch 150 | Batch: 25 | Loss: 8.0892\n",
      "Epoch 150 | Batch: 26 | Loss: 7.2093\n",
      "Epoch 150 | Batch: 27 | Loss: 7.5184\n",
      "Epoch 150 | Batch: 28 | Loss: 6.2652\n",
      "Epoch 150 | Batch: 29 | Loss: 7.0534\n",
      "Epoch 150 | Batch: 30 | Loss: 7.5712\n",
      "Epoch 150 | Batch: 31 | Loss: 11.9802\n",
      "Epoch 150 | Batch: 32 | Loss: 6.0231\n",
      "Epoch 150 | Batch: 33 | Loss: 3.2423\n",
      "Epoch 150 | Batch: 34 | Loss: 6.2292\n",
      "Epoch 150 | Batch: 35 | Loss: 5.5796\n",
      "Epoch 150 | Batch: 36 | Loss: 5.4874\n",
      "Epoch 150 | Batch: 37 | Loss: 8.4955\n",
      "Epoch 150 | Batch: 38 | Loss: 7.5720\n",
      "Epoch 150 | Batch: 39 | Loss: 11.3340\n",
      "Epoch 150 | Batch: 40 | Loss: 4.7617\n",
      "Epoch 150 | Batch: 41 | Loss: 8.6517\n",
      "Epoch 150 | Batch: 42 | Loss: 5.6567\n",
      "Epoch 150 | Batch: 43 | Loss: 3.6658\n",
      "Epoch 150 | Batch: 44 | Loss: 7.5731\n",
      "Epoch 150 | Batch: 45 | Loss: 14.6326\n",
      "Epoch 150 | Batch: 46 | Loss: 13.5559\n",
      "Epoch 150 | Batch: 47 | Loss: 13.9387\n",
      "Epoch 150 | Batch: 48 | Loss: 8.8390\n",
      "Mean 8.88152226805687\n",
      "Epoch 151 | Batch: 1 | Loss: 8.2146\n",
      "Epoch 151 | Batch: 2 | Loss: 8.6535\n",
      "Epoch 151 | Batch: 3 | Loss: 16.9918\n",
      "Epoch 151 | Batch: 4 | Loss: 25.9306\n",
      "Epoch 151 | Batch: 5 | Loss: 12.2416\n",
      "Epoch 151 | Batch: 6 | Loss: 9.3149\n",
      "Epoch 151 | Batch: 7 | Loss: 5.7043\n",
      "Epoch 151 | Batch: 8 | Loss: 11.2284\n",
      "Epoch 151 | Batch: 9 | Loss: 10.4375\n",
      "Epoch 151 | Batch: 10 | Loss: 5.3490\n",
      "Epoch 151 | Batch: 11 | Loss: 5.4442\n",
      "Epoch 151 | Batch: 12 | Loss: 4.1721\n",
      "Epoch 151 | Batch: 13 | Loss: 3.6576\n",
      "Epoch 151 | Batch: 14 | Loss: 2.8298\n",
      "Epoch 151 | Batch: 15 | Loss: 9.4085\n",
      "Epoch 151 | Batch: 16 | Loss: 6.3237\n",
      "Epoch 151 | Batch: 17 | Loss: 2.2775\n",
      "Epoch 151 | Batch: 18 | Loss: 10.9741\n",
      "Epoch 151 | Batch: 19 | Loss: 9.6099\n",
      "Epoch 151 | Batch: 20 | Loss: 4.6576\n",
      "Epoch 151 | Batch: 21 | Loss: 9.1091\n",
      "Epoch 151 | Batch: 22 | Loss: 10.8758\n",
      "Epoch 151 | Batch: 23 | Loss: 7.1493\n",
      "Epoch 151 | Batch: 24 | Loss: 8.4599\n",
      "Epoch 151 | Batch: 25 | Loss: 12.2769\n",
      "Epoch 151 | Batch: 26 | Loss: 6.4501\n",
      "Epoch 151 | Batch: 27 | Loss: 5.8257\n",
      "Epoch 151 | Batch: 28 | Loss: 11.0034\n",
      "Epoch 151 | Batch: 29 | Loss: 9.2260\n",
      "Epoch 151 | Batch: 30 | Loss: 12.4597\n",
      "Epoch 151 | Batch: 31 | Loss: 10.9890\n",
      "Epoch 151 | Batch: 32 | Loss: 7.8685\n",
      "Epoch 151 | Batch: 33 | Loss: 3.6903\n",
      "Epoch 151 | Batch: 34 | Loss: 7.5966\n",
      "Epoch 151 | Batch: 35 | Loss: 19.3420\n",
      "Epoch 151 | Batch: 36 | Loss: 33.7980\n",
      "Epoch 151 | Batch: 37 | Loss: 8.1220\n",
      "Epoch 151 | Batch: 38 | Loss: 7.2361\n",
      "Epoch 151 | Batch: 39 | Loss: 9.3289\n",
      "Epoch 151 | Batch: 40 | Loss: 4.8593\n",
      "Epoch 151 | Batch: 41 | Loss: 5.8775\n",
      "Epoch 151 | Batch: 42 | Loss: 3.1921\n",
      "Epoch 151 | Batch: 43 | Loss: 4.9488\n",
      "Epoch 151 | Batch: 44 | Loss: 11.7901\n",
      "Epoch 151 | Batch: 45 | Loss: 10.7206\n",
      "Epoch 151 | Batch: 46 | Loss: 5.8812\n",
      "Epoch 151 | Batch: 47 | Loss: 9.9247\n",
      "Epoch 151 | Batch: 48 | Loss: 2.8472\n",
      "Mean 9.047300159931183\n",
      "Epoch 152 | Batch: 1 | Loss: 10.6954\n",
      "Epoch 152 | Batch: 2 | Loss: 11.8994\n",
      "Epoch 152 | Batch: 3 | Loss: 20.9795\n",
      "Epoch 152 | Batch: 4 | Loss: 25.1734\n",
      "Epoch 152 | Batch: 5 | Loss: 17.1185\n",
      "Epoch 152 | Batch: 6 | Loss: 6.0054\n",
      "Epoch 152 | Batch: 7 | Loss: 5.7755\n",
      "Epoch 152 | Batch: 8 | Loss: 6.4197\n",
      "Epoch 152 | Batch: 9 | Loss: 5.4468\n",
      "Epoch 152 | Batch: 10 | Loss: 14.8722\n",
      "Epoch 152 | Batch: 11 | Loss: 11.4796\n",
      "Epoch 152 | Batch: 12 | Loss: 10.5278\n",
      "Epoch 152 | Batch: 13 | Loss: 10.7400\n",
      "Epoch 152 | Batch: 14 | Loss: 6.4689\n",
      "Epoch 152 | Batch: 15 | Loss: 4.6239\n",
      "Epoch 152 | Batch: 16 | Loss: 4.5607\n",
      "Epoch 152 | Batch: 17 | Loss: 4.6477\n",
      "Epoch 152 | Batch: 18 | Loss: 8.4532\n",
      "Epoch 152 | Batch: 19 | Loss: 19.6909\n",
      "Epoch 152 | Batch: 20 | Loss: 12.3695\n",
      "Epoch 152 | Batch: 21 | Loss: 5.6403\n",
      "Epoch 152 | Batch: 22 | Loss: 10.4008\n",
      "Epoch 152 | Batch: 23 | Loss: 8.6654\n",
      "Epoch 152 | Batch: 24 | Loss: 12.8699\n",
      "Epoch 152 | Batch: 25 | Loss: 5.9050\n",
      "Epoch 152 | Batch: 26 | Loss: 3.0908\n",
      "Epoch 152 | Batch: 27 | Loss: 10.0245\n",
      "Epoch 152 | Batch: 28 | Loss: 14.3668\n",
      "Epoch 152 | Batch: 29 | Loss: 20.2112\n",
      "Epoch 152 | Batch: 30 | Loss: 3.9480\n",
      "Epoch 152 | Batch: 31 | Loss: 20.0388\n",
      "Epoch 152 | Batch: 32 | Loss: 15.2436\n",
      "Epoch 152 | Batch: 33 | Loss: 6.5762\n",
      "Epoch 152 | Batch: 34 | Loss: 7.7936\n",
      "Epoch 152 | Batch: 35 | Loss: 8.7290\n",
      "Epoch 152 | Batch: 36 | Loss: 7.0949\n",
      "Epoch 152 | Batch: 37 | Loss: 4.9958\n",
      "Epoch 152 | Batch: 38 | Loss: 7.1283\n",
      "Epoch 152 | Batch: 39 | Loss: 8.5127\n",
      "Epoch 152 | Batch: 40 | Loss: 7.2568\n",
      "Epoch 152 | Batch: 41 | Loss: 5.7053\n",
      "Epoch 152 | Batch: 42 | Loss: 9.1429\n",
      "Epoch 152 | Batch: 43 | Loss: 9.7650\n",
      "Epoch 152 | Batch: 44 | Loss: 8.6911\n",
      "Epoch 152 | Batch: 45 | Loss: 8.8873\n",
      "Epoch 152 | Batch: 46 | Loss: 12.5707\n",
      "Epoch 152 | Batch: 47 | Loss: 18.2956\n",
      "Epoch 152 | Batch: 48 | Loss: 8.5455\n",
      "Mean 10.167578925689062\n",
      "Epoch 153 | Batch: 1 | Loss: 9.7835\n",
      "Epoch 153 | Batch: 2 | Loss: 16.7582\n",
      "Epoch 153 | Batch: 3 | Loss: 11.4997\n",
      "Epoch 153 | Batch: 4 | Loss: 8.2172\n",
      "Epoch 153 | Batch: 5 | Loss: 5.7417\n",
      "Epoch 153 | Batch: 6 | Loss: 13.6920\n",
      "Epoch 153 | Batch: 7 | Loss: 15.4143\n",
      "Epoch 153 | Batch: 8 | Loss: 7.3237\n",
      "Epoch 153 | Batch: 9 | Loss: 8.6108\n",
      "Epoch 153 | Batch: 10 | Loss: 8.1373\n",
      "Epoch 153 | Batch: 11 | Loss: 4.4891\n",
      "Epoch 153 | Batch: 12 | Loss: 6.8926\n",
      "Epoch 153 | Batch: 13 | Loss: 4.6387\n",
      "Epoch 153 | Batch: 14 | Loss: 6.1282\n",
      "Epoch 153 | Batch: 15 | Loss: 5.6662\n",
      "Epoch 153 | Batch: 16 | Loss: 9.9564\n",
      "Epoch 153 | Batch: 17 | Loss: 12.0077\n",
      "Epoch 153 | Batch: 18 | Loss: 13.5754\n",
      "Epoch 153 | Batch: 19 | Loss: 9.4781\n",
      "Epoch 153 | Batch: 20 | Loss: 17.6688\n",
      "Epoch 153 | Batch: 21 | Loss: 22.0426\n",
      "Epoch 153 | Batch: 22 | Loss: 9.0378\n",
      "Epoch 153 | Batch: 23 | Loss: 10.9478\n",
      "Epoch 153 | Batch: 24 | Loss: 9.0467\n",
      "Epoch 153 | Batch: 25 | Loss: 8.7586\n",
      "Epoch 153 | Batch: 26 | Loss: 13.1808\n",
      "Epoch 153 | Batch: 27 | Loss: 8.0656\n",
      "Epoch 153 | Batch: 28 | Loss: 10.5960\n",
      "Epoch 153 | Batch: 29 | Loss: 7.1243\n",
      "Epoch 153 | Batch: 30 | Loss: 12.6239\n",
      "Epoch 153 | Batch: 31 | Loss: 6.2296\n",
      "Epoch 153 | Batch: 32 | Loss: 4.2374\n",
      "Epoch 153 | Batch: 33 | Loss: 14.2588\n",
      "Epoch 153 | Batch: 34 | Loss: 12.4329\n",
      "Epoch 153 | Batch: 35 | Loss: 6.6273\n",
      "Epoch 153 | Batch: 36 | Loss: 12.6586\n",
      "Epoch 153 | Batch: 37 | Loss: 7.1585\n",
      "Epoch 153 | Batch: 38 | Loss: 14.4748\n",
      "Epoch 153 | Batch: 39 | Loss: 4.5335\n",
      "Epoch 153 | Batch: 40 | Loss: 4.2833\n",
      "Epoch 153 | Batch: 41 | Loss: 8.0988\n",
      "Epoch 153 | Batch: 42 | Loss: 6.3373\n",
      "Epoch 153 | Batch: 43 | Loss: 11.1759\n",
      "Epoch 153 | Batch: 44 | Loss: 10.8176\n",
      "Epoch 153 | Batch: 45 | Loss: 9.1236\n",
      "Epoch 153 | Batch: 46 | Loss: 3.3398\n",
      "Epoch 153 | Batch: 47 | Loss: 5.4010\n",
      "Epoch 153 | Batch: 48 | Loss: 0.7549\n",
      "Mean 9.35515172407031\n",
      "Epoch 154 | Batch: 1 | Loss: 5.3399\n",
      "Epoch 154 | Batch: 2 | Loss: 9.2279\n",
      "Epoch 154 | Batch: 3 | Loss: 9.8753\n",
      "Epoch 154 | Batch: 4 | Loss: 7.5491\n",
      "Epoch 154 | Batch: 5 | Loss: 11.2833\n",
      "Epoch 154 | Batch: 6 | Loss: 6.8649\n",
      "Epoch 154 | Batch: 7 | Loss: 6.5999\n",
      "Epoch 154 | Batch: 8 | Loss: 11.0702\n",
      "Epoch 154 | Batch: 9 | Loss: 7.1213\n",
      "Epoch 154 | Batch: 10 | Loss: 5.2217\n",
      "Epoch 154 | Batch: 11 | Loss: 3.8815\n",
      "Epoch 154 | Batch: 12 | Loss: 8.1995\n",
      "Epoch 154 | Batch: 13 | Loss: 19.5615\n",
      "Epoch 154 | Batch: 14 | Loss: 31.1093\n",
      "Epoch 154 | Batch: 15 | Loss: 14.0612\n",
      "Epoch 154 | Batch: 16 | Loss: 5.4918\n",
      "Epoch 154 | Batch: 17 | Loss: 8.3256\n",
      "Epoch 154 | Batch: 18 | Loss: 5.5365\n",
      "Epoch 154 | Batch: 19 | Loss: 9.8261\n",
      "Epoch 154 | Batch: 20 | Loss: 9.2435\n",
      "Epoch 154 | Batch: 21 | Loss: 6.9120\n",
      "Epoch 154 | Batch: 22 | Loss: 8.4453\n",
      "Epoch 154 | Batch: 23 | Loss: 5.8333\n",
      "Epoch 154 | Batch: 24 | Loss: 5.9711\n",
      "Epoch 154 | Batch: 25 | Loss: 5.0019\n",
      "Epoch 154 | Batch: 26 | Loss: 1.8738\n",
      "Epoch 154 | Batch: 27 | Loss: 5.7391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154 | Batch: 28 | Loss: 7.1613\n",
      "Epoch 154 | Batch: 29 | Loss: 7.4862\n",
      "Epoch 154 | Batch: 30 | Loss: 7.1319\n",
      "Epoch 154 | Batch: 31 | Loss: 10.1625\n",
      "Epoch 154 | Batch: 32 | Loss: 11.0238\n",
      "Epoch 154 | Batch: 33 | Loss: 6.6660\n",
      "Epoch 154 | Batch: 34 | Loss: 17.3006\n",
      "Epoch 154 | Batch: 35 | Loss: 14.4157\n",
      "Epoch 154 | Batch: 36 | Loss: 3.7656\n",
      "Epoch 154 | Batch: 37 | Loss: 10.4937\n",
      "Epoch 154 | Batch: 38 | Loss: 6.3901\n",
      "Epoch 154 | Batch: 39 | Loss: 13.5455\n",
      "Epoch 154 | Batch: 40 | Loss: 8.8364\n",
      "Epoch 154 | Batch: 41 | Loss: 12.3665\n",
      "Epoch 154 | Batch: 42 | Loss: 15.2150\n",
      "Epoch 154 | Batch: 43 | Loss: 18.1313\n",
      "Epoch 154 | Batch: 44 | Loss: 20.9883\n",
      "Epoch 154 | Batch: 45 | Loss: 20.5210\n",
      "Epoch 154 | Batch: 46 | Loss: 6.4549\n",
      "Epoch 154 | Batch: 47 | Loss: 5.9596\n",
      "Epoch 154 | Batch: 48 | Loss: 3.2996\n",
      "Mean 9.635046211381754\n",
      "Epoch 155 | Batch: 1 | Loss: 2.4294\n",
      "Epoch 155 | Batch: 2 | Loss: 2.0970\n",
      "Epoch 155 | Batch: 3 | Loss: 4.1177\n",
      "Epoch 155 | Batch: 4 | Loss: 9.0267\n",
      "Epoch 155 | Batch: 5 | Loss: 8.2399\n",
      "Epoch 155 | Batch: 6 | Loss: 10.1654\n",
      "Epoch 155 | Batch: 7 | Loss: 5.6560\n",
      "Epoch 155 | Batch: 8 | Loss: 6.4861\n",
      "Epoch 155 | Batch: 9 | Loss: 2.8872\n",
      "Epoch 155 | Batch: 10 | Loss: 10.9824\n",
      "Epoch 155 | Batch: 11 | Loss: 10.9845\n",
      "Epoch 155 | Batch: 12 | Loss: 8.5255\n",
      "Epoch 155 | Batch: 13 | Loss: 7.5540\n",
      "Epoch 155 | Batch: 14 | Loss: 5.8000\n",
      "Epoch 155 | Batch: 15 | Loss: 6.4206\n",
      "Epoch 155 | Batch: 16 | Loss: 3.5835\n",
      "Epoch 155 | Batch: 17 | Loss: 12.5020\n",
      "Epoch 155 | Batch: 18 | Loss: 11.2836\n",
      "Epoch 155 | Batch: 19 | Loss: 3.0260\n",
      "Epoch 155 | Batch: 20 | Loss: 4.9953\n",
      "Epoch 155 | Batch: 21 | Loss: 9.1818\n",
      "Epoch 155 | Batch: 22 | Loss: 10.1872\n",
      "Epoch 155 | Batch: 23 | Loss: 12.9912\n",
      "Epoch 155 | Batch: 24 | Loss: 10.4041\n",
      "Epoch 155 | Batch: 25 | Loss: 7.7838\n",
      "Epoch 155 | Batch: 26 | Loss: 10.5482\n",
      "Epoch 155 | Batch: 27 | Loss: 7.1169\n",
      "Epoch 155 | Batch: 28 | Loss: 6.8985\n",
      "Epoch 155 | Batch: 29 | Loss: 3.1222\n",
      "Epoch 155 | Batch: 30 | Loss: 10.7555\n",
      "Epoch 155 | Batch: 31 | Loss: 6.2914\n",
      "Epoch 155 | Batch: 32 | Loss: 11.0103\n",
      "Epoch 155 | Batch: 33 | Loss: 9.0847\n",
      "Epoch 155 | Batch: 34 | Loss: 7.2513\n",
      "Epoch 155 | Batch: 35 | Loss: 14.3036\n",
      "Epoch 155 | Batch: 36 | Loss: 6.9083\n",
      "Epoch 155 | Batch: 37 | Loss: 6.1732\n",
      "Epoch 155 | Batch: 38 | Loss: 6.9488\n",
      "Epoch 155 | Batch: 39 | Loss: 6.7911\n",
      "Epoch 155 | Batch: 40 | Loss: 7.9663\n",
      "Epoch 155 | Batch: 41 | Loss: 4.7812\n",
      "Epoch 155 | Batch: 42 | Loss: 7.6352\n",
      "Epoch 155 | Batch: 43 | Loss: 15.0729\n",
      "Epoch 155 | Batch: 44 | Loss: 13.5280\n",
      "Epoch 155 | Batch: 45 | Loss: 11.9079\n",
      "Epoch 155 | Batch: 46 | Loss: 8.4611\n",
      "Epoch 155 | Batch: 47 | Loss: 9.6439\n",
      "Epoch 155 | Batch: 48 | Loss: 7.3922\n",
      "Mean 8.060487801829973\n",
      "Epoch 156 | Batch: 1 | Loss: 4.6111\n",
      "Epoch 156 | Batch: 2 | Loss: 7.1754\n",
      "Epoch 156 | Batch: 3 | Loss: 4.4604\n",
      "Epoch 156 | Batch: 4 | Loss: 5.9850\n",
      "Epoch 156 | Batch: 5 | Loss: 6.9347\n",
      "Epoch 156 | Batch: 6 | Loss: 7.7750\n",
      "Epoch 156 | Batch: 7 | Loss: 8.3442\n",
      "Epoch 156 | Batch: 8 | Loss: 10.8976\n",
      "Epoch 156 | Batch: 9 | Loss: 23.9069\n",
      "Epoch 156 | Batch: 10 | Loss: 14.1589\n",
      "Epoch 156 | Batch: 11 | Loss: 12.0013\n",
      "Epoch 156 | Batch: 12 | Loss: 10.0919\n",
      "Epoch 156 | Batch: 13 | Loss: 7.1843\n",
      "Epoch 156 | Batch: 14 | Loss: 6.9617\n",
      "Epoch 156 | Batch: 15 | Loss: 6.7817\n",
      "Epoch 156 | Batch: 16 | Loss: 15.8761\n",
      "Epoch 156 | Batch: 17 | Loss: 10.0980\n",
      "Epoch 156 | Batch: 18 | Loss: 8.0462\n",
      "Epoch 156 | Batch: 19 | Loss: 20.9120\n",
      "Epoch 156 | Batch: 20 | Loss: 8.2994\n",
      "Epoch 156 | Batch: 21 | Loss: 3.8286\n",
      "Epoch 156 | Batch: 22 | Loss: 5.1797\n",
      "Epoch 156 | Batch: 23 | Loss: 4.6963\n",
      "Epoch 156 | Batch: 24 | Loss: 11.9087\n",
      "Epoch 156 | Batch: 25 | Loss: 12.4880\n",
      "Epoch 156 | Batch: 26 | Loss: 13.7677\n",
      "Epoch 156 | Batch: 27 | Loss: 3.9398\n",
      "Epoch 156 | Batch: 28 | Loss: 8.1178\n",
      "Epoch 156 | Batch: 29 | Loss: 8.5269\n",
      "Epoch 156 | Batch: 30 | Loss: 15.1862\n",
      "Epoch 156 | Batch: 31 | Loss: 11.9577\n",
      "Epoch 156 | Batch: 32 | Loss: 12.8693\n",
      "Epoch 156 | Batch: 33 | Loss: 11.7916\n",
      "Epoch 156 | Batch: 34 | Loss: 4.9694\n",
      "Epoch 156 | Batch: 35 | Loss: 14.0116\n",
      "Epoch 156 | Batch: 36 | Loss: 16.3685\n",
      "Epoch 156 | Batch: 37 | Loss: 10.8237\n",
      "Epoch 156 | Batch: 38 | Loss: 20.8871\n",
      "Epoch 156 | Batch: 39 | Loss: 19.5894\n",
      "Epoch 156 | Batch: 40 | Loss: 14.8785\n",
      "Epoch 156 | Batch: 41 | Loss: 12.9978\n",
      "Epoch 156 | Batch: 42 | Loss: 9.1441\n",
      "Epoch 156 | Batch: 43 | Loss: 7.0659\n",
      "Epoch 156 | Batch: 44 | Loss: 9.4524\n",
      "Epoch 156 | Batch: 45 | Loss: 13.8071\n",
      "Epoch 156 | Batch: 46 | Loss: 7.5606\n",
      "Epoch 156 | Batch: 47 | Loss: 7.9769\n",
      "Epoch 156 | Batch: 48 | Loss: 2.4916\n",
      "Mean 10.349687442183495\n",
      "Epoch 157 | Batch: 1 | Loss: 4.0323\n",
      "Epoch 157 | Batch: 2 | Loss: 4.2954\n",
      "Epoch 157 | Batch: 3 | Loss: 9.2953\n",
      "Epoch 157 | Batch: 4 | Loss: 7.8636\n",
      "Epoch 157 | Batch: 5 | Loss: 11.1734\n",
      "Epoch 157 | Batch: 6 | Loss: 7.9555\n",
      "Epoch 157 | Batch: 7 | Loss: 9.5996\n",
      "Epoch 157 | Batch: 8 | Loss: 3.2440\n",
      "Epoch 157 | Batch: 9 | Loss: 7.1076\n",
      "Epoch 157 | Batch: 10 | Loss: 7.6804\n",
      "Epoch 157 | Batch: 11 | Loss: 7.6094\n",
      "Epoch 157 | Batch: 12 | Loss: 10.4120\n",
      "Epoch 157 | Batch: 13 | Loss: 7.2032\n",
      "Epoch 157 | Batch: 14 | Loss: 5.0730\n",
      "Epoch 157 | Batch: 15 | Loss: 5.1946\n",
      "Epoch 157 | Batch: 16 | Loss: 13.2406\n",
      "Epoch 157 | Batch: 17 | Loss: 9.6550\n",
      "Epoch 157 | Batch: 18 | Loss: 12.6833\n",
      "Epoch 157 | Batch: 19 | Loss: 16.6003\n",
      "Epoch 157 | Batch: 20 | Loss: 13.9976\n",
      "Epoch 157 | Batch: 21 | Loss: 14.2270\n",
      "Epoch 157 | Batch: 22 | Loss: 5.2810\n",
      "Epoch 157 | Batch: 23 | Loss: 5.8557\n",
      "Epoch 157 | Batch: 24 | Loss: 5.9863\n",
      "Epoch 157 | Batch: 25 | Loss: 2.4670\n",
      "Epoch 157 | Batch: 26 | Loss: 5.7826\n",
      "Epoch 157 | Batch: 27 | Loss: 8.8040\n",
      "Epoch 157 | Batch: 28 | Loss: 7.0147\n",
      "Epoch 157 | Batch: 29 | Loss: 11.0303\n",
      "Epoch 157 | Batch: 30 | Loss: 18.0656\n",
      "Epoch 157 | Batch: 31 | Loss: 26.3126\n",
      "Epoch 157 | Batch: 32 | Loss: 11.0940\n",
      "Epoch 157 | Batch: 33 | Loss: 7.8204\n",
      "Epoch 157 | Batch: 34 | Loss: 7.5390\n",
      "Epoch 157 | Batch: 35 | Loss: 4.6488\n",
      "Epoch 157 | Batch: 36 | Loss: 6.1983\n",
      "Epoch 157 | Batch: 37 | Loss: 7.8690\n",
      "Epoch 157 | Batch: 38 | Loss: 9.1848\n",
      "Epoch 157 | Batch: 39 | Loss: 7.7898\n",
      "Epoch 157 | Batch: 40 | Loss: 11.1233\n",
      "Epoch 157 | Batch: 41 | Loss: 9.9957\n",
      "Epoch 157 | Batch: 42 | Loss: 9.8209\n",
      "Epoch 157 | Batch: 43 | Loss: 8.2341\n",
      "Epoch 157 | Batch: 44 | Loss: 6.8278\n",
      "Epoch 157 | Batch: 45 | Loss: 10.0431\n",
      "Epoch 157 | Batch: 46 | Loss: 7.0748\n",
      "Epoch 157 | Batch: 47 | Loss: 9.3030\n",
      "Epoch 157 | Batch: 48 | Loss: 2.1363\n",
      "Mean 8.780130580067635\n",
      "Epoch 158 | Batch: 1 | Loss: 2.7562\n",
      "Epoch 158 | Batch: 2 | Loss: 7.8789\n",
      "Epoch 158 | Batch: 3 | Loss: 4.8468\n",
      "Epoch 158 | Batch: 4 | Loss: 9.2514\n",
      "Epoch 158 | Batch: 5 | Loss: 6.0827\n",
      "Epoch 158 | Batch: 6 | Loss: 9.2166\n",
      "Epoch 158 | Batch: 7 | Loss: 7.9758\n",
      "Epoch 158 | Batch: 8 | Loss: 5.7775\n",
      "Epoch 158 | Batch: 9 | Loss: 8.6185\n",
      "Epoch 158 | Batch: 10 | Loss: 6.1806\n",
      "Epoch 158 | Batch: 11 | Loss: 3.2244\n",
      "Epoch 158 | Batch: 12 | Loss: 9.1567\n",
      "Epoch 158 | Batch: 13 | Loss: 5.5999\n",
      "Epoch 158 | Batch: 14 | Loss: 5.4626\n",
      "Epoch 158 | Batch: 15 | Loss: 3.1439\n",
      "Epoch 158 | Batch: 16 | Loss: 11.5781\n",
      "Epoch 158 | Batch: 17 | Loss: 12.4506\n",
      "Epoch 158 | Batch: 18 | Loss: 8.2433\n",
      "Epoch 158 | Batch: 19 | Loss: 8.6535\n",
      "Epoch 158 | Batch: 20 | Loss: 12.7436\n",
      "Epoch 158 | Batch: 21 | Loss: 17.9957\n",
      "Epoch 158 | Batch: 22 | Loss: 7.5629\n",
      "Epoch 158 | Batch: 23 | Loss: 5.3155\n",
      "Epoch 158 | Batch: 24 | Loss: 3.1749\n",
      "Epoch 158 | Batch: 25 | Loss: 4.7680\n",
      "Epoch 158 | Batch: 26 | Loss: 4.6349\n",
      "Epoch 158 | Batch: 27 | Loss: 8.6067\n",
      "Epoch 158 | Batch: 28 | Loss: 8.1100\n",
      "Epoch 158 | Batch: 29 | Loss: 7.8220\n",
      "Epoch 158 | Batch: 30 | Loss: 6.1932\n",
      "Epoch 158 | Batch: 31 | Loss: 7.4335\n",
      "Epoch 158 | Batch: 32 | Loss: 9.8261\n",
      "Epoch 158 | Batch: 33 | Loss: 9.7865\n",
      "Epoch 158 | Batch: 34 | Loss: 2.5676\n",
      "Epoch 158 | Batch: 35 | Loss: 11.9473\n",
      "Epoch 158 | Batch: 36 | Loss: 14.0090\n",
      "Epoch 158 | Batch: 37 | Loss: 9.3302\n",
      "Epoch 158 | Batch: 38 | Loss: 5.8756\n",
      "Epoch 158 | Batch: 39 | Loss: 7.4111\n",
      "Epoch 158 | Batch: 40 | Loss: 12.1736\n",
      "Epoch 158 | Batch: 41 | Loss: 4.5115\n",
      "Epoch 158 | Batch: 42 | Loss: 3.2314\n",
      "Epoch 158 | Batch: 43 | Loss: 6.8364\n",
      "Epoch 158 | Batch: 44 | Loss: 10.5046\n",
      "Epoch 158 | Batch: 45 | Loss: 8.2666\n",
      "Epoch 158 | Batch: 46 | Loss: 11.3188\n",
      "Epoch 158 | Batch: 47 | Loss: 7.2807\n",
      "Epoch 158 | Batch: 48 | Loss: 0.8944\n",
      "Mean 7.629794606318076\n",
      "Epoch 159 | Batch: 1 | Loss: 9.8673\n",
      "Epoch 159 | Batch: 2 | Loss: 10.8920\n",
      "Epoch 159 | Batch: 3 | Loss: 18.0842\n",
      "Epoch 159 | Batch: 4 | Loss: 9.7632\n",
      "Epoch 159 | Batch: 5 | Loss: 8.0451\n",
      "Epoch 159 | Batch: 6 | Loss: 2.0722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159 | Batch: 7 | Loss: 11.8106\n",
      "Epoch 159 | Batch: 8 | Loss: 10.8055\n",
      "Epoch 159 | Batch: 9 | Loss: 6.7496\n",
      "Epoch 159 | Batch: 10 | Loss: 8.6726\n",
      "Epoch 159 | Batch: 11 | Loss: 6.1441\n",
      "Epoch 159 | Batch: 12 | Loss: 7.5433\n",
      "Epoch 159 | Batch: 13 | Loss: 8.1243\n",
      "Epoch 159 | Batch: 14 | Loss: 5.6529\n",
      "Epoch 159 | Batch: 15 | Loss: 15.2791\n",
      "Epoch 159 | Batch: 16 | Loss: 12.6316\n",
      "Epoch 159 | Batch: 17 | Loss: 17.2763\n",
      "Epoch 159 | Batch: 18 | Loss: 15.9493\n",
      "Epoch 159 | Batch: 19 | Loss: 4.0016\n",
      "Epoch 159 | Batch: 20 | Loss: 7.4941\n",
      "Epoch 159 | Batch: 21 | Loss: 8.6098\n",
      "Epoch 159 | Batch: 22 | Loss: 5.5924\n",
      "Epoch 159 | Batch: 23 | Loss: 10.9301\n",
      "Epoch 159 | Batch: 24 | Loss: 6.6159\n",
      "Epoch 159 | Batch: 25 | Loss: 7.0053\n",
      "Epoch 159 | Batch: 26 | Loss: 6.0240\n",
      "Epoch 159 | Batch: 27 | Loss: 9.7642\n",
      "Epoch 159 | Batch: 28 | Loss: 5.2522\n",
      "Epoch 159 | Batch: 29 | Loss: 11.6996\n",
      "Epoch 159 | Batch: 30 | Loss: 8.6184\n",
      "Epoch 159 | Batch: 31 | Loss: 7.3506\n",
      "Epoch 159 | Batch: 32 | Loss: 5.4302\n",
      "Epoch 159 | Batch: 33 | Loss: 9.6726\n",
      "Epoch 159 | Batch: 34 | Loss: 21.6315\n",
      "Epoch 159 | Batch: 35 | Loss: 5.5693\n",
      "Epoch 159 | Batch: 36 | Loss: 10.0690\n",
      "Epoch 159 | Batch: 37 | Loss: 9.0114\n",
      "Epoch 159 | Batch: 38 | Loss: 9.5634\n",
      "Epoch 159 | Batch: 39 | Loss: 5.2004\n",
      "Epoch 159 | Batch: 40 | Loss: 8.0074\n",
      "Epoch 159 | Batch: 41 | Loss: 8.1980\n",
      "Epoch 159 | Batch: 42 | Loss: 9.4679\n",
      "Epoch 159 | Batch: 43 | Loss: 12.8389\n",
      "Epoch 159 | Batch: 44 | Loss: 8.8753\n",
      "Epoch 159 | Batch: 45 | Loss: 4.2971\n",
      "Epoch 159 | Batch: 46 | Loss: 8.7474\n",
      "Epoch 159 | Batch: 47 | Loss: 11.0362\n",
      "Epoch 159 | Batch: 48 | Loss: 7.4602\n",
      "Mean 9.154118989904722\n",
      "Epoch 160 | Batch: 1 | Loss: 10.7967\n",
      "Epoch 160 | Batch: 2 | Loss: 10.5169\n",
      "Epoch 160 | Batch: 3 | Loss: 7.7563\n",
      "Epoch 160 | Batch: 4 | Loss: 6.8082\n",
      "Epoch 160 | Batch: 5 | Loss: 5.0192\n",
      "Epoch 160 | Batch: 6 | Loss: 2.6132\n",
      "Epoch 160 | Batch: 7 | Loss: 5.8058\n",
      "Epoch 160 | Batch: 8 | Loss: 5.2416\n",
      "Epoch 160 | Batch: 9 | Loss: 8.3160\n",
      "Epoch 160 | Batch: 10 | Loss: 14.7677\n",
      "Epoch 160 | Batch: 11 | Loss: 16.3527\n",
      "Epoch 160 | Batch: 12 | Loss: 13.9213\n",
      "Epoch 160 | Batch: 13 | Loss: 18.3190\n",
      "Epoch 160 | Batch: 14 | Loss: 14.7623\n",
      "Epoch 160 | Batch: 15 | Loss: 7.8484\n",
      "Epoch 160 | Batch: 16 | Loss: 11.2398\n",
      "Epoch 160 | Batch: 17 | Loss: 13.1906\n",
      "Epoch 160 | Batch: 18 | Loss: 24.3117\n",
      "Epoch 160 | Batch: 19 | Loss: 25.2625\n",
      "Epoch 160 | Batch: 20 | Loss: 9.2613\n",
      "Epoch 160 | Batch: 21 | Loss: 5.5287\n",
      "Epoch 160 | Batch: 22 | Loss: 15.4794\n",
      "Epoch 160 | Batch: 23 | Loss: 12.8219\n",
      "Epoch 160 | Batch: 24 | Loss: 10.4380\n",
      "Epoch 160 | Batch: 25 | Loss: 15.7140\n",
      "Epoch 160 | Batch: 26 | Loss: 4.9096\n",
      "Epoch 160 | Batch: 27 | Loss: 9.0376\n",
      "Epoch 160 | Batch: 28 | Loss: 12.6419\n",
      "Epoch 160 | Batch: 29 | Loss: 9.7845\n",
      "Epoch 160 | Batch: 30 | Loss: 11.0897\n",
      "Epoch 160 | Batch: 31 | Loss: 7.9363\n",
      "Epoch 160 | Batch: 32 | Loss: 4.6377\n",
      "Epoch 160 | Batch: 33 | Loss: 12.3714\n",
      "Epoch 160 | Batch: 34 | Loss: 7.2542\n",
      "Epoch 160 | Batch: 35 | Loss: 4.8058\n",
      "Epoch 160 | Batch: 36 | Loss: 6.9049\n",
      "Epoch 160 | Batch: 37 | Loss: 7.5774\n",
      "Epoch 160 | Batch: 38 | Loss: 9.5434\n",
      "Epoch 160 | Batch: 39 | Loss: 9.2807\n",
      "Epoch 160 | Batch: 40 | Loss: 7.2767\n",
      "Epoch 160 | Batch: 41 | Loss: 8.7106\n",
      "Epoch 160 | Batch: 42 | Loss: 7.8914\n",
      "Epoch 160 | Batch: 43 | Loss: 14.2734\n",
      "Epoch 160 | Batch: 44 | Loss: 11.7456\n",
      "Epoch 160 | Batch: 45 | Loss: 8.9440\n",
      "Epoch 160 | Batch: 46 | Loss: 6.8633\n",
      "Epoch 160 | Batch: 47 | Loss: 7.1112\n",
      "Epoch 160 | Batch: 48 | Loss: 5.3079\n",
      "Mean 10.166507398088774\n",
      "Epoch 161 | Batch: 1 | Loss: 8.1986\n",
      "Epoch 161 | Batch: 2 | Loss: 14.5127\n",
      "Epoch 161 | Batch: 3 | Loss: 15.3650\n",
      "Epoch 161 | Batch: 4 | Loss: 2.8757\n",
      "Epoch 161 | Batch: 5 | Loss: 1.1817\n",
      "Epoch 161 | Batch: 6 | Loss: 8.4267\n",
      "Epoch 161 | Batch: 7 | Loss: 8.7055\n",
      "Epoch 161 | Batch: 8 | Loss: 10.3474\n",
      "Epoch 161 | Batch: 9 | Loss: 5.0903\n",
      "Epoch 161 | Batch: 10 | Loss: 10.7278\n",
      "Epoch 161 | Batch: 11 | Loss: 4.9691\n",
      "Epoch 161 | Batch: 12 | Loss: 15.1851\n",
      "Epoch 161 | Batch: 13 | Loss: 6.1333\n",
      "Epoch 161 | Batch: 14 | Loss: 6.1442\n",
      "Epoch 161 | Batch: 15 | Loss: 15.9936\n",
      "Epoch 161 | Batch: 16 | Loss: 9.3597\n",
      "Epoch 161 | Batch: 17 | Loss: 4.4856\n",
      "Epoch 161 | Batch: 18 | Loss: 17.7516\n",
      "Epoch 161 | Batch: 19 | Loss: 18.7709\n",
      "Epoch 161 | Batch: 20 | Loss: 10.8694\n",
      "Epoch 161 | Batch: 21 | Loss: 5.7715\n",
      "Epoch 161 | Batch: 22 | Loss: 4.9728\n",
      "Epoch 161 | Batch: 23 | Loss: 5.1423\n",
      "Epoch 161 | Batch: 24 | Loss: 6.9639\n",
      "Epoch 161 | Batch: 25 | Loss: 4.1884\n",
      "Epoch 161 | Batch: 26 | Loss: 2.9161\n",
      "Epoch 161 | Batch: 27 | Loss: 6.0562\n",
      "Epoch 161 | Batch: 28 | Loss: 2.8478\n",
      "Epoch 161 | Batch: 29 | Loss: 3.0865\n",
      "Epoch 161 | Batch: 30 | Loss: 2.3762\n",
      "Epoch 161 | Batch: 31 | Loss: 13.3064\n",
      "Epoch 161 | Batch: 32 | Loss: 8.5098\n",
      "Epoch 161 | Batch: 33 | Loss: 5.0387\n",
      "Epoch 161 | Batch: 34 | Loss: 9.4711\n",
      "Epoch 161 | Batch: 35 | Loss: 14.5743\n",
      "Epoch 161 | Batch: 36 | Loss: 9.5162\n",
      "Epoch 161 | Batch: 37 | Loss: 15.9396\n",
      "Epoch 161 | Batch: 38 | Loss: 12.9748\n",
      "Epoch 161 | Batch: 39 | Loss: 10.8446\n",
      "Epoch 161 | Batch: 40 | Loss: 1.3288\n",
      "Epoch 161 | Batch: 41 | Loss: 15.1297\n",
      "Epoch 161 | Batch: 42 | Loss: 8.0173\n",
      "Epoch 161 | Batch: 43 | Loss: 7.9092\n",
      "Epoch 161 | Batch: 44 | Loss: 10.4739\n",
      "Epoch 161 | Batch: 45 | Loss: 7.9682\n",
      "Epoch 161 | Batch: 46 | Loss: 7.3482\n",
      "Epoch 161 | Batch: 47 | Loss: 9.8612\n",
      "Epoch 161 | Batch: 48 | Loss: 5.1339\n",
      "Mean 8.599201376239458\n",
      "Epoch 162 | Batch: 1 | Loss: 5.6852\n",
      "Epoch 162 | Batch: 2 | Loss: 7.0598\n",
      "Epoch 162 | Batch: 3 | Loss: 9.8967\n",
      "Epoch 162 | Batch: 4 | Loss: 6.9838\n",
      "Epoch 162 | Batch: 5 | Loss: 7.9440\n",
      "Epoch 162 | Batch: 6 | Loss: 14.0548\n",
      "Epoch 162 | Batch: 7 | Loss: 16.0809\n",
      "Epoch 162 | Batch: 8 | Loss: 3.2056\n",
      "Epoch 162 | Batch: 9 | Loss: 10.5245\n",
      "Epoch 162 | Batch: 10 | Loss: 11.6809\n",
      "Epoch 162 | Batch: 11 | Loss: 5.8407\n",
      "Epoch 162 | Batch: 12 | Loss: 8.7181\n",
      "Epoch 162 | Batch: 13 | Loss: 5.9867\n",
      "Epoch 162 | Batch: 14 | Loss: 6.9287\n",
      "Epoch 162 | Batch: 15 | Loss: 8.7620\n",
      "Epoch 162 | Batch: 16 | Loss: 10.5025\n",
      "Epoch 162 | Batch: 17 | Loss: 4.5675\n",
      "Epoch 162 | Batch: 18 | Loss: 9.3076\n",
      "Epoch 162 | Batch: 19 | Loss: 8.7289\n",
      "Epoch 162 | Batch: 20 | Loss: 7.5964\n",
      "Epoch 162 | Batch: 21 | Loss: 6.1845\n",
      "Epoch 162 | Batch: 22 | Loss: 6.7661\n",
      "Epoch 162 | Batch: 23 | Loss: 14.4701\n",
      "Epoch 162 | Batch: 24 | Loss: 7.5347\n",
      "Epoch 162 | Batch: 25 | Loss: 5.0171\n",
      "Epoch 162 | Batch: 26 | Loss: 13.9996\n",
      "Epoch 162 | Batch: 27 | Loss: 7.6805\n",
      "Epoch 162 | Batch: 28 | Loss: 4.3139\n",
      "Epoch 162 | Batch: 29 | Loss: 5.9684\n",
      "Epoch 162 | Batch: 30 | Loss: 4.3403\n",
      "Epoch 162 | Batch: 31 | Loss: 13.4900\n",
      "Epoch 162 | Batch: 32 | Loss: 8.4260\n",
      "Epoch 162 | Batch: 33 | Loss: 3.4712\n",
      "Epoch 162 | Batch: 34 | Loss: 8.3977\n",
      "Epoch 162 | Batch: 35 | Loss: 9.9167\n",
      "Epoch 162 | Batch: 36 | Loss: 14.2187\n",
      "Epoch 162 | Batch: 37 | Loss: 16.8440\n",
      "Epoch 162 | Batch: 38 | Loss: 7.3395\n",
      "Epoch 162 | Batch: 39 | Loss: 10.9840\n",
      "Epoch 162 | Batch: 40 | Loss: 4.1263\n",
      "Epoch 162 | Batch: 41 | Loss: 14.4013\n",
      "Epoch 162 | Batch: 42 | Loss: 26.8536\n",
      "Epoch 162 | Batch: 43 | Loss: 4.6286\n",
      "Epoch 162 | Batch: 44 | Loss: 8.3201\n",
      "Epoch 162 | Batch: 45 | Loss: 8.3108\n",
      "Epoch 162 | Batch: 46 | Loss: 12.4069\n",
      "Epoch 162 | Batch: 47 | Loss: 9.5259\n",
      "Epoch 162 | Batch: 48 | Loss: 15.1879\n",
      "Mean 9.232912391424179\n",
      "Epoch 163 | Batch: 1 | Loss: 6.0501\n",
      "Epoch 163 | Batch: 2 | Loss: 8.2266\n",
      "Epoch 163 | Batch: 3 | Loss: 8.3466\n",
      "Epoch 163 | Batch: 4 | Loss: 4.6338\n",
      "Epoch 163 | Batch: 5 | Loss: 8.5391\n",
      "Epoch 163 | Batch: 6 | Loss: 8.9740\n",
      "Epoch 163 | Batch: 7 | Loss: 22.8499\n",
      "Epoch 163 | Batch: 8 | Loss: 22.4056\n",
      "Epoch 163 | Batch: 9 | Loss: 18.5614\n",
      "Epoch 163 | Batch: 10 | Loss: 9.0166\n",
      "Epoch 163 | Batch: 11 | Loss: 5.9497\n",
      "Epoch 163 | Batch: 12 | Loss: 7.5582\n",
      "Epoch 163 | Batch: 13 | Loss: 4.3508\n",
      "Epoch 163 | Batch: 14 | Loss: 5.4329\n",
      "Epoch 163 | Batch: 15 | Loss: 4.6621\n",
      "Epoch 163 | Batch: 16 | Loss: 10.2504\n",
      "Epoch 163 | Batch: 17 | Loss: 5.8462\n",
      "Epoch 163 | Batch: 18 | Loss: 11.5719\n",
      "Epoch 163 | Batch: 19 | Loss: 7.6989\n",
      "Epoch 163 | Batch: 20 | Loss: 4.7516\n",
      "Epoch 163 | Batch: 21 | Loss: 7.7128\n",
      "Epoch 163 | Batch: 22 | Loss: 3.6499\n",
      "Epoch 163 | Batch: 23 | Loss: 2.7337\n",
      "Epoch 163 | Batch: 24 | Loss: 8.8051\n",
      "Epoch 163 | Batch: 25 | Loss: 13.4910\n",
      "Epoch 163 | Batch: 26 | Loss: 13.4349\n",
      "Epoch 163 | Batch: 27 | Loss: 12.3135\n",
      "Epoch 163 | Batch: 28 | Loss: 7.0737\n",
      "Epoch 163 | Batch: 29 | Loss: 8.0533\n",
      "Epoch 163 | Batch: 30 | Loss: 9.2824\n",
      "Epoch 163 | Batch: 31 | Loss: 10.3706\n",
      "Epoch 163 | Batch: 32 | Loss: 10.8433\n",
      "Epoch 163 | Batch: 33 | Loss: 5.3122\n",
      "Epoch 163 | Batch: 34 | Loss: 7.7739\n",
      "Epoch 163 | Batch: 35 | Loss: 8.7758\n",
      "Epoch 163 | Batch: 36 | Loss: 8.5355\n",
      "Epoch 163 | Batch: 37 | Loss: 10.2010\n",
      "Epoch 163 | Batch: 38 | Loss: 6.1275\n",
      "Epoch 163 | Batch: 39 | Loss: 7.7961\n",
      "Epoch 163 | Batch: 40 | Loss: 13.6016\n",
      "Epoch 163 | Batch: 41 | Loss: 10.0182\n",
      "Epoch 163 | Batch: 42 | Loss: 5.2297\n",
      "Epoch 163 | Batch: 43 | Loss: 11.9569\n",
      "Epoch 163 | Batch: 44 | Loss: 10.2461\n",
      "Epoch 163 | Batch: 45 | Loss: 11.2740\n",
      "Epoch 163 | Batch: 46 | Loss: 12.1902\n",
      "Epoch 163 | Batch: 47 | Loss: 13.9769\n",
      "Epoch 163 | Batch: 48 | Loss: 3.2548\n",
      "Mean 9.160641034444174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164 | Batch: 1 | Loss: 6.9648\n",
      "Epoch 164 | Batch: 2 | Loss: 8.6687\n",
      "Epoch 164 | Batch: 3 | Loss: 9.3981\n",
      "Epoch 164 | Batch: 4 | Loss: 19.4853\n",
      "Epoch 164 | Batch: 5 | Loss: 15.7051\n",
      "Epoch 164 | Batch: 6 | Loss: 16.3387\n",
      "Epoch 164 | Batch: 7 | Loss: 10.8594\n",
      "Epoch 164 | Batch: 8 | Loss: 8.8574\n",
      "Epoch 164 | Batch: 9 | Loss: 7.7410\n",
      "Epoch 164 | Batch: 10 | Loss: 10.6298\n",
      "Epoch 164 | Batch: 11 | Loss: 13.4179\n",
      "Epoch 164 | Batch: 12 | Loss: 9.7687\n",
      "Epoch 164 | Batch: 13 | Loss: 8.5499\n",
      "Epoch 164 | Batch: 14 | Loss: 6.8469\n",
      "Epoch 164 | Batch: 15 | Loss: 7.4477\n",
      "Epoch 164 | Batch: 16 | Loss: 8.9877\n",
      "Epoch 164 | Batch: 17 | Loss: 6.5060\n",
      "Epoch 164 | Batch: 18 | Loss: 10.9710\n",
      "Epoch 164 | Batch: 19 | Loss: 8.3837\n",
      "Epoch 164 | Batch: 20 | Loss: 10.8110\n",
      "Epoch 164 | Batch: 21 | Loss: 6.2995\n",
      "Epoch 164 | Batch: 22 | Loss: 1.8597\n",
      "Epoch 164 | Batch: 23 | Loss: 12.1699\n",
      "Epoch 164 | Batch: 24 | Loss: 9.4064\n",
      "Epoch 164 | Batch: 25 | Loss: 9.7289\n",
      "Epoch 164 | Batch: 26 | Loss: 17.6299\n",
      "Epoch 164 | Batch: 27 | Loss: 19.0223\n",
      "Epoch 164 | Batch: 28 | Loss: 4.9699\n",
      "Epoch 164 | Batch: 29 | Loss: 5.9850\n",
      "Epoch 164 | Batch: 30 | Loss: 4.8568\n",
      "Epoch 164 | Batch: 31 | Loss: 2.8788\n",
      "Epoch 164 | Batch: 32 | Loss: 11.3741\n",
      "Epoch 164 | Batch: 33 | Loss: 8.7292\n",
      "Epoch 164 | Batch: 34 | Loss: 6.6089\n",
      "Epoch 164 | Batch: 35 | Loss: 7.7573\n",
      "Epoch 164 | Batch: 36 | Loss: 4.3412\n",
      "Epoch 164 | Batch: 37 | Loss: 8.4757\n",
      "Epoch 164 | Batch: 38 | Loss: 14.4387\n",
      "Epoch 164 | Batch: 39 | Loss: 29.3712\n",
      "Epoch 164 | Batch: 40 | Loss: 34.6565\n",
      "Epoch 164 | Batch: 41 | Loss: 9.5726\n",
      "Epoch 164 | Batch: 42 | Loss: 6.1649\n",
      "Epoch 164 | Batch: 43 | Loss: 7.7004\n",
      "Epoch 164 | Batch: 44 | Loss: 8.2584\n",
      "Epoch 164 | Batch: 45 | Loss: 4.4817\n",
      "Epoch 164 | Batch: 46 | Loss: 4.4742\n",
      "Epoch 164 | Batch: 47 | Loss: 6.3187\n",
      "Epoch 164 | Batch: 48 | Loss: 0.6835\n",
      "Mean 9.886523318787416\n",
      "Epoch 165 | Batch: 1 | Loss: 9.2454\n",
      "Epoch 165 | Batch: 2 | Loss: 16.5109\n",
      "Epoch 165 | Batch: 3 | Loss: 6.7522\n",
      "Epoch 165 | Batch: 4 | Loss: 4.1317\n",
      "Epoch 165 | Batch: 5 | Loss: 6.0900\n",
      "Epoch 165 | Batch: 6 | Loss: 12.7314\n",
      "Epoch 165 | Batch: 7 | Loss: 10.9716\n",
      "Epoch 165 | Batch: 8 | Loss: 9.1046\n",
      "Epoch 165 | Batch: 9 | Loss: 9.2152\n",
      "Epoch 165 | Batch: 10 | Loss: 4.4256\n",
      "Epoch 165 | Batch: 11 | Loss: 6.4802\n",
      "Epoch 165 | Batch: 12 | Loss: 17.3584\n",
      "Epoch 165 | Batch: 13 | Loss: 5.3092\n",
      "Epoch 165 | Batch: 14 | Loss: 7.7184\n",
      "Epoch 165 | Batch: 15 | Loss: 7.7762\n",
      "Epoch 165 | Batch: 16 | Loss: 6.9764\n",
      "Epoch 165 | Batch: 17 | Loss: 4.0378\n",
      "Epoch 165 | Batch: 18 | Loss: 6.9471\n",
      "Epoch 165 | Batch: 19 | Loss: 7.1362\n",
      "Epoch 165 | Batch: 20 | Loss: 4.2821\n",
      "Epoch 165 | Batch: 21 | Loss: 11.2833\n",
      "Epoch 165 | Batch: 22 | Loss: 7.8207\n",
      "Epoch 165 | Batch: 23 | Loss: 12.9089\n",
      "Epoch 165 | Batch: 24 | Loss: 6.5193\n",
      "Epoch 165 | Batch: 25 | Loss: 4.6957\n",
      "Epoch 165 | Batch: 26 | Loss: 6.1300\n",
      "Epoch 165 | Batch: 27 | Loss: 12.1862\n",
      "Epoch 165 | Batch: 28 | Loss: 19.8556\n",
      "Epoch 165 | Batch: 29 | Loss: 9.0538\n",
      "Epoch 165 | Batch: 30 | Loss: 6.0180\n",
      "Epoch 165 | Batch: 31 | Loss: 6.2898\n",
      "Epoch 165 | Batch: 32 | Loss: 3.5053\n",
      "Epoch 165 | Batch: 33 | Loss: 4.9877\n",
      "Epoch 165 | Batch: 34 | Loss: 5.3016\n",
      "Epoch 165 | Batch: 35 | Loss: 11.5143\n",
      "Epoch 165 | Batch: 36 | Loss: 5.9879\n",
      "Epoch 165 | Batch: 37 | Loss: 7.2061\n",
      "Epoch 165 | Batch: 38 | Loss: 6.2432\n",
      "Epoch 165 | Batch: 39 | Loss: 9.8995\n",
      "Epoch 165 | Batch: 40 | Loss: 10.4665\n",
      "Epoch 165 | Batch: 41 | Loss: 9.7324\n",
      "Epoch 165 | Batch: 42 | Loss: 4.5679\n",
      "Epoch 165 | Batch: 43 | Loss: 11.5145\n",
      "Epoch 165 | Batch: 44 | Loss: 8.9453\n",
      "Epoch 165 | Batch: 45 | Loss: 12.6539\n",
      "Epoch 165 | Batch: 46 | Loss: 5.6938\n",
      "Epoch 165 | Batch: 47 | Loss: 3.9240\n",
      "Epoch 165 | Batch: 48 | Loss: 1.6016\n",
      "Mean 8.118904404342175\n",
      "Epoch 166 | Batch: 1 | Loss: 8.0862\n",
      "Epoch 166 | Batch: 2 | Loss: 21.9747\n",
      "Epoch 166 | Batch: 3 | Loss: 29.2840\n",
      "Epoch 166 | Batch: 4 | Loss: 16.2149\n",
      "Epoch 166 | Batch: 5 | Loss: 8.2816\n",
      "Epoch 166 | Batch: 6 | Loss: 6.5576\n",
      "Epoch 166 | Batch: 7 | Loss: 7.2744\n",
      "Epoch 166 | Batch: 8 | Loss: 7.4250\n",
      "Epoch 166 | Batch: 9 | Loss: 11.4333\n",
      "Epoch 166 | Batch: 10 | Loss: 11.7115\n",
      "Epoch 166 | Batch: 11 | Loss: 8.2952\n",
      "Epoch 166 | Batch: 12 | Loss: 3.1285\n",
      "Epoch 166 | Batch: 13 | Loss: 4.3874\n",
      "Epoch 166 | Batch: 14 | Loss: 7.2327\n",
      "Epoch 166 | Batch: 15 | Loss: 2.3376\n",
      "Epoch 166 | Batch: 16 | Loss: 7.1165\n",
      "Epoch 166 | Batch: 17 | Loss: 1.8928\n",
      "Epoch 166 | Batch: 18 | Loss: 6.8346\n",
      "Epoch 166 | Batch: 19 | Loss: 12.5562\n",
      "Epoch 166 | Batch: 20 | Loss: 6.1461\n",
      "Epoch 166 | Batch: 21 | Loss: 12.4438\n",
      "Epoch 166 | Batch: 22 | Loss: 7.3554\n",
      "Epoch 166 | Batch: 23 | Loss: 11.4279\n",
      "Epoch 166 | Batch: 24 | Loss: 13.6042\n",
      "Epoch 166 | Batch: 25 | Loss: 9.2816\n",
      "Epoch 166 | Batch: 26 | Loss: 8.3559\n",
      "Epoch 166 | Batch: 27 | Loss: 7.3327\n",
      "Epoch 166 | Batch: 28 | Loss: 7.4457\n",
      "Epoch 166 | Batch: 29 | Loss: 9.8322\n",
      "Epoch 166 | Batch: 30 | Loss: 8.1524\n",
      "Epoch 166 | Batch: 31 | Loss: 10.7365\n",
      "Epoch 166 | Batch: 32 | Loss: 9.6544\n",
      "Epoch 166 | Batch: 33 | Loss: 7.1542\n",
      "Epoch 166 | Batch: 34 | Loss: 4.2496\n",
      "Epoch 166 | Batch: 35 | Loss: 8.1974\n",
      "Epoch 166 | Batch: 36 | Loss: 9.3557\n",
      "Epoch 166 | Batch: 37 | Loss: 5.6720\n",
      "Epoch 166 | Batch: 38 | Loss: 4.6051\n",
      "Epoch 166 | Batch: 39 | Loss: 11.6611\n",
      "Epoch 166 | Batch: 40 | Loss: 7.9522\n",
      "Epoch 166 | Batch: 41 | Loss: 8.2957\n",
      "Epoch 166 | Batch: 42 | Loss: 6.6627\n",
      "Epoch 166 | Batch: 43 | Loss: 10.2543\n",
      "Epoch 166 | Batch: 44 | Loss: 1.9615\n",
      "Epoch 166 | Batch: 45 | Loss: 4.6026\n",
      "Epoch 166 | Batch: 46 | Loss: 4.5468\n",
      "Epoch 166 | Batch: 47 | Loss: 12.2428\n",
      "Epoch 166 | Batch: 48 | Loss: 3.2496\n",
      "Mean 8.634518864254156\n",
      "Epoch 167 | Batch: 1 | Loss: 8.9192\n",
      "Epoch 167 | Batch: 2 | Loss: 8.1195\n",
      "Epoch 167 | Batch: 3 | Loss: 6.4361\n",
      "Epoch 167 | Batch: 4 | Loss: 10.5308\n",
      "Epoch 167 | Batch: 5 | Loss: 4.8469\n",
      "Epoch 167 | Batch: 6 | Loss: 8.5137\n",
      "Epoch 167 | Batch: 7 | Loss: 8.0781\n",
      "Epoch 167 | Batch: 8 | Loss: 5.2419\n",
      "Epoch 167 | Batch: 9 | Loss: 3.4549\n",
      "Epoch 167 | Batch: 10 | Loss: 6.3890\n",
      "Epoch 167 | Batch: 11 | Loss: 9.6524\n",
      "Epoch 167 | Batch: 12 | Loss: 4.8626\n",
      "Epoch 167 | Batch: 13 | Loss: 14.6082\n",
      "Epoch 167 | Batch: 14 | Loss: 12.9794\n",
      "Epoch 167 | Batch: 15 | Loss: 7.1277\n",
      "Epoch 167 | Batch: 16 | Loss: 12.7480\n",
      "Epoch 167 | Batch: 17 | Loss: 6.5192\n",
      "Epoch 167 | Batch: 18 | Loss: 4.5367\n",
      "Epoch 167 | Batch: 19 | Loss: 7.1277\n",
      "Epoch 167 | Batch: 20 | Loss: 8.4548\n",
      "Epoch 167 | Batch: 21 | Loss: 7.9231\n",
      "Epoch 167 | Batch: 22 | Loss: 4.9853\n",
      "Epoch 167 | Batch: 23 | Loss: 1.3609\n",
      "Epoch 167 | Batch: 24 | Loss: 15.5602\n",
      "Epoch 167 | Batch: 25 | Loss: 13.4912\n",
      "Epoch 167 | Batch: 26 | Loss: 12.9833\n",
      "Epoch 167 | Batch: 27 | Loss: 3.8258\n",
      "Epoch 167 | Batch: 28 | Loss: 9.9330\n",
      "Epoch 167 | Batch: 29 | Loss: 7.4250\n",
      "Epoch 167 | Batch: 30 | Loss: 7.6717\n",
      "Epoch 167 | Batch: 31 | Loss: 12.4287\n",
      "Epoch 167 | Batch: 32 | Loss: 9.6755\n",
      "Epoch 167 | Batch: 33 | Loss: 9.5006\n",
      "Epoch 167 | Batch: 34 | Loss: 4.9304\n",
      "Epoch 167 | Batch: 35 | Loss: 5.2363\n",
      "Epoch 167 | Batch: 36 | Loss: 6.5287\n",
      "Epoch 167 | Batch: 37 | Loss: 7.5168\n",
      "Epoch 167 | Batch: 38 | Loss: 7.1439\n",
      "Epoch 167 | Batch: 39 | Loss: 17.3476\n",
      "Epoch 167 | Batch: 40 | Loss: 7.3374\n",
      "Epoch 167 | Batch: 41 | Loss: 17.1137\n",
      "Epoch 167 | Batch: 42 | Loss: 9.2478\n",
      "Epoch 167 | Batch: 43 | Loss: 3.4132\n",
      "Epoch 167 | Batch: 44 | Loss: 4.0402\n",
      "Epoch 167 | Batch: 45 | Loss: 8.6201\n",
      "Epoch 167 | Batch: 46 | Loss: 7.8760\n",
      "Epoch 167 | Batch: 47 | Loss: 6.7369\n",
      "Epoch 167 | Batch: 48 | Loss: 3.3255\n",
      "Mean 8.173461755116781\n",
      "Epoch 168 | Batch: 1 | Loss: 5.6859\n",
      "Epoch 168 | Batch: 2 | Loss: 15.5623\n",
      "Epoch 168 | Batch: 3 | Loss: 3.8987\n",
      "Epoch 168 | Batch: 4 | Loss: 8.5148\n",
      "Epoch 168 | Batch: 5 | Loss: 19.0653\n",
      "Epoch 168 | Batch: 6 | Loss: 13.3263\n",
      "Epoch 168 | Batch: 7 | Loss: 9.1188\n",
      "Epoch 168 | Batch: 8 | Loss: 10.8556\n",
      "Epoch 168 | Batch: 9 | Loss: 7.0876\n",
      "Epoch 168 | Batch: 10 | Loss: 5.4757\n",
      "Epoch 168 | Batch: 11 | Loss: 7.4613\n",
      "Epoch 168 | Batch: 12 | Loss: 2.9068\n",
      "Epoch 168 | Batch: 13 | Loss: 4.0088\n",
      "Epoch 168 | Batch: 14 | Loss: 5.1437\n",
      "Epoch 168 | Batch: 15 | Loss: 7.7205\n",
      "Epoch 168 | Batch: 16 | Loss: 5.6074\n",
      "Epoch 168 | Batch: 17 | Loss: 6.8796\n",
      "Epoch 168 | Batch: 18 | Loss: 14.6849\n",
      "Epoch 168 | Batch: 19 | Loss: 5.5841\n",
      "Epoch 168 | Batch: 20 | Loss: 10.0056\n",
      "Epoch 168 | Batch: 21 | Loss: 9.8584\n",
      "Epoch 168 | Batch: 22 | Loss: 6.6268\n",
      "Epoch 168 | Batch: 23 | Loss: 8.9001\n",
      "Epoch 168 | Batch: 24 | Loss: 8.0352\n",
      "Epoch 168 | Batch: 25 | Loss: 13.4655\n",
      "Epoch 168 | Batch: 26 | Loss: 8.3384\n",
      "Epoch 168 | Batch: 27 | Loss: 6.8233\n",
      "Epoch 168 | Batch: 28 | Loss: 6.3706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168 | Batch: 29 | Loss: 6.2167\n",
      "Epoch 168 | Batch: 30 | Loss: 5.3883\n",
      "Epoch 168 | Batch: 31 | Loss: 13.7426\n",
      "Epoch 168 | Batch: 32 | Loss: 7.1248\n",
      "Epoch 168 | Batch: 33 | Loss: 9.6847\n",
      "Epoch 168 | Batch: 34 | Loss: 3.9424\n",
      "Epoch 168 | Batch: 35 | Loss: 10.1195\n",
      "Epoch 168 | Batch: 36 | Loss: 7.4107\n",
      "Epoch 168 | Batch: 37 | Loss: 4.7177\n",
      "Epoch 168 | Batch: 38 | Loss: 1.8697\n",
      "Epoch 168 | Batch: 39 | Loss: 3.2354\n",
      "Epoch 168 | Batch: 40 | Loss: 6.0532\n",
      "Epoch 168 | Batch: 41 | Loss: 14.4097\n",
      "Epoch 168 | Batch: 42 | Loss: 17.8512\n",
      "Epoch 168 | Batch: 43 | Loss: 6.4140\n",
      "Epoch 168 | Batch: 44 | Loss: 9.0493\n",
      "Epoch 168 | Batch: 45 | Loss: 6.6567\n",
      "Epoch 168 | Batch: 46 | Loss: 5.4326\n",
      "Epoch 168 | Batch: 47 | Loss: 4.8806\n",
      "Epoch 168 | Batch: 48 | Loss: 4.5822\n",
      "Mean 8.037376893063387\n",
      "Epoch 169 | Batch: 1 | Loss: 3.9440\n",
      "Epoch 169 | Batch: 2 | Loss: 10.2007\n",
      "Epoch 169 | Batch: 3 | Loss: 7.3928\n",
      "Epoch 169 | Batch: 4 | Loss: 18.1511\n",
      "Epoch 169 | Batch: 5 | Loss: 14.7226\n",
      "Epoch 169 | Batch: 6 | Loss: 8.2960\n",
      "Epoch 169 | Batch: 7 | Loss: 7.0688\n",
      "Epoch 169 | Batch: 8 | Loss: 5.0198\n",
      "Epoch 169 | Batch: 9 | Loss: 7.6600\n",
      "Epoch 169 | Batch: 10 | Loss: 11.8270\n",
      "Epoch 169 | Batch: 11 | Loss: 4.6555\n",
      "Epoch 169 | Batch: 12 | Loss: 11.8472\n",
      "Epoch 169 | Batch: 13 | Loss: 7.9137\n",
      "Epoch 169 | Batch: 14 | Loss: 11.3908\n",
      "Epoch 169 | Batch: 15 | Loss: 4.3677\n",
      "Epoch 169 | Batch: 16 | Loss: 9.4375\n",
      "Epoch 169 | Batch: 17 | Loss: 8.3146\n",
      "Epoch 169 | Batch: 18 | Loss: 4.0214\n",
      "Epoch 169 | Batch: 19 | Loss: 8.8667\n",
      "Epoch 169 | Batch: 20 | Loss: 3.6934\n",
      "Epoch 169 | Batch: 21 | Loss: 12.1873\n",
      "Epoch 169 | Batch: 22 | Loss: 18.6017\n",
      "Epoch 169 | Batch: 23 | Loss: 16.3199\n",
      "Epoch 169 | Batch: 24 | Loss: 8.7930\n",
      "Epoch 169 | Batch: 25 | Loss: 4.1153\n",
      "Epoch 169 | Batch: 26 | Loss: 8.6640\n",
      "Epoch 169 | Batch: 27 | Loss: 5.5031\n",
      "Epoch 169 | Batch: 28 | Loss: 3.7727\n",
      "Epoch 169 | Batch: 29 | Loss: 5.8852\n",
      "Epoch 169 | Batch: 30 | Loss: 4.4290\n",
      "Epoch 169 | Batch: 31 | Loss: 3.6801\n",
      "Epoch 169 | Batch: 32 | Loss: 10.8159\n",
      "Epoch 169 | Batch: 33 | Loss: 5.8909\n",
      "Epoch 169 | Batch: 34 | Loss: 8.7230\n",
      "Epoch 169 | Batch: 35 | Loss: 8.1346\n",
      "Epoch 169 | Batch: 36 | Loss: 7.8328\n",
      "Epoch 169 | Batch: 37 | Loss: 11.3342\n",
      "Epoch 169 | Batch: 38 | Loss: 10.8893\n",
      "Epoch 169 | Batch: 39 | Loss: 20.2931\n",
      "Epoch 169 | Batch: 40 | Loss: 19.3584\n",
      "Epoch 169 | Batch: 41 | Loss: 11.9631\n",
      "Epoch 169 | Batch: 42 | Loss: 7.0454\n",
      "Epoch 169 | Batch: 43 | Loss: 6.7657\n",
      "Epoch 169 | Batch: 44 | Loss: 6.8133\n",
      "Epoch 169 | Batch: 45 | Loss: 11.1738\n",
      "Epoch 169 | Batch: 46 | Loss: 9.3982\n",
      "Epoch 169 | Batch: 47 | Loss: 15.0260\n",
      "Epoch 169 | Batch: 48 | Loss: 2.8102\n",
      "Mean 9.06271534661452\n",
      "Epoch 170 | Batch: 1 | Loss: 5.3740\n",
      "Epoch 170 | Batch: 2 | Loss: 5.2392\n",
      "Epoch 170 | Batch: 3 | Loss: 5.1086\n",
      "Epoch 170 | Batch: 4 | Loss: 8.3115\n",
      "Epoch 170 | Batch: 5 | Loss: 7.9282\n",
      "Epoch 170 | Batch: 6 | Loss: 4.0866\n",
      "Epoch 170 | Batch: 7 | Loss: 7.1626\n",
      "Epoch 170 | Batch: 8 | Loss: 4.5082\n",
      "Epoch 170 | Batch: 9 | Loss: 10.2812\n",
      "Epoch 170 | Batch: 10 | Loss: 12.5153\n",
      "Epoch 170 | Batch: 11 | Loss: 12.8618\n",
      "Epoch 170 | Batch: 12 | Loss: 11.0493\n",
      "Epoch 170 | Batch: 13 | Loss: 5.2894\n",
      "Epoch 170 | Batch: 14 | Loss: 17.2932\n",
      "Epoch 170 | Batch: 15 | Loss: 9.7901\n",
      "Epoch 170 | Batch: 16 | Loss: 6.2171\n",
      "Epoch 170 | Batch: 17 | Loss: 9.9172\n",
      "Epoch 170 | Batch: 18 | Loss: 4.9902\n",
      "Epoch 170 | Batch: 19 | Loss: 4.7682\n",
      "Epoch 170 | Batch: 20 | Loss: 5.1859\n",
      "Epoch 170 | Batch: 21 | Loss: 9.8005\n",
      "Epoch 170 | Batch: 22 | Loss: 4.6154\n",
      "Epoch 170 | Batch: 23 | Loss: 9.1291\n",
      "Epoch 170 | Batch: 24 | Loss: 10.1472\n",
      "Epoch 170 | Batch: 25 | Loss: 7.8629\n",
      "Epoch 170 | Batch: 26 | Loss: 5.9541\n",
      "Epoch 170 | Batch: 27 | Loss: 7.1336\n",
      "Epoch 170 | Batch: 28 | Loss: 6.3039\n",
      "Epoch 170 | Batch: 29 | Loss: 3.2958\n",
      "Epoch 170 | Batch: 30 | Loss: 21.4303\n",
      "Epoch 170 | Batch: 31 | Loss: 52.1314\n",
      "Epoch 170 | Batch: 32 | Loss: 9.7439\n",
      "Epoch 170 | Batch: 33 | Loss: 9.9917\n",
      "Epoch 170 | Batch: 34 | Loss: 5.5814\n",
      "Epoch 170 | Batch: 35 | Loss: 5.3644\n",
      "Epoch 170 | Batch: 36 | Loss: 10.5234\n",
      "Epoch 170 | Batch: 37 | Loss: 7.8438\n",
      "Epoch 170 | Batch: 38 | Loss: 7.9757\n",
      "Epoch 170 | Batch: 39 | Loss: 5.8194\n",
      "Epoch 170 | Batch: 40 | Loss: 16.5021\n",
      "Epoch 170 | Batch: 41 | Loss: 19.4903\n",
      "Epoch 170 | Batch: 42 | Loss: 6.1572\n",
      "Epoch 170 | Batch: 43 | Loss: 4.8241\n",
      "Epoch 170 | Batch: 44 | Loss: 6.9613\n",
      "Epoch 170 | Batch: 45 | Loss: 2.4644\n",
      "Epoch 170 | Batch: 46 | Loss: 5.3811\n",
      "Epoch 170 | Batch: 47 | Loss: 6.2857\n",
      "Epoch 170 | Batch: 48 | Loss: 3.7082\n",
      "Mean 8.9645839681228\n",
      "Epoch 171 | Batch: 1 | Loss: 3.0129\n",
      "Epoch 171 | Batch: 2 | Loss: 15.0765\n",
      "Epoch 171 | Batch: 3 | Loss: 7.9939\n",
      "Epoch 171 | Batch: 4 | Loss: 9.2178\n",
      "Epoch 171 | Batch: 5 | Loss: 13.3720\n",
      "Epoch 171 | Batch: 6 | Loss: 16.2592\n",
      "Epoch 171 | Batch: 7 | Loss: 16.5762\n",
      "Epoch 171 | Batch: 8 | Loss: 5.9371\n",
      "Epoch 171 | Batch: 9 | Loss: 8.0332\n",
      "Epoch 171 | Batch: 10 | Loss: 5.9495\n",
      "Epoch 171 | Batch: 11 | Loss: 3.9545\n",
      "Epoch 171 | Batch: 12 | Loss: 6.3665\n",
      "Epoch 171 | Batch: 13 | Loss: 4.7269\n",
      "Epoch 171 | Batch: 14 | Loss: 6.2384\n",
      "Epoch 171 | Batch: 15 | Loss: 6.7270\n",
      "Epoch 171 | Batch: 16 | Loss: 3.1335\n",
      "Epoch 171 | Batch: 17 | Loss: 10.2579\n",
      "Epoch 171 | Batch: 18 | Loss: 7.0065\n",
      "Epoch 171 | Batch: 19 | Loss: 14.8396\n",
      "Epoch 171 | Batch: 20 | Loss: 15.0523\n",
      "Epoch 171 | Batch: 21 | Loss: 9.7920\n",
      "Epoch 171 | Batch: 22 | Loss: 20.5408\n",
      "Epoch 171 | Batch: 23 | Loss: 8.4656\n",
      "Epoch 171 | Batch: 24 | Loss: 7.9697\n",
      "Epoch 171 | Batch: 25 | Loss: 13.7329\n",
      "Epoch 171 | Batch: 26 | Loss: 5.3771\n",
      "Epoch 171 | Batch: 27 | Loss: 4.1698\n",
      "Epoch 171 | Batch: 28 | Loss: 7.5780\n",
      "Epoch 171 | Batch: 29 | Loss: 6.9324\n",
      "Epoch 171 | Batch: 30 | Loss: 13.0661\n",
      "Epoch 171 | Batch: 31 | Loss: 5.7804\n",
      "Epoch 171 | Batch: 32 | Loss: 11.8857\n",
      "Epoch 171 | Batch: 33 | Loss: 5.6379\n",
      "Epoch 171 | Batch: 34 | Loss: 8.0114\n",
      "Epoch 171 | Batch: 35 | Loss: 11.1839\n",
      "Epoch 171 | Batch: 36 | Loss: 3.9531\n",
      "Epoch 171 | Batch: 37 | Loss: 7.5021\n",
      "Epoch 171 | Batch: 38 | Loss: 10.5989\n",
      "Epoch 171 | Batch: 39 | Loss: 7.0819\n",
      "Epoch 171 | Batch: 40 | Loss: 10.5350\n",
      "Epoch 171 | Batch: 41 | Loss: 4.6450\n",
      "Epoch 171 | Batch: 42 | Loss: 8.9764\n",
      "Epoch 171 | Batch: 43 | Loss: 3.8503\n",
      "Epoch 171 | Batch: 44 | Loss: 20.9923\n",
      "Epoch 171 | Batch: 45 | Loss: 14.6651\n",
      "Epoch 171 | Batch: 46 | Loss: 7.8894\n",
      "Epoch 171 | Batch: 47 | Loss: 5.5768\n",
      "Epoch 171 | Batch: 48 | Loss: 2.8054\n",
      "Mean 8.935980831583342\n",
      "Epoch 172 | Batch: 1 | Loss: 6.0114\n",
      "Epoch 172 | Batch: 2 | Loss: 5.8530\n",
      "Epoch 172 | Batch: 3 | Loss: 16.2610\n",
      "Epoch 172 | Batch: 4 | Loss: 15.6550\n",
      "Epoch 172 | Batch: 5 | Loss: 8.6569\n",
      "Epoch 172 | Batch: 6 | Loss: 10.2193\n",
      "Epoch 172 | Batch: 7 | Loss: 6.9684\n",
      "Epoch 172 | Batch: 8 | Loss: 8.9810\n",
      "Epoch 172 | Batch: 9 | Loss: 4.3343\n",
      "Epoch 172 | Batch: 10 | Loss: 5.7176\n",
      "Epoch 172 | Batch: 11 | Loss: 3.0751\n",
      "Epoch 172 | Batch: 12 | Loss: 6.1499\n",
      "Epoch 172 | Batch: 13 | Loss: 4.9961\n",
      "Epoch 172 | Batch: 14 | Loss: 8.7482\n",
      "Epoch 172 | Batch: 15 | Loss: 12.1189\n",
      "Epoch 172 | Batch: 16 | Loss: 10.0974\n",
      "Epoch 172 | Batch: 17 | Loss: 2.1281\n",
      "Epoch 172 | Batch: 18 | Loss: 5.7859\n",
      "Epoch 172 | Batch: 19 | Loss: 18.0165\n",
      "Epoch 172 | Batch: 20 | Loss: 12.6680\n",
      "Epoch 172 | Batch: 21 | Loss: 7.3649\n",
      "Epoch 172 | Batch: 22 | Loss: 5.6107\n",
      "Epoch 172 | Batch: 23 | Loss: 12.0173\n",
      "Epoch 172 | Batch: 24 | Loss: 10.6760\n",
      "Epoch 172 | Batch: 25 | Loss: 11.4748\n",
      "Epoch 172 | Batch: 26 | Loss: 12.2712\n",
      "Epoch 172 | Batch: 27 | Loss: 6.2214\n",
      "Epoch 172 | Batch: 28 | Loss: 8.1876\n",
      "Epoch 172 | Batch: 29 | Loss: 16.9068\n",
      "Epoch 172 | Batch: 30 | Loss: 13.8317\n",
      "Epoch 172 | Batch: 31 | Loss: 12.4542\n",
      "Epoch 172 | Batch: 32 | Loss: 11.2848\n",
      "Epoch 172 | Batch: 33 | Loss: 1.7519\n",
      "Epoch 172 | Batch: 34 | Loss: 9.6227\n",
      "Epoch 172 | Batch: 35 | Loss: 5.7318\n",
      "Epoch 172 | Batch: 36 | Loss: 15.6859\n",
      "Epoch 172 | Batch: 37 | Loss: 10.2272\n",
      "Epoch 172 | Batch: 38 | Loss: 15.7502\n",
      "Epoch 172 | Batch: 39 | Loss: 8.8153\n",
      "Epoch 172 | Batch: 40 | Loss: 7.3482\n",
      "Epoch 172 | Batch: 41 | Loss: 8.3358\n",
      "Epoch 172 | Batch: 42 | Loss: 10.7500\n",
      "Epoch 172 | Batch: 43 | Loss: 6.5464\n",
      "Epoch 172 | Batch: 44 | Loss: 7.9073\n",
      "Epoch 172 | Batch: 45 | Loss: 12.0327\n",
      "Epoch 172 | Batch: 46 | Loss: 6.2866\n",
      "Epoch 172 | Batch: 47 | Loss: 8.7428\n",
      "Epoch 172 | Batch: 48 | Loss: 8.5041\n",
      "Mean 9.266306574145952\n",
      "Epoch 173 | Batch: 1 | Loss: 6.7746\n",
      "Epoch 173 | Batch: 2 | Loss: 5.5446\n",
      "Epoch 173 | Batch: 3 | Loss: 5.9230\n",
      "Epoch 173 | Batch: 4 | Loss: 8.9971\n",
      "Epoch 173 | Batch: 5 | Loss: 5.5136\n",
      "Epoch 173 | Batch: 6 | Loss: 5.6108\n",
      "Epoch 173 | Batch: 7 | Loss: 9.5178\n",
      "Epoch 173 | Batch: 8 | Loss: 8.9929\n",
      "Epoch 173 | Batch: 9 | Loss: 4.0831\n",
      "Epoch 173 | Batch: 10 | Loss: 10.6910\n",
      "Epoch 173 | Batch: 11 | Loss: 12.4325\n",
      "Epoch 173 | Batch: 12 | Loss: 2.8749\n",
      "Epoch 173 | Batch: 13 | Loss: 5.5861\n",
      "Epoch 173 | Batch: 14 | Loss: 4.4785\n",
      "Epoch 173 | Batch: 15 | Loss: 5.9459\n",
      "Epoch 173 | Batch: 16 | Loss: 4.7436\n",
      "Epoch 173 | Batch: 17 | Loss: 9.5744\n",
      "Epoch 173 | Batch: 18 | Loss: 6.4339\n",
      "Epoch 173 | Batch: 19 | Loss: 11.4414\n",
      "Epoch 173 | Batch: 20 | Loss: 19.1632\n",
      "Epoch 173 | Batch: 21 | Loss: 9.7848\n",
      "Epoch 173 | Batch: 22 | Loss: 13.9871\n",
      "Epoch 173 | Batch: 23 | Loss: 13.5179\n",
      "Epoch 173 | Batch: 24 | Loss: 1.0260\n",
      "Epoch 173 | Batch: 25 | Loss: 9.4635\n",
      "Epoch 173 | Batch: 26 | Loss: 5.5250\n",
      "Epoch 173 | Batch: 27 | Loss: 12.0188\n",
      "Epoch 173 | Batch: 28 | Loss: 10.6970\n",
      "Epoch 173 | Batch: 29 | Loss: 9.8277\n",
      "Epoch 173 | Batch: 30 | Loss: 8.2825\n",
      "Epoch 173 | Batch: 31 | Loss: 7.3729\n",
      "Epoch 173 | Batch: 32 | Loss: 9.0729\n",
      "Epoch 173 | Batch: 33 | Loss: 2.8575\n",
      "Epoch 173 | Batch: 34 | Loss: 11.0649\n",
      "Epoch 173 | Batch: 35 | Loss: 7.0043\n",
      "Epoch 173 | Batch: 36 | Loss: 4.3965\n",
      "Epoch 173 | Batch: 37 | Loss: 5.3636\n",
      "Epoch 173 | Batch: 38 | Loss: 6.4586\n",
      "Epoch 173 | Batch: 39 | Loss: 10.6475\n",
      "Epoch 173 | Batch: 40 | Loss: 10.7874\n",
      "Epoch 173 | Batch: 41 | Loss: 18.6448\n",
      "Epoch 173 | Batch: 42 | Loss: 16.5487\n",
      "Epoch 173 | Batch: 43 | Loss: 14.6884\n",
      "Epoch 173 | Batch: 44 | Loss: 14.1056\n",
      "Epoch 173 | Batch: 45 | Loss: 6.2847\n",
      "Epoch 173 | Batch: 46 | Loss: 10.7242\n",
      "Epoch 173 | Batch: 47 | Loss: 5.6737\n",
      "Epoch 173 | Batch: 48 | Loss: 5.7194\n",
      "Mean 8.663934096693993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174 | Batch: 1 | Loss: 12.4843\n",
      "Epoch 174 | Batch: 2 | Loss: 10.1892\n",
      "Epoch 174 | Batch: 3 | Loss: 10.6850\n",
      "Epoch 174 | Batch: 4 | Loss: 8.0512\n",
      "Epoch 174 | Batch: 5 | Loss: 18.3229\n",
      "Epoch 174 | Batch: 6 | Loss: 22.5561\n",
      "Epoch 174 | Batch: 7 | Loss: 27.0354\n",
      "Epoch 174 | Batch: 8 | Loss: 15.3680\n",
      "Epoch 174 | Batch: 9 | Loss: 5.3794\n",
      "Epoch 174 | Batch: 10 | Loss: 13.5183\n",
      "Epoch 174 | Batch: 11 | Loss: 10.9758\n",
      "Epoch 174 | Batch: 12 | Loss: 11.8956\n",
      "Epoch 174 | Batch: 13 | Loss: 6.2246\n",
      "Epoch 174 | Batch: 14 | Loss: 5.7320\n",
      "Epoch 174 | Batch: 15 | Loss: 5.2945\n",
      "Epoch 174 | Batch: 16 | Loss: 9.1078\n",
      "Epoch 174 | Batch: 17 | Loss: 6.1000\n",
      "Epoch 174 | Batch: 18 | Loss: 9.6003\n",
      "Epoch 174 | Batch: 19 | Loss: 13.4108\n",
      "Epoch 174 | Batch: 20 | Loss: 24.7095\n",
      "Epoch 174 | Batch: 21 | Loss: 12.5997\n",
      "Epoch 174 | Batch: 22 | Loss: 6.2286\n",
      "Epoch 174 | Batch: 23 | Loss: 18.4417\n",
      "Epoch 174 | Batch: 24 | Loss: 16.0938\n",
      "Epoch 174 | Batch: 25 | Loss: 15.7896\n",
      "Epoch 174 | Batch: 26 | Loss: 7.8473\n",
      "Epoch 174 | Batch: 27 | Loss: 10.4730\n",
      "Epoch 174 | Batch: 28 | Loss: 12.5423\n",
      "Epoch 174 | Batch: 29 | Loss: 15.9510\n",
      "Epoch 174 | Batch: 30 | Loss: 9.7929\n",
      "Epoch 174 | Batch: 31 | Loss: 2.8759\n",
      "Epoch 174 | Batch: 32 | Loss: 5.9023\n",
      "Epoch 174 | Batch: 33 | Loss: 10.3393\n",
      "Epoch 174 | Batch: 34 | Loss: 7.4844\n",
      "Epoch 174 | Batch: 35 | Loss: 8.8775\n",
      "Epoch 174 | Batch: 36 | Loss: 13.2003\n",
      "Epoch 174 | Batch: 37 | Loss: 17.1192\n",
      "Epoch 174 | Batch: 38 | Loss: 5.6280\n",
      "Epoch 174 | Batch: 39 | Loss: 9.8607\n",
      "Epoch 174 | Batch: 40 | Loss: 9.1706\n",
      "Epoch 174 | Batch: 41 | Loss: 5.4063\n",
      "Epoch 174 | Batch: 42 | Loss: 7.6311\n",
      "Epoch 174 | Batch: 43 | Loss: 11.8534\n",
      "Epoch 174 | Batch: 44 | Loss: 10.0206\n",
      "Epoch 174 | Batch: 45 | Loss: 7.5373\n",
      "Epoch 174 | Batch: 46 | Loss: 4.2583\n",
      "Epoch 174 | Batch: 47 | Loss: 8.0610\n",
      "Epoch 174 | Batch: 48 | Loss: 1.8425\n",
      "Mean 10.82227773219347\n",
      "Epoch 175 | Batch: 1 | Loss: 10.7813\n",
      "Epoch 175 | Batch: 2 | Loss: 7.2977\n",
      "Epoch 175 | Batch: 3 | Loss: 7.0882\n",
      "Epoch 175 | Batch: 4 | Loss: 11.6333\n",
      "Epoch 175 | Batch: 5 | Loss: 7.3048\n",
      "Epoch 175 | Batch: 6 | Loss: 8.5841\n",
      "Epoch 175 | Batch: 7 | Loss: 5.8141\n",
      "Epoch 175 | Batch: 8 | Loss: 8.2559\n",
      "Epoch 175 | Batch: 9 | Loss: 9.3883\n",
      "Epoch 175 | Batch: 10 | Loss: 4.4960\n",
      "Epoch 175 | Batch: 11 | Loss: 3.0542\n",
      "Epoch 175 | Batch: 12 | Loss: 15.4245\n",
      "Epoch 175 | Batch: 13 | Loss: 15.5574\n",
      "Epoch 175 | Batch: 14 | Loss: 12.2295\n",
      "Epoch 175 | Batch: 15 | Loss: 6.8210\n",
      "Epoch 175 | Batch: 16 | Loss: 8.7869\n",
      "Epoch 175 | Batch: 17 | Loss: 19.8531\n",
      "Epoch 175 | Batch: 18 | Loss: 9.4126\n",
      "Epoch 175 | Batch: 19 | Loss: 11.2274\n",
      "Epoch 175 | Batch: 20 | Loss: 8.9950\n",
      "Epoch 175 | Batch: 21 | Loss: 11.4452\n",
      "Epoch 175 | Batch: 22 | Loss: 10.5245\n",
      "Epoch 175 | Batch: 23 | Loss: 11.0329\n",
      "Epoch 175 | Batch: 24 | Loss: 6.7185\n",
      "Epoch 175 | Batch: 25 | Loss: 6.2764\n",
      "Epoch 175 | Batch: 26 | Loss: 3.6275\n",
      "Epoch 175 | Batch: 27 | Loss: 12.8030\n",
      "Epoch 175 | Batch: 28 | Loss: 7.8840\n",
      "Epoch 175 | Batch: 29 | Loss: 7.0123\n",
      "Epoch 175 | Batch: 30 | Loss: 5.7554\n",
      "Epoch 175 | Batch: 31 | Loss: 11.8375\n",
      "Epoch 175 | Batch: 32 | Loss: 5.8403\n",
      "Epoch 175 | Batch: 33 | Loss: 10.8833\n",
      "Epoch 175 | Batch: 34 | Loss: 7.6967\n",
      "Epoch 175 | Batch: 35 | Loss: 5.7248\n",
      "Epoch 175 | Batch: 36 | Loss: 3.6412\n",
      "Epoch 175 | Batch: 37 | Loss: 10.7761\n",
      "Epoch 175 | Batch: 38 | Loss: 8.4186\n",
      "Epoch 175 | Batch: 39 | Loss: 9.1254\n",
      "Epoch 175 | Batch: 40 | Loss: 7.6009\n",
      "Epoch 175 | Batch: 41 | Loss: 8.6118\n",
      "Epoch 175 | Batch: 42 | Loss: 5.4383\n",
      "Epoch 175 | Batch: 43 | Loss: 6.3006\n",
      "Epoch 175 | Batch: 44 | Loss: 6.4264\n",
      "Epoch 175 | Batch: 45 | Loss: 7.2382\n",
      "Epoch 175 | Batch: 46 | Loss: 4.3459\n",
      "Epoch 175 | Batch: 47 | Loss: 10.8891\n",
      "Epoch 175 | Batch: 48 | Loss: 1.5094\n",
      "Mean 8.487285435199738\n",
      "Epoch 176 | Batch: 1 | Loss: 6.5031\n",
      "Epoch 176 | Batch: 2 | Loss: 7.6448\n",
      "Epoch 176 | Batch: 3 | Loss: 4.4998\n",
      "Epoch 176 | Batch: 4 | Loss: 9.7351\n",
      "Epoch 176 | Batch: 5 | Loss: 4.9632\n",
      "Epoch 176 | Batch: 6 | Loss: 9.5775\n",
      "Epoch 176 | Batch: 7 | Loss: 5.7377\n",
      "Epoch 176 | Batch: 8 | Loss: 4.6221\n",
      "Epoch 176 | Batch: 9 | Loss: 6.9782\n",
      "Epoch 176 | Batch: 10 | Loss: 3.5167\n",
      "Epoch 176 | Batch: 11 | Loss: 9.0558\n",
      "Epoch 176 | Batch: 12 | Loss: 2.5789\n",
      "Epoch 176 | Batch: 13 | Loss: 4.4347\n",
      "Epoch 176 | Batch: 14 | Loss: 14.5039\n",
      "Epoch 176 | Batch: 15 | Loss: 19.3719\n",
      "Epoch 176 | Batch: 16 | Loss: 8.3815\n",
      "Epoch 176 | Batch: 17 | Loss: 10.5962\n",
      "Epoch 176 | Batch: 18 | Loss: 4.8648\n",
      "Epoch 176 | Batch: 19 | Loss: 9.6250\n",
      "Epoch 176 | Batch: 20 | Loss: 9.2013\n",
      "Epoch 176 | Batch: 21 | Loss: 13.6784\n",
      "Epoch 176 | Batch: 22 | Loss: 12.9034\n",
      "Epoch 176 | Batch: 23 | Loss: 8.5809\n",
      "Epoch 176 | Batch: 24 | Loss: 5.9776\n",
      "Epoch 176 | Batch: 25 | Loss: 3.7391\n",
      "Epoch 176 | Batch: 26 | Loss: 12.0205\n",
      "Epoch 176 | Batch: 27 | Loss: 5.8041\n",
      "Epoch 176 | Batch: 28 | Loss: 12.3569\n",
      "Epoch 176 | Batch: 29 | Loss: 2.9069\n",
      "Epoch 176 | Batch: 30 | Loss: 9.3499\n",
      "Epoch 176 | Batch: 31 | Loss: 11.9775\n",
      "Epoch 176 | Batch: 32 | Loss: 9.3840\n",
      "Epoch 176 | Batch: 33 | Loss: 4.7459\n",
      "Epoch 176 | Batch: 34 | Loss: 5.7440\n",
      "Epoch 176 | Batch: 35 | Loss: 7.1441\n",
      "Epoch 176 | Batch: 36 | Loss: 5.4619\n",
      "Epoch 176 | Batch: 37 | Loss: 6.1200\n",
      "Epoch 176 | Batch: 38 | Loss: 10.0752\n",
      "Epoch 176 | Batch: 39 | Loss: 23.0524\n",
      "Epoch 176 | Batch: 40 | Loss: 24.5973\n",
      "Epoch 176 | Batch: 41 | Loss: 10.8626\n",
      "Epoch 176 | Batch: 42 | Loss: 7.2531\n",
      "Epoch 176 | Batch: 43 | Loss: 11.4414\n",
      "Epoch 176 | Batch: 44 | Loss: 5.6457\n",
      "Epoch 176 | Batch: 45 | Loss: 7.3624\n",
      "Epoch 176 | Batch: 46 | Loss: 9.7137\n",
      "Epoch 176 | Batch: 47 | Loss: 6.9540\n",
      "Epoch 176 | Batch: 48 | Loss: 4.3094\n",
      "Mean 8.6573855727911\n",
      "Epoch 177 | Batch: 1 | Loss: 3.7561\n",
      "Epoch 177 | Batch: 2 | Loss: 9.2132\n",
      "Epoch 177 | Batch: 3 | Loss: 19.5340\n",
      "Epoch 177 | Batch: 4 | Loss: 13.0744\n",
      "Epoch 177 | Batch: 5 | Loss: 6.1594\n",
      "Epoch 177 | Batch: 6 | Loss: 8.5757\n",
      "Epoch 177 | Batch: 7 | Loss: 5.4423\n",
      "Epoch 177 | Batch: 8 | Loss: 4.8607\n",
      "Epoch 177 | Batch: 9 | Loss: 6.6197\n",
      "Epoch 177 | Batch: 10 | Loss: 4.0745\n",
      "Epoch 177 | Batch: 11 | Loss: 9.5181\n",
      "Epoch 177 | Batch: 12 | Loss: 6.5228\n",
      "Epoch 177 | Batch: 13 | Loss: 9.1539\n",
      "Epoch 177 | Batch: 14 | Loss: 4.4012\n",
      "Epoch 177 | Batch: 15 | Loss: 7.2853\n",
      "Epoch 177 | Batch: 16 | Loss: 17.4319\n",
      "Epoch 177 | Batch: 17 | Loss: 19.1814\n",
      "Epoch 177 | Batch: 18 | Loss: 7.7461\n",
      "Epoch 177 | Batch: 19 | Loss: 5.5487\n",
      "Epoch 177 | Batch: 20 | Loss: 6.4290\n",
      "Epoch 177 | Batch: 21 | Loss: 4.1262\n",
      "Epoch 177 | Batch: 22 | Loss: 9.5018\n",
      "Epoch 177 | Batch: 23 | Loss: 6.1723\n",
      "Epoch 177 | Batch: 24 | Loss: 5.5061\n",
      "Epoch 177 | Batch: 25 | Loss: 4.5923\n",
      "Epoch 177 | Batch: 26 | Loss: 7.3947\n",
      "Epoch 177 | Batch: 27 | Loss: 5.9662\n",
      "Epoch 177 | Batch: 28 | Loss: 11.5320\n",
      "Epoch 177 | Batch: 29 | Loss: 5.6468\n",
      "Epoch 177 | Batch: 30 | Loss: 7.0523\n",
      "Epoch 177 | Batch: 31 | Loss: 5.7975\n",
      "Epoch 177 | Batch: 32 | Loss: 16.2507\n",
      "Epoch 177 | Batch: 33 | Loss: 13.4952\n",
      "Epoch 177 | Batch: 34 | Loss: 13.6351\n",
      "Epoch 177 | Batch: 35 | Loss: 3.0295\n",
      "Epoch 177 | Batch: 36 | Loss: 12.7104\n",
      "Epoch 177 | Batch: 37 | Loss: 7.7833\n",
      "Epoch 177 | Batch: 38 | Loss: 10.9695\n",
      "Epoch 177 | Batch: 39 | Loss: 10.4073\n",
      "Epoch 177 | Batch: 40 | Loss: 7.7205\n",
      "Epoch 177 | Batch: 41 | Loss: 4.6447\n",
      "Epoch 177 | Batch: 42 | Loss: 6.6075\n",
      "Epoch 177 | Batch: 43 | Loss: 17.4391\n",
      "Epoch 177 | Batch: 44 | Loss: 5.3840\n",
      "Epoch 177 | Batch: 45 | Loss: 11.7803\n",
      "Epoch 177 | Batch: 46 | Loss: 15.2081\n",
      "Epoch 177 | Batch: 47 | Loss: 3.5407\n",
      "Epoch 177 | Batch: 48 | Loss: 8.7794\n",
      "Mean 8.691705425580343\n",
      "Epoch 178 | Batch: 1 | Loss: 8.6795\n",
      "Epoch 178 | Batch: 2 | Loss: 10.0381\n",
      "Epoch 178 | Batch: 3 | Loss: 8.3873\n",
      "Epoch 178 | Batch: 4 | Loss: 8.5514\n",
      "Epoch 178 | Batch: 5 | Loss: 2.7632\n",
      "Epoch 178 | Batch: 6 | Loss: 6.4148\n",
      "Epoch 178 | Batch: 7 | Loss: 5.0545\n",
      "Epoch 178 | Batch: 8 | Loss: 10.4832\n",
      "Epoch 178 | Batch: 9 | Loss: 9.7940\n",
      "Epoch 178 | Batch: 10 | Loss: 5.5737\n",
      "Epoch 178 | Batch: 11 | Loss: 3.4781\n",
      "Epoch 178 | Batch: 12 | Loss: 11.0771\n",
      "Epoch 178 | Batch: 13 | Loss: 16.7885\n",
      "Epoch 178 | Batch: 14 | Loss: 8.5305\n",
      "Epoch 178 | Batch: 15 | Loss: 7.4874\n",
      "Epoch 178 | Batch: 16 | Loss: 10.6930\n",
      "Epoch 178 | Batch: 17 | Loss: 5.7255\n",
      "Epoch 178 | Batch: 18 | Loss: 13.2626\n",
      "Epoch 178 | Batch: 19 | Loss: 8.5838\n",
      "Epoch 178 | Batch: 20 | Loss: 7.0723\n",
      "Epoch 178 | Batch: 21 | Loss: 5.6179\n",
      "Epoch 178 | Batch: 22 | Loss: 6.3085\n",
      "Epoch 178 | Batch: 23 | Loss: 9.0333\n",
      "Epoch 178 | Batch: 24 | Loss: 9.7051\n",
      "Epoch 178 | Batch: 25 | Loss: 7.2515\n",
      "Epoch 178 | Batch: 26 | Loss: 9.5481\n",
      "Epoch 178 | Batch: 27 | Loss: 17.1258\n",
      "Epoch 178 | Batch: 28 | Loss: 5.3299\n",
      "Epoch 178 | Batch: 29 | Loss: 7.5554\n",
      "Epoch 178 | Batch: 30 | Loss: 12.7700\n",
      "Epoch 178 | Batch: 31 | Loss: 5.6077\n",
      "Epoch 178 | Batch: 32 | Loss: 2.5793\n",
      "Epoch 178 | Batch: 33 | Loss: 6.5124\n",
      "Epoch 178 | Batch: 34 | Loss: 29.1607\n",
      "Epoch 178 | Batch: 35 | Loss: 15.7696\n",
      "Epoch 178 | Batch: 36 | Loss: 14.0247\n",
      "Epoch 178 | Batch: 37 | Loss: 13.1387\n",
      "Epoch 178 | Batch: 38 | Loss: 7.3613\n",
      "Epoch 178 | Batch: 39 | Loss: 8.0735\n",
      "Epoch 178 | Batch: 40 | Loss: 7.3804\n",
      "Epoch 178 | Batch: 41 | Loss: 7.5054\n",
      "Epoch 178 | Batch: 42 | Loss: 8.2589\n",
      "Epoch 178 | Batch: 43 | Loss: 2.6516\n",
      "Epoch 178 | Batch: 44 | Loss: 8.5362\n",
      "Epoch 178 | Batch: 45 | Loss: 3.7580\n",
      "Epoch 178 | Batch: 46 | Loss: 4.9330\n",
      "Epoch 178 | Batch: 47 | Loss: 4.3800\n",
      "Epoch 178 | Batch: 48 | Loss: 6.0271\n",
      "Mean 8.632138018806776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179 | Batch: 1 | Loss: 7.5186\n",
      "Epoch 179 | Batch: 2 | Loss: 4.5457\n",
      "Epoch 179 | Batch: 3 | Loss: 3.2373\n",
      "Epoch 179 | Batch: 4 | Loss: 10.5212\n",
      "Epoch 179 | Batch: 5 | Loss: 5.4758\n",
      "Epoch 179 | Batch: 6 | Loss: 14.6122\n",
      "Epoch 179 | Batch: 7 | Loss: 11.9798\n",
      "Epoch 179 | Batch: 8 | Loss: 14.6282\n",
      "Epoch 179 | Batch: 9 | Loss: 6.3080\n",
      "Epoch 179 | Batch: 10 | Loss: 5.6667\n",
      "Epoch 179 | Batch: 11 | Loss: 9.5941\n",
      "Epoch 179 | Batch: 12 | Loss: 9.4950\n",
      "Epoch 179 | Batch: 13 | Loss: 8.9385\n",
      "Epoch 179 | Batch: 14 | Loss: 4.7429\n",
      "Epoch 179 | Batch: 15 | Loss: 7.9098\n",
      "Epoch 179 | Batch: 16 | Loss: 5.3408\n",
      "Epoch 179 | Batch: 17 | Loss: 6.3127\n",
      "Epoch 179 | Batch: 18 | Loss: 6.1515\n",
      "Epoch 179 | Batch: 19 | Loss: 7.3220\n",
      "Epoch 179 | Batch: 20 | Loss: 11.5330\n",
      "Epoch 179 | Batch: 21 | Loss: 9.1973\n",
      "Epoch 179 | Batch: 22 | Loss: 8.6942\n",
      "Epoch 179 | Batch: 23 | Loss: 10.5458\n",
      "Epoch 179 | Batch: 24 | Loss: 4.1114\n",
      "Epoch 179 | Batch: 25 | Loss: 7.9410\n",
      "Epoch 179 | Batch: 26 | Loss: 6.1427\n",
      "Epoch 179 | Batch: 27 | Loss: 10.8684\n",
      "Epoch 179 | Batch: 28 | Loss: 9.5902\n",
      "Epoch 179 | Batch: 29 | Loss: 5.8336\n",
      "Epoch 179 | Batch: 30 | Loss: 6.4692\n",
      "Epoch 179 | Batch: 31 | Loss: 7.2773\n",
      "Epoch 179 | Batch: 32 | Loss: 5.5032\n",
      "Epoch 179 | Batch: 33 | Loss: 6.1918\n",
      "Epoch 179 | Batch: 34 | Loss: 6.7260\n",
      "Epoch 179 | Batch: 35 | Loss: 1.8893\n",
      "Epoch 179 | Batch: 36 | Loss: 5.6257\n",
      "Epoch 179 | Batch: 37 | Loss: 5.4541\n",
      "Epoch 179 | Batch: 38 | Loss: 17.3431\n",
      "Epoch 179 | Batch: 39 | Loss: 18.0024\n",
      "Epoch 179 | Batch: 40 | Loss: 23.8452\n",
      "Epoch 179 | Batch: 41 | Loss: 6.1074\n",
      "Epoch 179 | Batch: 42 | Loss: 2.5031\n",
      "Epoch 179 | Batch: 43 | Loss: 3.3027\n",
      "Epoch 179 | Batch: 44 | Loss: 7.9705\n",
      "Epoch 179 | Batch: 45 | Loss: 14.5773\n",
      "Epoch 179 | Batch: 46 | Loss: 6.8151\n",
      "Epoch 179 | Batch: 47 | Loss: 16.2055\n",
      "Epoch 179 | Batch: 48 | Loss: 7.1393\n",
      "Mean 8.410556341211\n",
      "Epoch 180 | Batch: 1 | Loss: 4.6796\n",
      "Epoch 180 | Batch: 2 | Loss: 10.8125\n",
      "Epoch 180 | Batch: 3 | Loss: 6.9317\n",
      "Epoch 180 | Batch: 4 | Loss: 4.8375\n",
      "Epoch 180 | Batch: 5 | Loss: 5.6105\n",
      "Epoch 180 | Batch: 6 | Loss: 8.9854\n",
      "Epoch 180 | Batch: 7 | Loss: 11.6014\n",
      "Epoch 180 | Batch: 8 | Loss: 8.4489\n",
      "Epoch 180 | Batch: 9 | Loss: 5.8057\n",
      "Epoch 180 | Batch: 10 | Loss: 2.5350\n",
      "Epoch 180 | Batch: 11 | Loss: 4.9899\n",
      "Epoch 180 | Batch: 12 | Loss: 11.6815\n",
      "Epoch 180 | Batch: 13 | Loss: 9.6182\n",
      "Epoch 180 | Batch: 14 | Loss: 6.4503\n",
      "Epoch 180 | Batch: 15 | Loss: 5.9677\n",
      "Epoch 180 | Batch: 16 | Loss: 6.5068\n",
      "Epoch 180 | Batch: 17 | Loss: 8.0309\n",
      "Epoch 180 | Batch: 18 | Loss: 9.3730\n",
      "Epoch 180 | Batch: 19 | Loss: 9.1334\n",
      "Epoch 180 | Batch: 20 | Loss: 9.0937\n",
      "Epoch 180 | Batch: 21 | Loss: 9.8429\n",
      "Epoch 180 | Batch: 22 | Loss: 7.9552\n",
      "Epoch 180 | Batch: 23 | Loss: 11.5018\n",
      "Epoch 180 | Batch: 24 | Loss: 4.5976\n",
      "Epoch 180 | Batch: 25 | Loss: 9.5919\n",
      "Epoch 180 | Batch: 26 | Loss: 5.8754\n",
      "Epoch 180 | Batch: 27 | Loss: 16.3681\n",
      "Epoch 180 | Batch: 28 | Loss: 10.4650\n",
      "Epoch 180 | Batch: 29 | Loss: 4.2839\n",
      "Epoch 180 | Batch: 30 | Loss: 8.1101\n",
      "Epoch 180 | Batch: 31 | Loss: 7.6328\n",
      "Epoch 180 | Batch: 32 | Loss: 3.6349\n",
      "Epoch 180 | Batch: 33 | Loss: 7.6446\n",
      "Epoch 180 | Batch: 34 | Loss: 7.5359\n",
      "Epoch 180 | Batch: 35 | Loss: 8.6414\n",
      "Epoch 180 | Batch: 36 | Loss: 10.3681\n",
      "Epoch 180 | Batch: 37 | Loss: 3.5660\n",
      "Epoch 180 | Batch: 38 | Loss: 7.1271\n",
      "Epoch 180 | Batch: 39 | Loss: 4.4875\n",
      "Epoch 180 | Batch: 40 | Loss: 14.8013\n",
      "Epoch 180 | Batch: 41 | Loss: 12.8439\n",
      "Epoch 180 | Batch: 42 | Loss: 10.8051\n",
      "Epoch 180 | Batch: 43 | Loss: 15.5800\n",
      "Epoch 180 | Batch: 44 | Loss: 5.5414\n",
      "Epoch 180 | Batch: 45 | Loss: 4.4829\n",
      "Epoch 180 | Batch: 46 | Loss: 10.8642\n",
      "Epoch 180 | Batch: 47 | Loss: 6.7181\n",
      "Epoch 180 | Batch: 48 | Loss: 4.5766\n",
      "Mean 8.052861973643303\n",
      "Epoch 181 | Batch: 1 | Loss: 2.2562\n",
      "Epoch 181 | Batch: 2 | Loss: 3.3936\n",
      "Epoch 181 | Batch: 3 | Loss: 4.0329\n",
      "Epoch 181 | Batch: 4 | Loss: 8.7642\n",
      "Epoch 181 | Batch: 5 | Loss: 8.6842\n",
      "Epoch 181 | Batch: 6 | Loss: 7.8005\n",
      "Epoch 181 | Batch: 7 | Loss: 9.4411\n",
      "Epoch 181 | Batch: 8 | Loss: 8.8585\n",
      "Epoch 181 | Batch: 9 | Loss: 3.1009\n",
      "Epoch 181 | Batch: 10 | Loss: 8.7478\n",
      "Epoch 181 | Batch: 11 | Loss: 10.8172\n",
      "Epoch 181 | Batch: 12 | Loss: 3.0884\n",
      "Epoch 181 | Batch: 13 | Loss: 12.4886\n",
      "Epoch 181 | Batch: 14 | Loss: 7.1377\n",
      "Epoch 181 | Batch: 15 | Loss: 5.7079\n",
      "Epoch 181 | Batch: 16 | Loss: 10.9974\n",
      "Epoch 181 | Batch: 17 | Loss: 4.1018\n",
      "Epoch 181 | Batch: 18 | Loss: 11.8133\n",
      "Epoch 181 | Batch: 19 | Loss: 7.4620\n",
      "Epoch 181 | Batch: 20 | Loss: 10.0297\n",
      "Epoch 181 | Batch: 21 | Loss: 12.0369\n",
      "Epoch 181 | Batch: 22 | Loss: 14.3871\n",
      "Epoch 181 | Batch: 23 | Loss: 6.3510\n",
      "Epoch 181 | Batch: 24 | Loss: 8.1597\n",
      "Epoch 181 | Batch: 25 | Loss: 8.0533\n",
      "Epoch 181 | Batch: 26 | Loss: 9.0772\n",
      "Epoch 181 | Batch: 27 | Loss: 13.6296\n",
      "Epoch 181 | Batch: 28 | Loss: 11.5636\n",
      "Epoch 181 | Batch: 29 | Loss: 11.4854\n",
      "Epoch 181 | Batch: 30 | Loss: 8.2793\n",
      "Epoch 181 | Batch: 31 | Loss: 6.5681\n",
      "Epoch 181 | Batch: 32 | Loss: 10.6059\n",
      "Epoch 181 | Batch: 33 | Loss: 5.9001\n",
      "Epoch 181 | Batch: 34 | Loss: 14.1295\n",
      "Epoch 181 | Batch: 35 | Loss: 14.1382\n",
      "Epoch 181 | Batch: 36 | Loss: 17.6170\n",
      "Epoch 181 | Batch: 37 | Loss: 11.8957\n",
      "Epoch 181 | Batch: 38 | Loss: 6.9572\n",
      "Epoch 181 | Batch: 39 | Loss: 13.2544\n",
      "Epoch 181 | Batch: 40 | Loss: 19.7522\n",
      "Epoch 181 | Batch: 41 | Loss: 13.5659\n",
      "Epoch 181 | Batch: 42 | Loss: 2.7661\n",
      "Epoch 181 | Batch: 43 | Loss: 12.7597\n",
      "Epoch 181 | Batch: 44 | Loss: 11.9810\n",
      "Epoch 181 | Batch: 45 | Loss: 22.5834\n",
      "Epoch 181 | Batch: 46 | Loss: 8.8988\n",
      "Epoch 181 | Batch: 47 | Loss: 5.8137\n",
      "Epoch 181 | Batch: 48 | Loss: 1.2696\n",
      "Mean 9.420902960002422\n",
      "Epoch 182 | Batch: 1 | Loss: 16.3900\n",
      "Epoch 182 | Batch: 2 | Loss: 12.7906\n",
      "Epoch 182 | Batch: 3 | Loss: 6.1841\n",
      "Epoch 182 | Batch: 4 | Loss: 5.4182\n",
      "Epoch 182 | Batch: 5 | Loss: 5.0277\n",
      "Epoch 182 | Batch: 6 | Loss: 14.1099\n",
      "Epoch 182 | Batch: 7 | Loss: 13.5797\n",
      "Epoch 182 | Batch: 8 | Loss: 12.9768\n",
      "Epoch 182 | Batch: 9 | Loss: 8.8960\n",
      "Epoch 182 | Batch: 10 | Loss: 12.1046\n",
      "Epoch 182 | Batch: 11 | Loss: 11.6494\n",
      "Epoch 182 | Batch: 12 | Loss: 4.1548\n",
      "Epoch 182 | Batch: 13 | Loss: 10.5115\n",
      "Epoch 182 | Batch: 14 | Loss: 14.6392\n",
      "Epoch 182 | Batch: 15 | Loss: 6.1378\n",
      "Epoch 182 | Batch: 16 | Loss: 9.8330\n",
      "Epoch 182 | Batch: 17 | Loss: 6.5353\n",
      "Epoch 182 | Batch: 18 | Loss: 10.0000\n",
      "Epoch 182 | Batch: 19 | Loss: 8.1797\n",
      "Epoch 182 | Batch: 20 | Loss: 7.1931\n",
      "Epoch 182 | Batch: 21 | Loss: 6.4987\n",
      "Epoch 182 | Batch: 22 | Loss: 4.3388\n",
      "Epoch 182 | Batch: 23 | Loss: 8.8578\n",
      "Epoch 182 | Batch: 24 | Loss: 6.7269\n",
      "Epoch 182 | Batch: 25 | Loss: 11.0194\n",
      "Epoch 182 | Batch: 26 | Loss: 10.0578\n",
      "Epoch 182 | Batch: 27 | Loss: 10.7924\n",
      "Epoch 182 | Batch: 28 | Loss: 9.1564\n",
      "Epoch 182 | Batch: 29 | Loss: 7.6677\n",
      "Epoch 182 | Batch: 30 | Loss: 9.6063\n",
      "Epoch 182 | Batch: 31 | Loss: 4.8485\n",
      "Epoch 182 | Batch: 32 | Loss: 11.3498\n",
      "Epoch 182 | Batch: 33 | Loss: 12.8279\n",
      "Epoch 182 | Batch: 34 | Loss: 9.6427\n",
      "Epoch 182 | Batch: 35 | Loss: 4.7322\n",
      "Epoch 182 | Batch: 36 | Loss: 13.1794\n",
      "Epoch 182 | Batch: 37 | Loss: 16.1842\n",
      "Epoch 182 | Batch: 38 | Loss: 9.8083\n",
      "Epoch 182 | Batch: 39 | Loss: 8.6917\n",
      "Epoch 182 | Batch: 40 | Loss: 6.1790\n",
      "Epoch 182 | Batch: 41 | Loss: 5.2883\n",
      "Epoch 182 | Batch: 42 | Loss: 9.0706\n",
      "Epoch 182 | Batch: 43 | Loss: 4.7917\n",
      "Epoch 182 | Batch: 44 | Loss: 4.9781\n",
      "Epoch 182 | Batch: 45 | Loss: 5.0670\n",
      "Epoch 182 | Batch: 46 | Loss: 5.3679\n",
      "Epoch 182 | Batch: 47 | Loss: 9.9801\n",
      "Epoch 182 | Batch: 48 | Loss: 4.7615\n",
      "Mean 8.912129918734232\n",
      "Epoch 183 | Batch: 1 | Loss: 3.1236\n",
      "Epoch 183 | Batch: 2 | Loss: 4.8140\n",
      "Epoch 183 | Batch: 3 | Loss: 9.1124\n",
      "Epoch 183 | Batch: 4 | Loss: 2.6546\n",
      "Epoch 183 | Batch: 5 | Loss: 4.7685\n",
      "Epoch 183 | Batch: 6 | Loss: 5.6590\n",
      "Epoch 183 | Batch: 7 | Loss: 15.5867\n",
      "Epoch 183 | Batch: 8 | Loss: 28.0825\n",
      "Epoch 183 | Batch: 9 | Loss: 4.8770\n",
      "Epoch 183 | Batch: 10 | Loss: 5.7295\n",
      "Epoch 183 | Batch: 11 | Loss: 16.0297\n",
      "Epoch 183 | Batch: 12 | Loss: 11.9050\n",
      "Epoch 183 | Batch: 13 | Loss: 21.5701\n",
      "Epoch 183 | Batch: 14 | Loss: 10.3374\n",
      "Epoch 183 | Batch: 15 | Loss: 7.0829\n",
      "Epoch 183 | Batch: 16 | Loss: 10.0469\n",
      "Epoch 183 | Batch: 17 | Loss: 8.5647\n",
      "Epoch 183 | Batch: 18 | Loss: 4.7907\n",
      "Epoch 183 | Batch: 19 | Loss: 3.6550\n",
      "Epoch 183 | Batch: 20 | Loss: 13.5885\n",
      "Epoch 183 | Batch: 21 | Loss: 8.6863\n",
      "Epoch 183 | Batch: 22 | Loss: 8.8074\n",
      "Epoch 183 | Batch: 23 | Loss: 7.1735\n",
      "Epoch 183 | Batch: 24 | Loss: 4.5607\n",
      "Epoch 183 | Batch: 25 | Loss: 3.5573\n",
      "Epoch 183 | Batch: 26 | Loss: 8.1918\n",
      "Epoch 183 | Batch: 27 | Loss: 3.8263\n",
      "Epoch 183 | Batch: 28 | Loss: 4.8929\n",
      "Epoch 183 | Batch: 29 | Loss: 9.1805\n",
      "Epoch 183 | Batch: 30 | Loss: 9.3523\n",
      "Epoch 183 | Batch: 31 | Loss: 5.7377\n",
      "Epoch 183 | Batch: 32 | Loss: 7.7037\n",
      "Epoch 183 | Batch: 33 | Loss: 8.3631\n",
      "Epoch 183 | Batch: 34 | Loss: 3.4226\n",
      "Epoch 183 | Batch: 35 | Loss: 5.6606\n",
      "Epoch 183 | Batch: 36 | Loss: 7.2544\n",
      "Epoch 183 | Batch: 37 | Loss: 6.4842\n",
      "Epoch 183 | Batch: 38 | Loss: 9.8823\n",
      "Epoch 183 | Batch: 39 | Loss: 4.6140\n",
      "Epoch 183 | Batch: 40 | Loss: 8.4195\n",
      "Epoch 183 | Batch: 41 | Loss: 8.2587\n",
      "Epoch 183 | Batch: 42 | Loss: 9.4245\n",
      "Epoch 183 | Batch: 43 | Loss: 5.3182\n",
      "Epoch 183 | Batch: 44 | Loss: 8.7095\n",
      "Epoch 183 | Batch: 45 | Loss: 6.8389\n",
      "Epoch 183 | Batch: 46 | Loss: 5.8469\n",
      "Epoch 183 | Batch: 47 | Loss: 8.3099\n",
      "Epoch 183 | Batch: 48 | Loss: 2.0916\n",
      "Mean 7.969751283526421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184 | Batch: 1 | Loss: 9.6932\n",
      "Epoch 184 | Batch: 2 | Loss: 10.4881\n",
      "Epoch 184 | Batch: 3 | Loss: 11.5545\n",
      "Epoch 184 | Batch: 4 | Loss: 7.8730\n",
      "Epoch 184 | Batch: 5 | Loss: 2.6755\n",
      "Epoch 184 | Batch: 6 | Loss: 2.1662\n",
      "Epoch 184 | Batch: 7 | Loss: 10.6206\n",
      "Epoch 184 | Batch: 8 | Loss: 6.9326\n",
      "Epoch 184 | Batch: 9 | Loss: 8.5917\n",
      "Epoch 184 | Batch: 10 | Loss: 12.9557\n",
      "Epoch 184 | Batch: 11 | Loss: 9.7530\n",
      "Epoch 184 | Batch: 12 | Loss: 2.5871\n",
      "Epoch 184 | Batch: 13 | Loss: 7.7558\n",
      "Epoch 184 | Batch: 14 | Loss: 10.2166\n",
      "Epoch 184 | Batch: 15 | Loss: 6.5391\n",
      "Epoch 184 | Batch: 16 | Loss: 10.7676\n",
      "Epoch 184 | Batch: 17 | Loss: 14.6115\n",
      "Epoch 184 | Batch: 18 | Loss: 9.3304\n",
      "Epoch 184 | Batch: 19 | Loss: 6.1559\n",
      "Epoch 184 | Batch: 20 | Loss: 8.4232\n",
      "Epoch 184 | Batch: 21 | Loss: 5.3204\n",
      "Epoch 184 | Batch: 22 | Loss: 9.8231\n",
      "Epoch 184 | Batch: 23 | Loss: 4.0679\n",
      "Epoch 184 | Batch: 24 | Loss: 3.1684\n",
      "Epoch 184 | Batch: 25 | Loss: 1.6226\n",
      "Epoch 184 | Batch: 26 | Loss: 11.0160\n",
      "Epoch 184 | Batch: 27 | Loss: 6.7855\n",
      "Epoch 184 | Batch: 28 | Loss: 11.1092\n",
      "Epoch 184 | Batch: 29 | Loss: 11.2766\n",
      "Epoch 184 | Batch: 30 | Loss: 5.5026\n",
      "Epoch 184 | Batch: 31 | Loss: 5.1208\n",
      "Epoch 184 | Batch: 32 | Loss: 10.0876\n",
      "Epoch 184 | Batch: 33 | Loss: 3.1304\n",
      "Epoch 184 | Batch: 34 | Loss: 17.9399\n",
      "Epoch 184 | Batch: 35 | Loss: 11.9187\n",
      "Epoch 184 | Batch: 36 | Loss: 9.2100\n",
      "Epoch 184 | Batch: 37 | Loss: 4.5021\n",
      "Epoch 184 | Batch: 38 | Loss: 5.7678\n",
      "Epoch 184 | Batch: 39 | Loss: 8.8885\n",
      "Epoch 184 | Batch: 40 | Loss: 9.7166\n",
      "Epoch 184 | Batch: 41 | Loss: 7.1517\n",
      "Epoch 184 | Batch: 42 | Loss: 7.5803\n",
      "Epoch 184 | Batch: 43 | Loss: 9.6563\n",
      "Epoch 184 | Batch: 44 | Loss: 7.4652\n",
      "Epoch 184 | Batch: 45 | Loss: 8.6669\n",
      "Epoch 184 | Batch: 46 | Loss: 7.5415\n",
      "Epoch 184 | Batch: 47 | Loss: 4.1779\n",
      "Epoch 184 | Batch: 48 | Loss: 2.4291\n",
      "Mean 7.923646693428357\n",
      "Epoch 185 | Batch: 1 | Loss: 7.8307\n",
      "Epoch 185 | Batch: 2 | Loss: 12.4795\n",
      "Epoch 185 | Batch: 3 | Loss: 6.5595\n",
      "Epoch 185 | Batch: 4 | Loss: 8.6295\n",
      "Epoch 185 | Batch: 5 | Loss: 12.8247\n",
      "Epoch 185 | Batch: 6 | Loss: 9.5267\n",
      "Epoch 185 | Batch: 7 | Loss: 5.4356\n",
      "Epoch 185 | Batch: 8 | Loss: 7.9254\n",
      "Epoch 185 | Batch: 9 | Loss: 6.1530\n",
      "Epoch 185 | Batch: 10 | Loss: 8.9987\n",
      "Epoch 185 | Batch: 11 | Loss: 2.7297\n",
      "Epoch 185 | Batch: 12 | Loss: 8.5599\n",
      "Epoch 185 | Batch: 13 | Loss: 7.5305\n",
      "Epoch 185 | Batch: 14 | Loss: 7.1470\n",
      "Epoch 185 | Batch: 15 | Loss: 12.4679\n",
      "Epoch 185 | Batch: 16 | Loss: 6.5088\n",
      "Epoch 185 | Batch: 17 | Loss: 14.2635\n",
      "Epoch 185 | Batch: 18 | Loss: 4.1506\n",
      "Epoch 185 | Batch: 19 | Loss: 2.9400\n",
      "Epoch 185 | Batch: 20 | Loss: 5.4952\n",
      "Epoch 185 | Batch: 21 | Loss: 7.7898\n",
      "Epoch 185 | Batch: 22 | Loss: 6.0103\n",
      "Epoch 185 | Batch: 23 | Loss: 7.3966\n",
      "Epoch 185 | Batch: 24 | Loss: 4.2687\n",
      "Epoch 185 | Batch: 25 | Loss: 8.7146\n",
      "Epoch 185 | Batch: 26 | Loss: 11.9569\n",
      "Epoch 185 | Batch: 27 | Loss: 12.4234\n",
      "Epoch 185 | Batch: 28 | Loss: 10.6081\n",
      "Epoch 185 | Batch: 29 | Loss: 27.8613\n",
      "Epoch 185 | Batch: 30 | Loss: 12.5409\n",
      "Epoch 185 | Batch: 31 | Loss: 3.0895\n",
      "Epoch 185 | Batch: 32 | Loss: 3.7597\n",
      "Epoch 185 | Batch: 33 | Loss: 8.1649\n",
      "Epoch 185 | Batch: 34 | Loss: 8.6186\n",
      "Epoch 185 | Batch: 35 | Loss: 10.0288\n",
      "Epoch 185 | Batch: 36 | Loss: 16.7904\n",
      "Epoch 185 | Batch: 37 | Loss: 4.0011\n",
      "Epoch 185 | Batch: 38 | Loss: 10.1909\n",
      "Epoch 185 | Batch: 39 | Loss: 13.6688\n",
      "Epoch 185 | Batch: 40 | Loss: 7.2600\n",
      "Epoch 185 | Batch: 41 | Loss: 3.4617\n",
      "Epoch 185 | Batch: 42 | Loss: 6.0916\n",
      "Epoch 185 | Batch: 43 | Loss: 20.3011\n",
      "Epoch 185 | Batch: 44 | Loss: 10.7314\n",
      "Epoch 185 | Batch: 45 | Loss: 13.9685\n",
      "Epoch 185 | Batch: 46 | Loss: 11.9779\n",
      "Epoch 185 | Batch: 47 | Loss: 10.1580\n",
      "Epoch 185 | Batch: 48 | Loss: 2.5545\n",
      "Mean 9.011347383260727\n",
      "Epoch 186 | Batch: 1 | Loss: 10.0940\n",
      "Epoch 186 | Batch: 2 | Loss: 6.0178\n",
      "Epoch 186 | Batch: 3 | Loss: 4.6518\n",
      "Epoch 186 | Batch: 4 | Loss: 1.8786\n",
      "Epoch 186 | Batch: 5 | Loss: 3.3369\n",
      "Epoch 186 | Batch: 6 | Loss: 9.2097\n",
      "Epoch 186 | Batch: 7 | Loss: 12.5977\n",
      "Epoch 186 | Batch: 8 | Loss: 4.4415\n",
      "Epoch 186 | Batch: 9 | Loss: 7.2981\n",
      "Epoch 186 | Batch: 10 | Loss: 5.0032\n",
      "Epoch 186 | Batch: 11 | Loss: 3.8202\n",
      "Epoch 186 | Batch: 12 | Loss: 5.5597\n",
      "Epoch 186 | Batch: 13 | Loss: 12.7223\n",
      "Epoch 186 | Batch: 14 | Loss: 16.1802\n",
      "Epoch 186 | Batch: 15 | Loss: 9.8208\n",
      "Epoch 186 | Batch: 16 | Loss: 8.8810\n",
      "Epoch 186 | Batch: 17 | Loss: 7.7015\n",
      "Epoch 186 | Batch: 18 | Loss: 5.6091\n",
      "Epoch 186 | Batch: 19 | Loss: 7.5137\n",
      "Epoch 186 | Batch: 20 | Loss: 6.4358\n",
      "Epoch 186 | Batch: 21 | Loss: 9.8297\n",
      "Epoch 186 | Batch: 22 | Loss: 12.9149\n",
      "Epoch 186 | Batch: 23 | Loss: 5.9769\n",
      "Epoch 186 | Batch: 24 | Loss: 7.7557\n",
      "Epoch 186 | Batch: 25 | Loss: 6.5222\n",
      "Epoch 186 | Batch: 26 | Loss: 9.9649\n",
      "Epoch 186 | Batch: 27 | Loss: 12.2097\n",
      "Epoch 186 | Batch: 28 | Loss: 8.3490\n",
      "Epoch 186 | Batch: 29 | Loss: 4.2645\n",
      "Epoch 186 | Batch: 30 | Loss: 5.0426\n",
      "Epoch 186 | Batch: 31 | Loss: 4.1119\n",
      "Epoch 186 | Batch: 32 | Loss: 9.4000\n",
      "Epoch 186 | Batch: 33 | Loss: 9.5784\n",
      "Epoch 186 | Batch: 34 | Loss: 8.5382\n",
      "Epoch 186 | Batch: 35 | Loss: 8.1149\n",
      "Epoch 186 | Batch: 36 | Loss: 2.1258\n",
      "Epoch 186 | Batch: 37 | Loss: 6.1424\n",
      "Epoch 186 | Batch: 38 | Loss: 11.1777\n",
      "Epoch 186 | Batch: 39 | Loss: 7.0258\n",
      "Epoch 186 | Batch: 40 | Loss: 6.1693\n",
      "Epoch 186 | Batch: 41 | Loss: 11.9672\n",
      "Epoch 186 | Batch: 42 | Loss: 10.5476\n",
      "Epoch 186 | Batch: 43 | Loss: 4.8240\n",
      "Epoch 186 | Batch: 44 | Loss: 5.3726\n",
      "Epoch 186 | Batch: 45 | Loss: 9.5536\n",
      "Epoch 186 | Batch: 46 | Loss: 12.0201\n",
      "Epoch 186 | Batch: 47 | Loss: 4.6379\n",
      "Epoch 186 | Batch: 48 | Loss: 2.8036\n",
      "Mean 7.619053627053897\n",
      "Epoch 187 | Batch: 1 | Loss: 6.6640\n",
      "Epoch 187 | Batch: 2 | Loss: 7.3526\n",
      "Epoch 187 | Batch: 3 | Loss: 7.7090\n",
      "Epoch 187 | Batch: 4 | Loss: 6.1515\n",
      "Epoch 187 | Batch: 5 | Loss: 2.5919\n",
      "Epoch 187 | Batch: 6 | Loss: 7.0994\n",
      "Epoch 187 | Batch: 7 | Loss: 4.5737\n",
      "Epoch 187 | Batch: 8 | Loss: 8.9325\n",
      "Epoch 187 | Batch: 9 | Loss: 7.1184\n",
      "Epoch 187 | Batch: 10 | Loss: 6.1813\n",
      "Epoch 187 | Batch: 11 | Loss: 6.4125\n",
      "Epoch 187 | Batch: 12 | Loss: 10.4198\n",
      "Epoch 187 | Batch: 13 | Loss: 5.4986\n",
      "Epoch 187 | Batch: 14 | Loss: 4.9508\n",
      "Epoch 187 | Batch: 15 | Loss: 7.7919\n",
      "Epoch 187 | Batch: 16 | Loss: 24.3268\n",
      "Epoch 187 | Batch: 17 | Loss: 9.3112\n",
      "Epoch 187 | Batch: 18 | Loss: 3.0265\n",
      "Epoch 187 | Batch: 19 | Loss: 4.4440\n",
      "Epoch 187 | Batch: 20 | Loss: 10.0709\n",
      "Epoch 187 | Batch: 21 | Loss: 8.3516\n",
      "Epoch 187 | Batch: 22 | Loss: 7.1736\n",
      "Epoch 187 | Batch: 23 | Loss: 10.3715\n",
      "Epoch 187 | Batch: 24 | Loss: 7.4057\n",
      "Epoch 187 | Batch: 25 | Loss: 7.5954\n",
      "Epoch 187 | Batch: 26 | Loss: 14.2732\n",
      "Epoch 187 | Batch: 27 | Loss: 19.3951\n",
      "Epoch 187 | Batch: 28 | Loss: 13.9272\n",
      "Epoch 187 | Batch: 29 | Loss: 19.1763\n",
      "Epoch 187 | Batch: 30 | Loss: 9.9968\n",
      "Epoch 187 | Batch: 31 | Loss: 8.0131\n",
      "Epoch 187 | Batch: 32 | Loss: 14.2954\n",
      "Epoch 187 | Batch: 33 | Loss: 22.8537\n",
      "Epoch 187 | Batch: 34 | Loss: 20.7857\n",
      "Epoch 187 | Batch: 35 | Loss: 7.0037\n",
      "Epoch 187 | Batch: 36 | Loss: 6.9063\n",
      "Epoch 187 | Batch: 37 | Loss: 5.5462\n",
      "Epoch 187 | Batch: 38 | Loss: 12.0745\n",
      "Epoch 187 | Batch: 39 | Loss: 16.0911\n",
      "Epoch 187 | Batch: 40 | Loss: 4.7511\n",
      "Epoch 187 | Batch: 41 | Loss: 7.1145\n",
      "Epoch 187 | Batch: 42 | Loss: 18.1799\n",
      "Epoch 187 | Batch: 43 | Loss: 25.7118\n",
      "Epoch 187 | Batch: 44 | Loss: 11.1496\n",
      "Epoch 187 | Batch: 45 | Loss: 7.5855\n",
      "Epoch 187 | Batch: 46 | Loss: 5.1747\n",
      "Epoch 187 | Batch: 47 | Loss: 5.5938\n",
      "Epoch 187 | Batch: 48 | Loss: 2.6936\n",
      "Mean 9.787873521447182\n",
      "Epoch 188 | Batch: 1 | Loss: 6.1757\n",
      "Epoch 188 | Batch: 2 | Loss: 10.5768\n",
      "Epoch 188 | Batch: 3 | Loss: 7.3298\n",
      "Epoch 188 | Batch: 4 | Loss: 7.3034\n",
      "Epoch 188 | Batch: 5 | Loss: 10.4822\n",
      "Epoch 188 | Batch: 6 | Loss: 6.5914\n",
      "Epoch 188 | Batch: 7 | Loss: 20.8766\n",
      "Epoch 188 | Batch: 8 | Loss: 6.2781\n",
      "Epoch 188 | Batch: 9 | Loss: 10.9370\n",
      "Epoch 188 | Batch: 10 | Loss: 7.6347\n",
      "Epoch 188 | Batch: 11 | Loss: 4.3690\n",
      "Epoch 188 | Batch: 12 | Loss: 6.9463\n",
      "Epoch 188 | Batch: 13 | Loss: 8.1295\n",
      "Epoch 188 | Batch: 14 | Loss: 6.1113\n",
      "Epoch 188 | Batch: 15 | Loss: 7.4506\n",
      "Epoch 188 | Batch: 16 | Loss: 9.3017\n",
      "Epoch 188 | Batch: 17 | Loss: 5.2789\n",
      "Epoch 188 | Batch: 18 | Loss: 9.0507\n",
      "Epoch 188 | Batch: 19 | Loss: 8.8535\n",
      "Epoch 188 | Batch: 20 | Loss: 5.5765\n",
      "Epoch 188 | Batch: 21 | Loss: 3.7232\n",
      "Epoch 188 | Batch: 22 | Loss: 8.0967\n",
      "Epoch 188 | Batch: 23 | Loss: 13.6731\n",
      "Epoch 188 | Batch: 24 | Loss: 17.7959\n",
      "Epoch 188 | Batch: 25 | Loss: 6.9728\n",
      "Epoch 188 | Batch: 26 | Loss: 6.7831\n",
      "Epoch 188 | Batch: 27 | Loss: 3.6989\n",
      "Epoch 188 | Batch: 28 | Loss: 6.9944\n",
      "Epoch 188 | Batch: 29 | Loss: 6.3108\n",
      "Epoch 188 | Batch: 30 | Loss: 7.3065\n",
      "Epoch 188 | Batch: 31 | Loss: 7.1199\n",
      "Epoch 188 | Batch: 32 | Loss: 2.6076\n",
      "Epoch 188 | Batch: 33 | Loss: 10.8035\n",
      "Epoch 188 | Batch: 34 | Loss: 2.2536\n",
      "Epoch 188 | Batch: 35 | Loss: 11.9455\n",
      "Epoch 188 | Batch: 36 | Loss: 9.0502\n",
      "Epoch 188 | Batch: 37 | Loss: 11.4041\n",
      "Epoch 188 | Batch: 38 | Loss: 10.9684\n",
      "Epoch 188 | Batch: 39 | Loss: 10.6282\n",
      "Epoch 188 | Batch: 40 | Loss: 6.8829\n",
      "Epoch 188 | Batch: 41 | Loss: 10.2298\n",
      "Epoch 188 | Batch: 42 | Loss: 6.8546\n",
      "Epoch 188 | Batch: 43 | Loss: 5.1540\n",
      "Epoch 188 | Batch: 44 | Loss: 9.5205\n",
      "Epoch 188 | Batch: 45 | Loss: 4.2017\n",
      "Epoch 188 | Batch: 46 | Loss: 8.9714\n",
      "Epoch 188 | Batch: 47 | Loss: 14.2169\n",
      "Epoch 188 | Batch: 48 | Loss: 6.6391\n",
      "Mean 8.25126620133718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189 | Batch: 1 | Loss: 5.6512\n",
      "Epoch 189 | Batch: 2 | Loss: 5.0860\n",
      "Epoch 189 | Batch: 3 | Loss: 7.6842\n",
      "Epoch 189 | Batch: 4 | Loss: 2.5011\n",
      "Epoch 189 | Batch: 5 | Loss: 5.4830\n",
      "Epoch 189 | Batch: 6 | Loss: 3.3482\n",
      "Epoch 189 | Batch: 7 | Loss: 5.6757\n",
      "Epoch 189 | Batch: 8 | Loss: 5.6246\n",
      "Epoch 189 | Batch: 9 | Loss: 5.1315\n",
      "Epoch 189 | Batch: 10 | Loss: 17.7856\n",
      "Epoch 189 | Batch: 11 | Loss: 8.4377\n",
      "Epoch 189 | Batch: 12 | Loss: 4.8871\n",
      "Epoch 189 | Batch: 13 | Loss: 5.9681\n",
      "Epoch 189 | Batch: 14 | Loss: 4.7890\n",
      "Epoch 189 | Batch: 15 | Loss: 4.1557\n",
      "Epoch 189 | Batch: 16 | Loss: 20.1080\n",
      "Epoch 189 | Batch: 17 | Loss: 16.9276\n",
      "Epoch 189 | Batch: 18 | Loss: 14.6306\n",
      "Epoch 189 | Batch: 19 | Loss: 5.4368\n",
      "Epoch 189 | Batch: 20 | Loss: 10.9248\n",
      "Epoch 189 | Batch: 21 | Loss: 11.5441\n",
      "Epoch 189 | Batch: 22 | Loss: 4.3063\n",
      "Epoch 189 | Batch: 23 | Loss: 8.8117\n",
      "Epoch 189 | Batch: 24 | Loss: 18.5930\n",
      "Epoch 189 | Batch: 25 | Loss: 7.0023\n",
      "Epoch 189 | Batch: 26 | Loss: 12.2638\n",
      "Epoch 189 | Batch: 27 | Loss: 10.4261\n",
      "Epoch 189 | Batch: 28 | Loss: 7.9560\n",
      "Epoch 189 | Batch: 29 | Loss: 6.7248\n",
      "Epoch 189 | Batch: 30 | Loss: 7.4818\n",
      "Epoch 189 | Batch: 31 | Loss: 6.8746\n",
      "Epoch 189 | Batch: 32 | Loss: 3.2178\n",
      "Epoch 189 | Batch: 33 | Loss: 5.6040\n",
      "Epoch 189 | Batch: 34 | Loss: 6.7242\n",
      "Epoch 189 | Batch: 35 | Loss: 6.9319\n",
      "Epoch 189 | Batch: 36 | Loss: 13.5484\n",
      "Epoch 189 | Batch: 37 | Loss: 5.8818\n",
      "Epoch 189 | Batch: 38 | Loss: 5.6978\n",
      "Epoch 189 | Batch: 39 | Loss: 6.8900\n",
      "Epoch 189 | Batch: 40 | Loss: 4.3938\n",
      "Epoch 189 | Batch: 41 | Loss: 6.9572\n",
      "Epoch 189 | Batch: 42 | Loss: 7.1736\n",
      "Epoch 189 | Batch: 43 | Loss: 12.1943\n",
      "Epoch 189 | Batch: 44 | Loss: 9.6187\n",
      "Epoch 189 | Batch: 45 | Loss: 6.4583\n",
      "Epoch 189 | Batch: 46 | Loss: 5.7516\n",
      "Epoch 189 | Batch: 47 | Loss: 10.5049\n",
      "Epoch 189 | Batch: 48 | Loss: 6.8654\n",
      "Mean 8.054894869526228\n",
      "Epoch 190 | Batch: 1 | Loss: 2.6730\n",
      "Epoch 190 | Batch: 2 | Loss: 13.1991\n",
      "Epoch 190 | Batch: 3 | Loss: 13.7075\n",
      "Epoch 190 | Batch: 4 | Loss: 7.0494\n",
      "Epoch 190 | Batch: 5 | Loss: 5.9418\n",
      "Epoch 190 | Batch: 6 | Loss: 12.1230\n",
      "Epoch 190 | Batch: 7 | Loss: 5.4815\n",
      "Epoch 190 | Batch: 8 | Loss: 6.2244\n",
      "Epoch 190 | Batch: 9 | Loss: 7.4013\n",
      "Epoch 190 | Batch: 10 | Loss: 7.8342\n",
      "Epoch 190 | Batch: 11 | Loss: 5.9236\n",
      "Epoch 190 | Batch: 12 | Loss: 9.8422\n",
      "Epoch 190 | Batch: 13 | Loss: 5.6716\n",
      "Epoch 190 | Batch: 14 | Loss: 5.9988\n",
      "Epoch 190 | Batch: 15 | Loss: 9.6279\n",
      "Epoch 190 | Batch: 16 | Loss: 9.4453\n",
      "Epoch 190 | Batch: 17 | Loss: 5.0069\n",
      "Epoch 190 | Batch: 18 | Loss: 10.6792\n",
      "Epoch 190 | Batch: 19 | Loss: 3.8400\n",
      "Epoch 190 | Batch: 20 | Loss: 9.5814\n",
      "Epoch 190 | Batch: 21 | Loss: 7.4201\n",
      "Epoch 190 | Batch: 22 | Loss: 13.8318\n",
      "Epoch 190 | Batch: 23 | Loss: 2.2727\n",
      "Epoch 190 | Batch: 24 | Loss: 8.2113\n",
      "Epoch 190 | Batch: 25 | Loss: 27.9820\n",
      "Epoch 190 | Batch: 26 | Loss: 10.8190\n",
      "Epoch 190 | Batch: 27 | Loss: 5.4639\n",
      "Epoch 190 | Batch: 28 | Loss: 8.2789\n",
      "Epoch 190 | Batch: 29 | Loss: 6.4589\n",
      "Epoch 190 | Batch: 30 | Loss: 10.4084\n",
      "Epoch 190 | Batch: 31 | Loss: 12.1965\n",
      "Epoch 190 | Batch: 32 | Loss: 15.1102\n",
      "Epoch 190 | Batch: 33 | Loss: 5.4988\n",
      "Epoch 190 | Batch: 34 | Loss: 10.4592\n",
      "Epoch 190 | Batch: 35 | Loss: 6.6941\n",
      "Epoch 190 | Batch: 36 | Loss: 11.5027\n",
      "Epoch 190 | Batch: 37 | Loss: 10.1601\n",
      "Epoch 190 | Batch: 38 | Loss: 7.0420\n",
      "Epoch 190 | Batch: 39 | Loss: 6.6584\n",
      "Epoch 190 | Batch: 40 | Loss: 8.8502\n",
      "Epoch 190 | Batch: 41 | Loss: 3.1272\n",
      "Epoch 190 | Batch: 42 | Loss: 10.7840\n",
      "Epoch 190 | Batch: 43 | Loss: 8.7522\n",
      "Epoch 190 | Batch: 44 | Loss: 10.6608\n",
      "Epoch 190 | Batch: 45 | Loss: 7.3518\n",
      "Epoch 190 | Batch: 46 | Loss: 4.8625\n",
      "Epoch 190 | Batch: 47 | Loss: 8.4249\n",
      "Epoch 190 | Batch: 48 | Loss: 3.8742\n",
      "Mean 8.550180966655413\n",
      "Epoch 191 | Batch: 1 | Loss: 6.9267\n",
      "Epoch 191 | Batch: 2 | Loss: 6.7238\n",
      "Epoch 191 | Batch: 3 | Loss: 6.1568\n",
      "Epoch 191 | Batch: 4 | Loss: 5.9852\n",
      "Epoch 191 | Batch: 5 | Loss: 7.5546\n",
      "Epoch 191 | Batch: 6 | Loss: 5.9048\n",
      "Epoch 191 | Batch: 7 | Loss: 4.7707\n",
      "Epoch 191 | Batch: 8 | Loss: 12.5897\n",
      "Epoch 191 | Batch: 9 | Loss: 4.3928\n",
      "Epoch 191 | Batch: 10 | Loss: 5.8885\n",
      "Epoch 191 | Batch: 11 | Loss: 6.8313\n",
      "Epoch 191 | Batch: 12 | Loss: 14.0532\n",
      "Epoch 191 | Batch: 13 | Loss: 12.4715\n",
      "Epoch 191 | Batch: 14 | Loss: 4.1463\n",
      "Epoch 191 | Batch: 15 | Loss: 5.5262\n",
      "Epoch 191 | Batch: 16 | Loss: 9.7281\n",
      "Epoch 191 | Batch: 17 | Loss: 10.1246\n",
      "Epoch 191 | Batch: 18 | Loss: 11.0905\n",
      "Epoch 191 | Batch: 19 | Loss: 8.3061\n",
      "Epoch 191 | Batch: 20 | Loss: 15.1684\n",
      "Epoch 191 | Batch: 21 | Loss: 15.8476\n",
      "Epoch 191 | Batch: 22 | Loss: 5.3541\n",
      "Epoch 191 | Batch: 23 | Loss: 8.9491\n",
      "Epoch 191 | Batch: 24 | Loss: 10.8467\n",
      "Epoch 191 | Batch: 25 | Loss: 13.3023\n",
      "Epoch 191 | Batch: 26 | Loss: 5.4943\n",
      "Epoch 191 | Batch: 27 | Loss: 8.0491\n",
      "Epoch 191 | Batch: 28 | Loss: 5.4484\n",
      "Epoch 191 | Batch: 29 | Loss: 9.8048\n",
      "Epoch 191 | Batch: 30 | Loss: 11.5703\n",
      "Epoch 191 | Batch: 31 | Loss: 13.9869\n",
      "Epoch 191 | Batch: 32 | Loss: 8.0941\n",
      "Epoch 191 | Batch: 33 | Loss: 6.5071\n",
      "Epoch 191 | Batch: 34 | Loss: 13.6746\n",
      "Epoch 191 | Batch: 35 | Loss: 7.6988\n",
      "Epoch 191 | Batch: 36 | Loss: 5.7283\n",
      "Epoch 191 | Batch: 37 | Loss: 10.2777\n",
      "Epoch 191 | Batch: 38 | Loss: 4.3501\n",
      "Epoch 191 | Batch: 39 | Loss: 6.6668\n",
      "Epoch 191 | Batch: 40 | Loss: 5.4613\n",
      "Epoch 191 | Batch: 41 | Loss: 8.2511\n",
      "Epoch 191 | Batch: 42 | Loss: 16.4560\n",
      "Epoch 191 | Batch: 43 | Loss: 4.9799\n",
      "Epoch 191 | Batch: 44 | Loss: 10.6158\n",
      "Epoch 191 | Batch: 45 | Loss: 7.6520\n",
      "Epoch 191 | Batch: 46 | Loss: 6.4275\n",
      "Epoch 191 | Batch: 47 | Loss: 5.4616\n",
      "Epoch 191 | Batch: 48 | Loss: 4.8768\n",
      "Mean 8.4619267086188\n",
      "Epoch 192 | Batch: 1 | Loss: 4.0318\n",
      "Epoch 192 | Batch: 2 | Loss: 10.4433\n",
      "Epoch 192 | Batch: 3 | Loss: 5.3342\n",
      "Epoch 192 | Batch: 4 | Loss: 6.4789\n",
      "Epoch 192 | Batch: 5 | Loss: 8.1187\n",
      "Epoch 192 | Batch: 6 | Loss: 7.3310\n",
      "Epoch 192 | Batch: 7 | Loss: 10.3965\n",
      "Epoch 192 | Batch: 8 | Loss: 9.5178\n",
      "Epoch 192 | Batch: 9 | Loss: 5.6517\n",
      "Epoch 192 | Batch: 10 | Loss: 6.7086\n",
      "Epoch 192 | Batch: 11 | Loss: 7.8106\n",
      "Epoch 192 | Batch: 12 | Loss: 10.3331\n",
      "Epoch 192 | Batch: 13 | Loss: 5.9978\n",
      "Epoch 192 | Batch: 14 | Loss: 6.6674\n",
      "Epoch 192 | Batch: 15 | Loss: 11.3681\n",
      "Epoch 192 | Batch: 16 | Loss: 11.2180\n",
      "Epoch 192 | Batch: 17 | Loss: 9.9179\n",
      "Epoch 192 | Batch: 18 | Loss: 9.4134\n",
      "Epoch 192 | Batch: 19 | Loss: 4.0690\n",
      "Epoch 192 | Batch: 20 | Loss: 6.1239\n",
      "Epoch 192 | Batch: 21 | Loss: 3.6063\n",
      "Epoch 192 | Batch: 22 | Loss: 7.3758\n",
      "Epoch 192 | Batch: 23 | Loss: 9.0451\n",
      "Epoch 192 | Batch: 24 | Loss: 7.4707\n",
      "Epoch 192 | Batch: 25 | Loss: 7.0145\n",
      "Epoch 192 | Batch: 26 | Loss: 9.7515\n",
      "Epoch 192 | Batch: 27 | Loss: 5.9700\n",
      "Epoch 192 | Batch: 28 | Loss: 12.0622\n",
      "Epoch 192 | Batch: 29 | Loss: 7.2544\n",
      "Epoch 192 | Batch: 30 | Loss: 5.2884\n",
      "Epoch 192 | Batch: 31 | Loss: 6.1509\n",
      "Epoch 192 | Batch: 32 | Loss: 10.3218\n",
      "Epoch 192 | Batch: 33 | Loss: 10.1572\n",
      "Epoch 192 | Batch: 34 | Loss: 6.5221\n",
      "Epoch 192 | Batch: 35 | Loss: 11.3292\n",
      "Epoch 192 | Batch: 36 | Loss: 20.1581\n",
      "Epoch 192 | Batch: 37 | Loss: 7.3886\n",
      "Epoch 192 | Batch: 38 | Loss: 12.3641\n",
      "Epoch 192 | Batch: 39 | Loss: 6.2069\n",
      "Epoch 192 | Batch: 40 | Loss: 5.9421\n",
      "Epoch 192 | Batch: 41 | Loss: 9.2248\n",
      "Epoch 192 | Batch: 42 | Loss: 9.2251\n",
      "Epoch 192 | Batch: 43 | Loss: 4.9787\n",
      "Epoch 192 | Batch: 44 | Loss: 9.2275\n",
      "Epoch 192 | Batch: 45 | Loss: 2.6806\n",
      "Epoch 192 | Batch: 46 | Loss: 4.2722\n",
      "Epoch 192 | Batch: 47 | Loss: 9.7733\n",
      "Epoch 192 | Batch: 48 | Loss: 1.9625\n",
      "Mean 7.909505183498065\n",
      "Epoch 193 | Batch: 1 | Loss: 15.6566\n",
      "Epoch 193 | Batch: 2 | Loss: 10.5604\n",
      "Epoch 193 | Batch: 3 | Loss: 4.2972\n",
      "Epoch 193 | Batch: 4 | Loss: 7.9176\n",
      "Epoch 193 | Batch: 5 | Loss: 9.4270\n",
      "Epoch 193 | Batch: 6 | Loss: 9.0570\n",
      "Epoch 193 | Batch: 7 | Loss: 2.2817\n",
      "Epoch 193 | Batch: 8 | Loss: 14.3993\n",
      "Epoch 193 | Batch: 9 | Loss: 23.0598\n",
      "Epoch 193 | Batch: 10 | Loss: 12.3327\n",
      "Epoch 193 | Batch: 11 | Loss: 5.0464\n",
      "Epoch 193 | Batch: 12 | Loss: 7.9366\n",
      "Epoch 193 | Batch: 13 | Loss: 20.8398\n",
      "Epoch 193 | Batch: 14 | Loss: 29.5464\n",
      "Epoch 193 | Batch: 15 | Loss: 4.8580\n",
      "Epoch 193 | Batch: 16 | Loss: 8.6831\n",
      "Epoch 193 | Batch: 17 | Loss: 15.1338\n",
      "Epoch 193 | Batch: 18 | Loss: 10.4449\n",
      "Epoch 193 | Batch: 19 | Loss: 11.9869\n",
      "Epoch 193 | Batch: 20 | Loss: 6.5834\n",
      "Epoch 193 | Batch: 21 | Loss: 3.0141\n",
      "Epoch 193 | Batch: 22 | Loss: 15.5642\n",
      "Epoch 193 | Batch: 23 | Loss: 14.3668\n",
      "Epoch 193 | Batch: 24 | Loss: 8.5372\n",
      "Epoch 193 | Batch: 25 | Loss: 5.5536\n",
      "Epoch 193 | Batch: 26 | Loss: 6.0782\n",
      "Epoch 193 | Batch: 27 | Loss: 8.3438\n",
      "Epoch 193 | Batch: 28 | Loss: 8.5994\n",
      "Epoch 193 | Batch: 29 | Loss: 10.8714\n",
      "Epoch 193 | Batch: 30 | Loss: 11.1664\n",
      "Epoch 193 | Batch: 31 | Loss: 5.0406\n",
      "Epoch 193 | Batch: 32 | Loss: 4.9051\n",
      "Epoch 193 | Batch: 33 | Loss: 6.6161\n",
      "Epoch 193 | Batch: 34 | Loss: 5.4688\n",
      "Epoch 193 | Batch: 35 | Loss: 8.4285\n",
      "Epoch 193 | Batch: 36 | Loss: 7.3009\n",
      "Epoch 193 | Batch: 37 | Loss: 6.7234\n",
      "Epoch 193 | Batch: 38 | Loss: 6.1101\n",
      "Epoch 193 | Batch: 39 | Loss: 10.1363\n",
      "Epoch 193 | Batch: 40 | Loss: 17.6028\n",
      "Epoch 193 | Batch: 41 | Loss: 9.3854\n",
      "Epoch 193 | Batch: 42 | Loss: 5.4020\n",
      "Epoch 193 | Batch: 43 | Loss: 4.6504\n",
      "Epoch 193 | Batch: 44 | Loss: 4.9737\n",
      "Epoch 193 | Batch: 45 | Loss: 5.5779\n",
      "Epoch 193 | Batch: 46 | Loss: 10.1518\n",
      "Epoch 193 | Batch: 47 | Loss: 2.8565\n",
      "Epoch 193 | Batch: 48 | Loss: 4.4313\n",
      "Mean 9.33136215309302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194 | Batch: 1 | Loss: 12.5380\n",
      "Epoch 194 | Batch: 2 | Loss: 7.9996\n",
      "Epoch 194 | Batch: 3 | Loss: 6.2459\n",
      "Epoch 194 | Batch: 4 | Loss: 8.3607\n",
      "Epoch 194 | Batch: 5 | Loss: 3.6367\n",
      "Epoch 194 | Batch: 6 | Loss: 10.7117\n",
      "Epoch 194 | Batch: 7 | Loss: 10.7791\n",
      "Epoch 194 | Batch: 8 | Loss: 8.1854\n",
      "Epoch 194 | Batch: 9 | Loss: 9.9519\n",
      "Epoch 194 | Batch: 10 | Loss: 2.5024\n",
      "Epoch 194 | Batch: 11 | Loss: 14.2809\n",
      "Epoch 194 | Batch: 12 | Loss: 20.1460\n",
      "Epoch 194 | Batch: 13 | Loss: 17.0444\n",
      "Epoch 194 | Batch: 14 | Loss: 6.4358\n",
      "Epoch 194 | Batch: 15 | Loss: 7.7265\n",
      "Epoch 194 | Batch: 16 | Loss: 6.6166\n",
      "Epoch 194 | Batch: 17 | Loss: 4.8672\n",
      "Epoch 194 | Batch: 18 | Loss: 13.1170\n",
      "Epoch 194 | Batch: 19 | Loss: 8.2111\n",
      "Epoch 194 | Batch: 20 | Loss: 8.7972\n",
      "Epoch 194 | Batch: 21 | Loss: 11.9370\n",
      "Epoch 194 | Batch: 22 | Loss: 3.9600\n",
      "Epoch 194 | Batch: 23 | Loss: 11.3576\n",
      "Epoch 194 | Batch: 24 | Loss: 6.1026\n",
      "Epoch 194 | Batch: 25 | Loss: 10.4797\n",
      "Epoch 194 | Batch: 26 | Loss: 20.4670\n",
      "Epoch 194 | Batch: 27 | Loss: 12.1892\n",
      "Epoch 194 | Batch: 28 | Loss: 3.7393\n",
      "Epoch 194 | Batch: 29 | Loss: 7.1469\n",
      "Epoch 194 | Batch: 30 | Loss: 12.6551\n",
      "Epoch 194 | Batch: 31 | Loss: 17.8984\n",
      "Epoch 194 | Batch: 32 | Loss: 12.3176\n",
      "Epoch 194 | Batch: 33 | Loss: 8.6025\n",
      "Epoch 194 | Batch: 34 | Loss: 16.2135\n",
      "Epoch 194 | Batch: 35 | Loss: 13.8214\n",
      "Epoch 194 | Batch: 36 | Loss: 5.7003\n",
      "Epoch 194 | Batch: 37 | Loss: 7.9182\n",
      "Epoch 194 | Batch: 38 | Loss: 6.1140\n",
      "Epoch 194 | Batch: 39 | Loss: 5.3359\n",
      "Epoch 194 | Batch: 40 | Loss: 11.0105\n",
      "Epoch 194 | Batch: 41 | Loss: 15.0857\n",
      "Epoch 194 | Batch: 42 | Loss: 8.5309\n",
      "Epoch 194 | Batch: 43 | Loss: 9.5001\n",
      "Epoch 194 | Batch: 44 | Loss: 17.8805\n",
      "Epoch 194 | Batch: 45 | Loss: 15.8138\n",
      "Epoch 194 | Batch: 46 | Loss: 12.4034\n",
      "Epoch 194 | Batch: 47 | Loss: 20.5626\n",
      "Epoch 194 | Batch: 48 | Loss: 5.4817\n",
      "Mean 10.382910574475924\n",
      "Epoch 195 | Batch: 1 | Loss: 12.0421\n",
      "Epoch 195 | Batch: 2 | Loss: 16.3071\n",
      "Epoch 195 | Batch: 3 | Loss: 13.7332\n",
      "Epoch 195 | Batch: 4 | Loss: 10.1670\n",
      "Epoch 195 | Batch: 5 | Loss: 17.3599\n",
      "Epoch 195 | Batch: 6 | Loss: 19.0360\n",
      "Epoch 195 | Batch: 7 | Loss: 8.1575\n",
      "Epoch 195 | Batch: 8 | Loss: 8.8356\n",
      "Epoch 195 | Batch: 9 | Loss: 3.6729\n",
      "Epoch 195 | Batch: 10 | Loss: 13.6493\n",
      "Epoch 195 | Batch: 11 | Loss: 7.7402\n",
      "Epoch 195 | Batch: 12 | Loss: 7.1709\n",
      "Epoch 195 | Batch: 13 | Loss: 10.6624\n",
      "Epoch 195 | Batch: 14 | Loss: 10.2478\n",
      "Epoch 195 | Batch: 15 | Loss: 5.8878\n",
      "Epoch 195 | Batch: 16 | Loss: 12.6997\n",
      "Epoch 195 | Batch: 17 | Loss: 8.3596\n",
      "Epoch 195 | Batch: 18 | Loss: 6.4170\n",
      "Epoch 195 | Batch: 19 | Loss: 13.0658\n",
      "Epoch 195 | Batch: 20 | Loss: 9.8429\n",
      "Epoch 195 | Batch: 21 | Loss: 6.0530\n",
      "Epoch 195 | Batch: 22 | Loss: 11.4463\n",
      "Epoch 195 | Batch: 23 | Loss: 9.2296\n",
      "Epoch 195 | Batch: 24 | Loss: 6.1003\n",
      "Epoch 195 | Batch: 25 | Loss: 11.1571\n",
      "Epoch 195 | Batch: 26 | Loss: 8.5050\n",
      "Epoch 195 | Batch: 27 | Loss: 4.1529\n",
      "Epoch 195 | Batch: 28 | Loss: 8.8788\n",
      "Epoch 195 | Batch: 29 | Loss: 11.9701\n",
      "Epoch 195 | Batch: 30 | Loss: 5.4232\n",
      "Epoch 195 | Batch: 31 | Loss: 6.4185\n",
      "Epoch 195 | Batch: 32 | Loss: 9.5655\n",
      "Epoch 195 | Batch: 33 | Loss: 4.7167\n",
      "Epoch 195 | Batch: 34 | Loss: 5.6557\n",
      "Epoch 195 | Batch: 35 | Loss: 7.2404\n",
      "Epoch 195 | Batch: 36 | Loss: 2.8873\n",
      "Epoch 195 | Batch: 37 | Loss: 6.8304\n",
      "Epoch 195 | Batch: 38 | Loss: 5.5308\n",
      "Epoch 195 | Batch: 39 | Loss: 12.8796\n",
      "Epoch 195 | Batch: 40 | Loss: 12.5440\n",
      "Epoch 195 | Batch: 41 | Loss: 15.2522\n",
      "Epoch 195 | Batch: 42 | Loss: 9.1892\n",
      "Epoch 195 | Batch: 43 | Loss: 5.9762\n",
      "Epoch 195 | Batch: 44 | Loss: 4.0220\n",
      "Epoch 195 | Batch: 45 | Loss: 16.5928\n",
      "Epoch 195 | Batch: 46 | Loss: 12.9932\n",
      "Epoch 195 | Batch: 47 | Loss: 4.4796\n",
      "Epoch 195 | Batch: 48 | Loss: 4.0297\n",
      "Mean 9.26613698899746\n",
      "Epoch 196 | Batch: 1 | Loss: 6.8430\n",
      "Epoch 196 | Batch: 2 | Loss: 7.2541\n",
      "Epoch 196 | Batch: 3 | Loss: 4.4323\n",
      "Epoch 196 | Batch: 4 | Loss: 4.0227\n",
      "Epoch 196 | Batch: 5 | Loss: 8.9640\n",
      "Epoch 196 | Batch: 6 | Loss: 12.2542\n",
      "Epoch 196 | Batch: 7 | Loss: 7.3196\n",
      "Epoch 196 | Batch: 8 | Loss: 5.9147\n",
      "Epoch 196 | Batch: 9 | Loss: 6.1501\n",
      "Epoch 196 | Batch: 10 | Loss: 2.2040\n",
      "Epoch 196 | Batch: 11 | Loss: 12.0839\n",
      "Epoch 196 | Batch: 12 | Loss: 8.5024\n",
      "Epoch 196 | Batch: 13 | Loss: 10.7028\n",
      "Epoch 196 | Batch: 14 | Loss: 7.2274\n",
      "Epoch 196 | Batch: 15 | Loss: 9.4690\n",
      "Epoch 196 | Batch: 16 | Loss: 14.3803\n",
      "Epoch 196 | Batch: 17 | Loss: 9.1876\n",
      "Epoch 196 | Batch: 18 | Loss: 5.5972\n",
      "Epoch 196 | Batch: 19 | Loss: 6.9380\n",
      "Epoch 196 | Batch: 20 | Loss: 5.4675\n",
      "Epoch 196 | Batch: 21 | Loss: 7.1588\n",
      "Epoch 196 | Batch: 22 | Loss: 5.1427\n",
      "Epoch 196 | Batch: 23 | Loss: 8.2578\n",
      "Epoch 196 | Batch: 24 | Loss: 4.1315\n",
      "Epoch 196 | Batch: 25 | Loss: 10.9740\n",
      "Epoch 196 | Batch: 26 | Loss: 31.7849\n",
      "Epoch 196 | Batch: 27 | Loss: 22.1420\n",
      "Epoch 196 | Batch: 28 | Loss: 9.9648\n",
      "Epoch 196 | Batch: 29 | Loss: 5.9740\n",
      "Epoch 196 | Batch: 30 | Loss: 8.6512\n",
      "Epoch 196 | Batch: 31 | Loss: 10.5545\n",
      "Epoch 196 | Batch: 32 | Loss: 6.2081\n",
      "Epoch 196 | Batch: 33 | Loss: 14.1502\n",
      "Epoch 196 | Batch: 34 | Loss: 6.1173\n",
      "Epoch 196 | Batch: 35 | Loss: 6.4127\n",
      "Epoch 196 | Batch: 36 | Loss: 3.3698\n",
      "Epoch 196 | Batch: 37 | Loss: 4.6246\n",
      "Epoch 196 | Batch: 38 | Loss: 12.3491\n",
      "Epoch 196 | Batch: 39 | Loss: 21.7954\n",
      "Epoch 196 | Batch: 40 | Loss: 16.5676\n",
      "Epoch 196 | Batch: 41 | Loss: 1.8615\n",
      "Epoch 196 | Batch: 42 | Loss: 6.6438\n",
      "Epoch 196 | Batch: 43 | Loss: 4.6666\n",
      "Epoch 196 | Batch: 44 | Loss: 13.3230\n",
      "Epoch 196 | Batch: 45 | Loss: 8.4004\n",
      "Epoch 196 | Batch: 46 | Loss: 11.2741\n",
      "Epoch 196 | Batch: 47 | Loss: 5.2510\n",
      "Epoch 196 | Batch: 48 | Loss: 1.1856\n",
      "Mean 8.830245087544123\n",
      "Epoch 197 | Batch: 1 | Loss: 9.1684\n",
      "Epoch 197 | Batch: 2 | Loss: 7.1672\n",
      "Epoch 197 | Batch: 3 | Loss: 12.4793\n",
      "Epoch 197 | Batch: 4 | Loss: 3.8832\n",
      "Epoch 197 | Batch: 5 | Loss: 7.7793\n",
      "Epoch 197 | Batch: 6 | Loss: 7.2687\n",
      "Epoch 197 | Batch: 7 | Loss: 4.8701\n",
      "Epoch 197 | Batch: 8 | Loss: 6.7985\n",
      "Epoch 197 | Batch: 9 | Loss: 3.4147\n",
      "Epoch 197 | Batch: 10 | Loss: 1.7350\n",
      "Epoch 197 | Batch: 11 | Loss: 8.3220\n",
      "Epoch 197 | Batch: 12 | Loss: 13.9557\n",
      "Epoch 197 | Batch: 13 | Loss: 9.5927\n",
      "Epoch 197 | Batch: 14 | Loss: 4.2843\n",
      "Epoch 197 | Batch: 15 | Loss: 4.8283\n",
      "Epoch 197 | Batch: 16 | Loss: 14.8163\n",
      "Epoch 197 | Batch: 17 | Loss: 5.8611\n",
      "Epoch 197 | Batch: 18 | Loss: 6.8617\n",
      "Epoch 197 | Batch: 19 | Loss: 10.1410\n",
      "Epoch 197 | Batch: 20 | Loss: 7.4166\n",
      "Epoch 197 | Batch: 21 | Loss: 7.4499\n",
      "Epoch 197 | Batch: 22 | Loss: 16.3875\n",
      "Epoch 197 | Batch: 23 | Loss: 13.5867\n",
      "Epoch 197 | Batch: 24 | Loss: 14.6045\n",
      "Epoch 197 | Batch: 25 | Loss: 7.6350\n",
      "Epoch 197 | Batch: 26 | Loss: 5.0278\n",
      "Epoch 197 | Batch: 27 | Loss: 6.8340\n",
      "Epoch 197 | Batch: 28 | Loss: 3.7837\n",
      "Epoch 197 | Batch: 29 | Loss: 14.7233\n",
      "Epoch 197 | Batch: 30 | Loss: 6.8518\n",
      "Epoch 197 | Batch: 31 | Loss: 4.9429\n",
      "Epoch 197 | Batch: 32 | Loss: 8.8470\n",
      "Epoch 197 | Batch: 33 | Loss: 6.4381\n",
      "Epoch 197 | Batch: 34 | Loss: 12.5858\n",
      "Epoch 197 | Batch: 35 | Loss: 13.6826\n",
      "Epoch 197 | Batch: 36 | Loss: 20.0303\n",
      "Epoch 197 | Batch: 37 | Loss: 12.3280\n",
      "Epoch 197 | Batch: 38 | Loss: 8.2732\n",
      "Epoch 197 | Batch: 39 | Loss: 3.9958\n",
      "Epoch 197 | Batch: 40 | Loss: 12.0241\n",
      "Epoch 197 | Batch: 41 | Loss: 6.1349\n",
      "Epoch 197 | Batch: 42 | Loss: 9.7432\n",
      "Epoch 197 | Batch: 43 | Loss: 6.1402\n",
      "Epoch 197 | Batch: 44 | Loss: 15.5182\n",
      "Epoch 197 | Batch: 45 | Loss: 5.4924\n",
      "Epoch 197 | Batch: 46 | Loss: 6.6105\n",
      "Epoch 197 | Batch: 47 | Loss: 9.8184\n",
      "Epoch 197 | Batch: 48 | Loss: 6.5185\n",
      "Mean 8.680255216856798\n",
      "Epoch 198 | Batch: 1 | Loss: 5.4073\n",
      "Epoch 198 | Batch: 2 | Loss: 7.0467\n",
      "Epoch 198 | Batch: 3 | Loss: 8.6452\n",
      "Epoch 198 | Batch: 4 | Loss: 3.3527\n",
      "Epoch 198 | Batch: 5 | Loss: 6.8806\n",
      "Epoch 198 | Batch: 6 | Loss: 8.6338\n",
      "Epoch 198 | Batch: 7 | Loss: 7.3892\n",
      "Epoch 198 | Batch: 8 | Loss: 2.6841\n",
      "Epoch 198 | Batch: 9 | Loss: 5.5013\n",
      "Epoch 198 | Batch: 10 | Loss: 8.5346\n",
      "Epoch 198 | Batch: 11 | Loss: 4.4090\n",
      "Epoch 198 | Batch: 12 | Loss: 11.1946\n",
      "Epoch 198 | Batch: 13 | Loss: 4.2773\n",
      "Epoch 198 | Batch: 14 | Loss: 8.1407\n",
      "Epoch 198 | Batch: 15 | Loss: 4.2116\n",
      "Epoch 198 | Batch: 16 | Loss: 4.7631\n",
      "Epoch 198 | Batch: 17 | Loss: 10.3599\n",
      "Epoch 198 | Batch: 18 | Loss: 11.5082\n",
      "Epoch 198 | Batch: 19 | Loss: 17.1118\n",
      "Epoch 198 | Batch: 20 | Loss: 14.7234\n",
      "Epoch 198 | Batch: 21 | Loss: 11.0336\n",
      "Epoch 198 | Batch: 22 | Loss: 6.6821\n",
      "Epoch 198 | Batch: 23 | Loss: 2.0434\n",
      "Epoch 198 | Batch: 24 | Loss: 15.6629\n",
      "Epoch 198 | Batch: 25 | Loss: 9.5798\n",
      "Epoch 198 | Batch: 26 | Loss: 17.8527\n",
      "Epoch 198 | Batch: 27 | Loss: 12.2514\n",
      "Epoch 198 | Batch: 28 | Loss: 10.8986\n",
      "Epoch 198 | Batch: 29 | Loss: 25.7309\n",
      "Epoch 198 | Batch: 30 | Loss: 22.2546\n",
      "Epoch 198 | Batch: 31 | Loss: 15.9711\n",
      "Epoch 198 | Batch: 32 | Loss: 10.3093\n",
      "Epoch 198 | Batch: 33 | Loss: 7.4834\n",
      "Epoch 198 | Batch: 34 | Loss: 7.7641\n",
      "Epoch 198 | Batch: 35 | Loss: 2.4489\n",
      "Epoch 198 | Batch: 36 | Loss: 9.1237\n",
      "Epoch 198 | Batch: 37 | Loss: 6.1529\n",
      "Epoch 198 | Batch: 38 | Loss: 6.7044\n",
      "Epoch 198 | Batch: 39 | Loss: 5.4180\n",
      "Epoch 198 | Batch: 40 | Loss: 7.8564\n",
      "Epoch 198 | Batch: 41 | Loss: 7.0703\n",
      "Epoch 198 | Batch: 42 | Loss: 8.8912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198 | Batch: 43 | Loss: 6.8753\n",
      "Epoch 198 | Batch: 44 | Loss: 12.3578\n",
      "Epoch 198 | Batch: 45 | Loss: 15.6697\n",
      "Epoch 198 | Batch: 46 | Loss: 8.2134\n",
      "Epoch 198 | Batch: 47 | Loss: 13.3058\n",
      "Epoch 198 | Batch: 48 | Loss: 9.4892\n",
      "Mean 9.372293437520662\n",
      "Epoch 199 | Batch: 1 | Loss: 8.4735\n",
      "Epoch 199 | Batch: 2 | Loss: 5.8246\n",
      "Epoch 199 | Batch: 3 | Loss: 8.5781\n",
      "Epoch 199 | Batch: 4 | Loss: 6.1993\n",
      "Epoch 199 | Batch: 5 | Loss: 1.9054\n",
      "Epoch 199 | Batch: 6 | Loss: 5.8877\n",
      "Epoch 199 | Batch: 7 | Loss: 6.8204\n",
      "Epoch 199 | Batch: 8 | Loss: 6.6202\n",
      "Epoch 199 | Batch: 9 | Loss: 11.2828\n",
      "Epoch 199 | Batch: 10 | Loss: 17.1008\n",
      "Epoch 199 | Batch: 11 | Loss: 36.7273\n",
      "Epoch 199 | Batch: 12 | Loss: 13.8306\n",
      "Epoch 199 | Batch: 13 | Loss: 22.0630\n",
      "Epoch 199 | Batch: 14 | Loss: 20.7525\n",
      "Epoch 199 | Batch: 15 | Loss: 16.8241\n",
      "Epoch 199 | Batch: 16 | Loss: 10.8058\n",
      "Epoch 199 | Batch: 17 | Loss: 10.4935\n",
      "Epoch 199 | Batch: 18 | Loss: 10.9297\n",
      "Epoch 199 | Batch: 19 | Loss: 9.1650\n",
      "Epoch 199 | Batch: 20 | Loss: 7.9258\n",
      "Epoch 199 | Batch: 21 | Loss: 5.1465\n",
      "Epoch 199 | Batch: 22 | Loss: 6.9512\n",
      "Epoch 199 | Batch: 23 | Loss: 4.7192\n",
      "Epoch 199 | Batch: 24 | Loss: 8.5376\n",
      "Epoch 199 | Batch: 25 | Loss: 7.6248\n",
      "Epoch 199 | Batch: 26 | Loss: 6.3551\n",
      "Epoch 199 | Batch: 27 | Loss: 6.0693\n",
      "Epoch 199 | Batch: 28 | Loss: 12.3696\n",
      "Epoch 199 | Batch: 29 | Loss: 7.6880\n",
      "Epoch 199 | Batch: 30 | Loss: 7.9599\n",
      "Epoch 199 | Batch: 31 | Loss: 12.4168\n",
      "Epoch 199 | Batch: 32 | Loss: 4.9928\n",
      "Epoch 199 | Batch: 33 | Loss: 4.8676\n",
      "Epoch 199 | Batch: 34 | Loss: 5.9246\n",
      "Epoch 199 | Batch: 35 | Loss: 8.1019\n",
      "Epoch 199 | Batch: 36 | Loss: 5.0817\n",
      "Epoch 199 | Batch: 37 | Loss: 9.3949\n",
      "Epoch 199 | Batch: 38 | Loss: 6.2675\n",
      "Epoch 199 | Batch: 39 | Loss: 6.4144\n",
      "Epoch 199 | Batch: 40 | Loss: 9.9393\n",
      "Epoch 199 | Batch: 41 | Loss: 11.9228\n",
      "Epoch 199 | Batch: 42 | Loss: 7.3781\n",
      "Epoch 199 | Batch: 43 | Loss: 8.7297\n",
      "Epoch 199 | Batch: 44 | Loss: 11.4269\n",
      "Epoch 199 | Batch: 45 | Loss: 5.7039\n",
      "Epoch 199 | Batch: 46 | Loss: 6.2321\n",
      "Epoch 199 | Batch: 47 | Loss: 7.2086\n",
      "Epoch 199 | Batch: 48 | Loss: 3.7759\n",
      "Mean 9.321058253447214\n",
      "Epoch 200 | Batch: 1 | Loss: 10.3992\n",
      "Epoch 200 | Batch: 2 | Loss: 11.2019\n",
      "Epoch 200 | Batch: 3 | Loss: 9.7658\n",
      "Epoch 200 | Batch: 4 | Loss: 4.3585\n",
      "Epoch 200 | Batch: 5 | Loss: 6.6531\n",
      "Epoch 200 | Batch: 6 | Loss: 7.4983\n",
      "Epoch 200 | Batch: 7 | Loss: 3.5938\n",
      "Epoch 200 | Batch: 8 | Loss: 14.0951\n",
      "Epoch 200 | Batch: 9 | Loss: 18.3738\n",
      "Epoch 200 | Batch: 10 | Loss: 19.7533\n",
      "Epoch 200 | Batch: 11 | Loss: 9.4927\n",
      "Epoch 200 | Batch: 12 | Loss: 10.9741\n",
      "Epoch 200 | Batch: 13 | Loss: 9.4490\n",
      "Epoch 200 | Batch: 14 | Loss: 10.1788\n",
      "Epoch 200 | Batch: 15 | Loss: 5.5723\n",
      "Epoch 200 | Batch: 16 | Loss: 9.5698\n",
      "Epoch 200 | Batch: 17 | Loss: 7.3316\n",
      "Epoch 200 | Batch: 18 | Loss: 10.0825\n",
      "Epoch 200 | Batch: 19 | Loss: 13.7569\n",
      "Epoch 200 | Batch: 20 | Loss: 9.1114\n",
      "Epoch 200 | Batch: 21 | Loss: 12.4250\n",
      "Epoch 200 | Batch: 22 | Loss: 16.1510\n",
      "Epoch 200 | Batch: 23 | Loss: 16.8821\n",
      "Epoch 200 | Batch: 24 | Loss: 6.2437\n",
      "Epoch 200 | Batch: 25 | Loss: 5.2152\n",
      "Epoch 200 | Batch: 26 | Loss: 10.2519\n",
      "Epoch 200 | Batch: 27 | Loss: 6.8028\n",
      "Epoch 200 | Batch: 28 | Loss: 3.7944\n",
      "Epoch 200 | Batch: 29 | Loss: 10.8044\n",
      "Epoch 200 | Batch: 30 | Loss: 4.6579\n",
      "Epoch 200 | Batch: 31 | Loss: 24.7901\n",
      "Epoch 200 | Batch: 32 | Loss: 12.8141\n",
      "Epoch 200 | Batch: 33 | Loss: 2.7276\n",
      "Epoch 200 | Batch: 34 | Loss: 5.2494\n",
      "Epoch 200 | Batch: 35 | Loss: 9.6938\n",
      "Epoch 200 | Batch: 36 | Loss: 7.4814\n",
      "Epoch 200 | Batch: 37 | Loss: 2.8699\n",
      "Epoch 200 | Batch: 38 | Loss: 7.1981\n",
      "Epoch 200 | Batch: 39 | Loss: 7.1129\n",
      "Epoch 200 | Batch: 40 | Loss: 7.1058\n",
      "Epoch 200 | Batch: 41 | Loss: 4.2063\n",
      "Epoch 200 | Batch: 42 | Loss: 9.9764\n",
      "Epoch 200 | Batch: 43 | Loss: 14.7135\n",
      "Epoch 200 | Batch: 44 | Loss: 9.0678\n",
      "Epoch 200 | Batch: 45 | Loss: 7.0584\n",
      "Epoch 200 | Batch: 46 | Loss: 8.3044\n",
      "Epoch 200 | Batch: 47 | Loss: 10.0934\n",
      "Epoch 200 | Batch: 48 | Loss: 2.5627\n",
      "Mean 9.322209392984709\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(200):\n",
    "  #Aplicăm modificarea de learning rate per epocă, deci va trebui să avem un istoric al loss-ului\n",
    "  losses=[]\n",
    "  for i, data in enumerate(train_loader, 0):\n",
    "      # get the inputs\n",
    "      inputs, labels = data\n",
    "\n",
    "      # Forward pass: Compute predicted y by passing x to the model\n",
    "      y_pred = model(inputs)\n",
    "\n",
    "      # Compute and print loss\n",
    "      loss = criterion(y_pred, labels)\n",
    "      loss2 = criterion2(y_pred, labels)  \n",
    "      print(f'Epoch {epoch + 1} | Batch: {i+1} | Loss: {loss2.item():.4f}')\n",
    "      losses.append(loss2.item())\n",
    "      # Zero gradients, perform a backward pass, and update the weights.\n",
    "      optimizer.zero_grad()\n",
    "      loss2.backward()\n",
    "      optimizer.step()\n",
    "  #if epoch % 100 == 0:\n",
    "  mean_loss=sum(losses)/len(losses)\n",
    "  print(f'Mean {mean_loss}')  \n",
    "  #scheduler.step(mean_loss)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SchedulerAndSequentialDiabetesExample.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
