{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {},
    "colab_type": "code",
    "id": "tVm68ThnfSQw"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {},
    "colab_type": "code",
    "id": "DKMMh-YnfSQ_"
   },
   "outputs": [],
   "source": [
    "#matplotlib este folosit pentru a realiza grafice\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XytQWckMfSRK"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"heart.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ph1yGMQAfSRS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5E3uQ9iBjyAM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8sJigmCckL9c"
   },
   "source": [
    "pandas_profiling - o librărie ce oferă o analiză automată a setului de date: distributii, tipuri de variabile, valori și o serie intreaga de informatii aditionale.\n",
    "Pentru a o putea folosi trebuie să ștergem libraria default din colab și să instalăm o versiune specifică. \n",
    "După instalare trebuie să facem restart la runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6qOnV1ewYzpu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping pandas-profiling as it is not installed.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f7NHkKyBXOX8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas-profiling==2.7.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/8a/25da481171f4912e2515a76fe31b7a4f036a443b8858b244ef7daaffd5b6/pandas_profiling-2.7.1-py2.py3-none-any.whl (252kB)\n",
      "\u001b[K     |████████████████████████████████| 256kB 1.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting visions[type_image_path]==0.4.1 (from pandas-profiling==2.7.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/fe/7614dec3db3f20882ff12dae0a58b579e97b590f2994ce9c953fe179d512/visions-0.4.1-py3-none-any.whl (58kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 2.2MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib in /home/ioana/anaconda3/lib/python3.7/site-packages (from pandas-profiling==2.7.1) (0.13.2)\n",
      "Collecting confuse>=1.0.0 (from pandas-profiling==2.7.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/64/85dbcea372efee5cba13eaa10a3bfa7019b8fe0c3c8314d8e189116e477a/confuse-1.1.0.tar.gz\n",
      "Requirement already satisfied: numpy>=1.16.0 in /home/ioana/anaconda3/lib/python3.7/site-packages (from pandas-profiling==2.7.1) (1.17.2)\n",
      "Collecting htmlmin>=0.1.12 (from pandas-profiling==2.7.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/b3/e7/fcd59e12169de19f0131ff2812077f964c6b960e7c09804d30a7bf2ab461/htmlmin-0.1.12.tar.gz\n",
      "Collecting astropy>=4.0 (from pandas-profiling==2.7.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/1b/80d536d16030f380fcd4bad5c38696f30476046d9f4d4813662e7f54ab7b/astropy-4.0.1.post1-cp37-cp37m-manylinux1_x86_64.whl (6.5MB)\n",
      "\u001b[K     |████████████████████████████████| 6.5MB 20.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3 (from pandas-profiling==2.7.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/6a/94b219b8ea0f2d580169e85ed1edc0163743f55aaeca8a44c2e8fc1e344e/pandas-1.0.3-cp37-cp37m-manylinux1_x86_64.whl (10.0MB)\n",
      "\u001b[K     |████████████████████████████████| 10.0MB 33.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=1.4.1 (from pandas-profiling==2.7.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/82/c1fe128f3526b128cfd185580ba40d01371c5d299fcf7f77968e22dfcc2e/scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1MB)\n",
      "\u001b[K     |████████████████████████████████| 26.1MB 63.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib>=3.2.0 (from pandas-profiling==2.7.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/c2/71fcf957710f3ba1f09088b35776a799ba7dd95f7c2b195ec800933b276b/matplotlib-3.2.1-cp37-cp37m-manylinux1_x86_64.whl (12.4MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4MB 57.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting missingno>=0.4.2 (from pandas-profiling==2.7.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/2b/de/6e4dd6d720c49939544352155dc06a08c9f7e4271aa631a559dfbeaaf9d4/missingno-0.4.2-py3-none-any.whl\n",
      "Collecting tangled-up-in-unicode>=0.0.4 (from pandas-profiling==2.7.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/e2/e588ab9298d4989ce7fdb2b97d18aac878d99dbdc379a4476a09d9271b68/tangled_up_in_unicode-0.0.6-py3-none-any.whl (3.1MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1MB 11.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests>=2.23.0 (from pandas-profiling==2.7.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/70/1935c770cb3be6e3a8b78ced23d7e0f3b187f5cbfab4749523ed65d7c9b1/requests-2.23.0-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 34.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4.43.0 (from pandas-profiling==2.7.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/40/058b12e8ba10e35f89c9b1fdfc2d4c7f8c05947df2d5eb3c7b258019fda0/tqdm-4.46.0-py2.py3-none-any.whl (63kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 37.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ipywidgets>=7.5.1 in /home/ioana/anaconda3/lib/python3.7/site-packages (from pandas-profiling==2.7.1) (7.5.1)\n",
      "Collecting phik>=0.9.10 (from pandas-profiling==2.7.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/86/572cecb01a104e0a12c002b0de74340fc6358aefa90cce39d5e2ffca2980/phik-0.9.12-py3-none-any.whl (610kB)\n",
      "\u001b[K     |████████████████████████████████| 614kB 45.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jinja2>=2.11.1 (from pandas-profiling==2.7.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/9e/f663a2aa66a09d838042ae1a2c5659828bb9b41ea3a6efa20a20fd92b121/Jinja2-2.11.2-py2.py3-none-any.whl (125kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 37.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx>=2.4 (from visions[type_image_path]==0.4.1->pandas-profiling==2.7.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/8f/dd6a8e85946def36e4f2c69c84219af0fa5e832b018c970e92f2ad337e45/networkx-2.4-py3-none-any.whl (1.6MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6MB 64.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting attrs>=19.3.0 (from visions[type_image_path]==0.4.1->pandas-profiling==2.7.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/db/4313ab3be961f7a763066401fb77f7748373b6094076ae2bda2806988af6/attrs-19.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: Pillow; extra == \"type_image_path\" in /home/ioana/anaconda3/lib/python3.7/site-packages (from visions[type_image_path]==0.4.1->pandas-profiling==2.7.1) (6.2.0)\n",
      "Collecting imagehash; extra == \"type_image_path\" (from visions[type_image_path]==0.4.1->pandas-profiling==2.7.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/5d/cc81830be3c4705a46cdbca74439b67f1017881383ba0127c41c4cecb7b3/ImageHash-4.1.0.tar.gz (291kB)\n",
      "\u001b[K     |████████████████████████████████| 296kB 16.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /home/ioana/anaconda3/lib/python3.7/site-packages (from confuse>=1.0.0->pandas-profiling==2.7.1) (5.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/ioana/anaconda3/lib/python3.7/site-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3->pandas-profiling==2.7.1) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ioana/anaconda3/lib/python3.7/site-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3->pandas-profiling==2.7.1) (2019.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ioana/anaconda3/lib/python3.7/site-packages (from matplotlib>=3.2.0->pandas-profiling==2.7.1) (2.4.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ioana/anaconda3/lib/python3.7/site-packages (from matplotlib>=3.2.0->pandas-profiling==2.7.1) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ioana/anaconda3/lib/python3.7/site-packages (from matplotlib>=3.2.0->pandas-profiling==2.7.1) (0.10.0)\n",
      "Requirement already satisfied: seaborn in /home/ioana/anaconda3/lib/python3.7/site-packages (from missingno>=0.4.2->pandas-profiling==2.7.1) (0.9.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ioana/anaconda3/lib/python3.7/site-packages (from requests>=2.23.0->pandas-profiling==2.7.1) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ioana/anaconda3/lib/python3.7/site-packages (from requests>=2.23.0->pandas-profiling==2.7.1) (1.24.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ioana/anaconda3/lib/python3.7/site-packages (from requests>=2.23.0->pandas-profiling==2.7.1) (2.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ioana/anaconda3/lib/python3.7/site-packages (from requests>=2.23.0->pandas-profiling==2.7.1) (3.0.4)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /home/ioana/anaconda3/lib/python3.7/site-packages (from ipywidgets>=7.5.1->pandas-profiling==2.7.1) (7.8.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /home/ioana/anaconda3/lib/python3.7/site-packages (from ipywidgets>=7.5.1->pandas-profiling==2.7.1) (3.5.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/ioana/anaconda3/lib/python3.7/site-packages (from ipywidgets>=7.5.1->pandas-profiling==2.7.1) (4.3.3)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /home/ioana/anaconda3/lib/python3.7/site-packages (from ipywidgets>=7.5.1->pandas-profiling==2.7.1) (4.4.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /home/ioana/anaconda3/lib/python3.7/site-packages (from ipywidgets>=7.5.1->pandas-profiling==2.7.1) (5.1.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numba>=0.38.1 in /home/ioana/anaconda3/lib/python3.7/site-packages (from phik>=0.9.10->pandas-profiling==2.7.1) (0.45.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/ioana/anaconda3/lib/python3.7/site-packages (from jinja2>=2.11.1->pandas-profiling==2.7.1) (1.1.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/ioana/anaconda3/lib/python3.7/site-packages (from networkx>=2.4->visions[type_image_path]==0.4.1->pandas-profiling==2.7.1) (4.4.0)\n",
      "Requirement already satisfied: six in /home/ioana/anaconda3/lib/python3.7/site-packages (from imagehash; extra == \"type_image_path\"->visions[type_image_path]==0.4.1->pandas-profiling==2.7.1) (1.12.0)\n",
      "Requirement already satisfied: PyWavelets in /home/ioana/anaconda3/lib/python3.7/site-packages (from imagehash; extra == \"type_image_path\"->visions[type_image_path]==0.4.1->pandas-profiling==2.7.1) (1.0.3)\n",
      "Requirement already satisfied: setuptools in /home/ioana/anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=3.2.0->pandas-profiling==2.7.1) (41.4.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /home/ioana/anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.15.1)\n",
      "Requirement already satisfied: pygments in /home/ioana/anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (2.4.2)\n",
      "Requirement already satisfied: backcall in /home/ioana/anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.1.0)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /home/ioana/anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (4.7.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /home/ioana/anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (2.0.10)\n",
      "Requirement already satisfied: pickleshare in /home/ioana/anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.7.5)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /home/ioana/anaconda3/lib/python3.7/site-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (6.0.1)\n",
      "Requirement already satisfied: ipython-genutils in /home/ioana/anaconda3/lib/python3.7/site-packages (from traitlets>=4.3.1->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.2.0)\n",
      "Requirement already satisfied: jupyter-core in /home/ioana/anaconda3/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (4.5.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /home/ioana/anaconda3/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (3.0.2)\n",
      "Requirement already satisfied: tornado>=4.2 in /home/ioana/anaconda3/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (6.0.3)\n",
      "Requirement already satisfied: jupyter-client in /home/ioana/anaconda3/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (5.3.3)\n",
      "Requirement already satisfied: llvmlite>=0.29.0dev0 in /home/ioana/anaconda3/lib/python3.7/site-packages (from numba>=0.38.1->phik>=0.9.10->pandas-profiling==2.7.1) (0.29.0)\n",
      "Requirement already satisfied: parso>=0.5.0 in /home/ioana/anaconda3/lib/python3.7/site-packages (from jedi>=0.10->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.5.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ioana/anaconda3/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.6.0)\n",
      "Requirement already satisfied: wcwidth in /home/ioana/anaconda3/lib/python3.7/site-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.1.7)\n",
      "Requirement already satisfied: nbconvert in /home/ioana/anaconda3/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (5.6.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /home/ioana/anaconda3/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.8.2)\n",
      "Requirement already satisfied: pyzmq>=17 in /home/ioana/anaconda3/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (18.1.0)\n",
      "Requirement already satisfied: Send2Trash in /home/ioana/anaconda3/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (1.5.0)\n",
      "Requirement already satisfied: prometheus-client in /home/ioana/anaconda3/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.7.1)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/ioana/anaconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.15.4)\n",
      "Requirement already satisfied: bleach in /home/ioana/anaconda3/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (3.1.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /home/ioana/anaconda3/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.3)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /home/ioana/anaconda3/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.8.4)\n",
      "Requirement already satisfied: testpath in /home/ioana/anaconda3/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.4.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/ioana/anaconda3/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (1.4.2)\n",
      "Requirement already satisfied: defusedxml in /home/ioana/anaconda3/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.6.0)\n",
      "Requirement already satisfied: webencodings in /home/ioana/anaconda3/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.5.1)\n",
      "Building wheels for collected packages: confuse, htmlmin, imagehash\n",
      "  Building wheel for confuse (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for confuse: filename=confuse-1.1.0-cp37-none-any.whl size=17573 sha256=26006b6eff67fcdc3217dac4e9b4b810ae19639fcfa2378e41aecda8b2a3af4d\n",
      "  Stored in directory: /home/ioana/.cache/pip/wheels/f6/8b/23/41a1b516f6d8d4cc81f5bdb55394a47cdbe9659c53668d3c9e\n",
      "  Building wheel for htmlmin (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for htmlmin: filename=htmlmin-0.1.12-cp37-none-any.whl size=27086 sha256=77ec1c820b7eb73a363093a4c9255b377652616c0057664a71c13fb6020fae22\n",
      "  Stored in directory: /home/ioana/.cache/pip/wheels/43/07/ac/7c5a9d708d65247ac1f94066cf1db075540b85716c30255459\n",
      "  Building wheel for imagehash (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for imagehash: filename=ImageHash-4.1.0-py2.py3-none-any.whl size=291990 sha256=868d33f23d00dba3d50e13225d3eb555b9983f9261a8dc5a0196baa63f1bf88d\n",
      "  Stored in directory: /home/ioana/.cache/pip/wheels/07/1c/dc/6831446f09feb8cc199ec73a0f2f0703253f6ae013a22f4be9\n",
      "Successfully built confuse htmlmin imagehash\n",
      "\u001b[31mERROR: phik 0.9.12 has requirement joblib>=0.14.1, but you'll have joblib 0.13.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: networkx, attrs, tangled-up-in-unicode, pandas, scipy, imagehash, visions, confuse, htmlmin, astropy, matplotlib, missingno, requests, tqdm, phik, jinja2, pandas-profiling\n",
      "  Found existing installation: networkx 2.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Uninstalling networkx-2.3:\n",
      "      Successfully uninstalled networkx-2.3\n",
      "  Found existing installation: attrs 19.2.0\n",
      "    Uninstalling attrs-19.2.0:\n",
      "      Successfully uninstalled attrs-19.2.0\n",
      "  Found existing installation: pandas 0.25.1\n",
      "    Uninstalling pandas-0.25.1:\n",
      "      Successfully uninstalled pandas-0.25.1\n",
      "  Found existing installation: scipy 1.3.1\n",
      "    Uninstalling scipy-1.3.1:\n",
      "      Successfully uninstalled scipy-1.3.1\n",
      "  Found existing installation: astropy 3.2.2\n",
      "    Uninstalling astropy-3.2.2:\n",
      "      Successfully uninstalled astropy-3.2.2\n",
      "  Found existing installation: matplotlib 3.1.1\n",
      "    Uninstalling matplotlib-3.1.1:\n",
      "      Successfully uninstalled matplotlib-3.1.1\n",
      "  Found existing installation: requests 2.22.0\n",
      "    Uninstalling requests-2.22.0:\n",
      "      Successfully uninstalled requests-2.22.0\n",
      "  Found existing installation: tqdm 4.36.1\n",
      "    Uninstalling tqdm-4.36.1:\n",
      "      Successfully uninstalled tqdm-4.36.1\n",
      "  Found existing installation: Jinja2 2.10.3\n",
      "    Uninstalling Jinja2-2.10.3:\n",
      "      Successfully uninstalled Jinja2-2.10.3\n",
      "Successfully installed astropy-4.0.1.post1 attrs-19.3.0 confuse-1.1.0 htmlmin-0.1.12 imagehash-4.1.0 jinja2-2.11.2 matplotlib-3.2.1 missingno-0.4.2 networkx-2.4 pandas-1.0.3 pandas-profiling-2.7.1 phik-0.9.12 requests-2.23.0 scipy-1.4.1 tangled-up-in-unicode-0.0.6 tqdm-4.46.0 visions-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas-profiling==2.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ntTgqB42XRSl"
   },
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C2hBghqyXR9G"
   },
   "outputs": [],
   "source": [
    "prof=ProfileReport(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E3bhlEFcWvKN"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0150812d5e439cb3cce3debb3709ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Summarize dataset', max=26.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0650e3e6688c422b9b610616c969ea72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Generate report structure', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid RGBA argument: '#000'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-41b3a608381c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Statisticile pot fi salvate În format html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'output.html'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas_profiling/profile_report.py\u001b[0m in \u001b[0;36mto_file\u001b[0;34m(self, output_file, silent)\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\".html\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0msuffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas_profiling/profile_report.py\u001b[0m in \u001b[0;36mto_html\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \"\"\"\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas_profiling/profile_report.py\u001b[0m in \u001b[0;36mhtml\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_html\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_html\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas_profiling/profile_report.py\u001b[0m in \u001b[0;36m_render_html\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas_profiling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpresentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflavours\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTMLReport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mdisable_progress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"progress_bar\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas_profiling/profile_report.py\u001b[0m in \u001b[0;36mreport\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_report_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas_profiling/report/structure/report.py\u001b[0m in \u001b[0;36mget_report_structure\u001b[0;34m(summary)\u001b[0m\n\u001b[1;32m    261\u001b[0m         ]\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_correlation_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0msection_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas_profiling/report/structure/correlations.py\u001b[0m in \u001b[0;36mget_correlation_items\u001b[0;34m(summary)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         diagram = Image(\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrelation_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mimage_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0malt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas_profiling/visualisation/plot.py\u001b[0m in \u001b[0;36mcorrelation_matrix\u001b[0;34m(data, vmin)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvmin\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cmap_half\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mcmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_bad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmap_bad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mset_bad\u001b[0;34m(self, color, alpha)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mxa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"f\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mxa\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0;31m# Negative values are out of range, but astype(int) would truncate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m             \u001b[0;31m# them towards zero.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0mxa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxa\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mto_rgba\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_nth_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mprop_cycler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'axes.prop_cycle'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprop_cycler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mby_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'color'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36m_to_rgba_no_colorcycle\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    231\u001b[0m             return (tuple(int(n, 16) / 255\n\u001b[1;32m    232\u001b[0m                           for n in [c[1]*2, c[2]*2, c[3]*2])\n\u001b[0;32m--> 233\u001b[0;31m                     + (alpha if alpha is not None else 1.,))\n\u001b[0m\u001b[1;32m    234\u001b[0m         \u001b[0;31m# hex color with alpha in #rrggbbaa format.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\A#[a-fA-F0-9]{8}\\Z\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid RGBA argument: '#000'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Statisticile pot fi salvate În format html\n",
    "prof.to_file(output_file='output.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5okLNZ-NV4HG"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e631be2180f74e0ab2726a1bd7a0af81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Generate report structure', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid RGBA argument: '#000'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas_profiling/profile_report.py\u001b[0m in \u001b[0;36m_repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_repr_html_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;34m\"\"\"The ipython notebook widgets user interface gets called by the jupyter notebook.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_notebook_iframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas_profiling/profile_report.py\u001b[0m in \u001b[0;36mto_notebook_iframe\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_notebook_iframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_widgets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas_profiling/report/presentation/flavours/widget/notebook.py\u001b[0m in \u001b[0;36mget_notebook_iframe\u001b[0;34m(profile)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_notebook_iframe_src\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mattribute\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"srcdoc\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_notebook_iframe_srcdoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas_profiling/report/presentation/flavours/widget/notebook.py\u001b[0m in \u001b[0;36mget_notebook_iframe_srcdoc\u001b[0;34m(profile)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"notebook\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"iframe\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"width\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"notebook\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"iframe\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"height\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mescape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0miframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'<iframe width=\"{width}\" height=\"{height}\" srcdoc=\"{src}\" frameborder=\"0\" allowfullscreen></iframe>'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas_profiling/profile_report.py\u001b[0m in \u001b[0;36mto_html\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \"\"\"\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas_profiling/profile_report.py\u001b[0m in \u001b[0;36mhtml\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_html\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_html\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas_profiling/profile_report.py\u001b[0m in \u001b[0;36m_render_html\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas_profiling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpresentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflavours\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTMLReport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mdisable_progress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"progress_bar\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas_profiling/profile_report.py\u001b[0m in \u001b[0;36mreport\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_report_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas_profiling/report/structure/report.py\u001b[0m in \u001b[0;36mget_report_structure\u001b[0;34m(summary)\u001b[0m\n\u001b[1;32m    261\u001b[0m         ]\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_correlation_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0msection_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas_profiling/report/structure/correlations.py\u001b[0m in \u001b[0;36mget_correlation_items\u001b[0;34m(summary)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         diagram = Image(\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrelation_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mimage_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0malt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas_profiling/visualisation/plot.py\u001b[0m in \u001b[0;36mcorrelation_matrix\u001b[0;34m(data, vmin)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvmin\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cmap_half\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mcmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_bad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmap_bad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mset_bad\u001b[0;34m(self, color, alpha)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mxa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"f\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mxa\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0;31m# Negative values are out of range, but astype(int) would truncate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m             \u001b[0;31m# them towards zero.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0mxa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxa\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mto_rgba\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_nth_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mprop_cycler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'axes.prop_cycle'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprop_cycler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mby_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'color'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36m_to_rgba_no_colorcycle\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    231\u001b[0m             return (tuple(int(n, 16) / 255\n\u001b[1;32m    232\u001b[0m                           for n in [c[1]*2, c[2]*2, c[3]*2])\n\u001b[0;32m--> 233\u001b[0;31m                     + (alpha if alpha is not None else 1.,))\n\u001b[0m\u001b[1;32m    234\u001b[0m         \u001b[0;31m# hex color with alpha in #rrggbbaa format.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\A#[a-fA-F0-9]{8}\\Z\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid RGBA argument: '#000'"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFRCAYAAABdSSXIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAR7ElEQVR4nO3dUWiV993A8d95a1O8sK6VJceLIAWlDAy1FwULq8OkITRpmE5b2optoaEgtHS0aFewMlyxY6zdcqUEIQPtBp2wlpnSbUSWjNbpzSS1DqzF07l2OWWrQ60j4vF5L/ouezPtzulMYn/6+Vz5+Pw5/vgb/fKcc3ieUlEURQAAafzPlR4AAPhixBsAkhFvAEhGvAEgGfEGgGTEGwCSqRvv5557Lu6888649957L3m+KIp44YUXorOzM3p7e+Pdd9+d9iEBgH+pG+9vfetbsXPnzs89Pzo6GpVKJX7961/H9773vfjud787nfMBAP+mbrzvuOOOmD9//ueeHx4ejlWrVkWpVIply5bFqVOn4uOPP57WIQGAf7nsz7yr1WqUy+XJ43K5HNVq9XJfFgD4HJcd70vdXbVUKl3uywIAn+Oy410ul2N8fHzyeHx8PJqbmy/3ZQGAz3HZ8W5vb4/XXnstiqKIQ4cOxbx588QbAGZQqd5TxZ5++uk4ePBgnDx5MhYsWBBPPvlknD9/PiIiHnzwwSiKIrZu3Rq/+93vYu7cubFt27Zoa2ubleEB4FpUN94AwJeLO6wBQDLiDQDJiDcAJCPeAJCMeANAMuINAMmINwAkI94AkIx4A0Ay4g0AyYg3ACQj3gCQjHgDQDLiDQDJiDcAJCPeAJCMeANAMuINAMmINwAkI94AkIx4A0Ay4g0AyYg3ACQj3gCQjHgDQDLiDQDJiDcAJCPeAJCMeANAMuINAMmINwAkI94AkIx4A0Ay4g0AyYg3ACQj3gCQjHgDQDLiDQDJiDcAJCPeAJCMeANAMuINAMmINwAkI94AkIx4A0Ay4g0AyYg3ACQj3gCQjHgDQDLiDQDJiDcAJNNQvEdHR6Orqys6OztjYGDgovMfffRRrF+/PlatWhW9vb0xMjIy7YMCAJ8pFUVR/KcFtVoturq6YnBwMFpaWmLt2rXx8ssvx+LFiyfXPP/88/G1r30tHnrooTh27Fg8/vjjsW/fvhkfHgCuRXWvvMfGxmLRokXR2toaTU1N0dPTE8PDw1PWlEqlOHPmTEREnD59Opqbm2dmWgAg5tRbUK1Wo1wuTx63tLTE2NjYlDVPPPFEPPbYY7F79+74xz/+EYODg9M/KQAQEQ1ceV/qXfVSqTTleGhoKFavXh2jo6MxMDAQmzZtigsXLkzflADApLrxLpfLMT4+PnlcrVYvelt8z549cc8990RExO233x4TExNx8uTJaR4VAIhoIN5tbW1RqVTixIkTce7cuRgaGor29vYpaxYuXBj79++PiIj3338/JiYm4uabb56ZiQHgGlf32+YRESMjI7Ft27ao1WqxZs2a2LBhQ/T398fSpUujo6Mjjh07Fps3b46zZ89GqVSKjRs3xte//vXZmB8ArjkNxRsA+PJwhzUASEa8ASAZ8QaAZMQbAJIRbwBIRrwBIBnxBoBkxBsAkhFvAEhGvAEgGfEGgGTEGwCSEW8ASEa8ASAZ8QaAZMQbAJIRbwBIRrwBIBnxBoBkxBsAkhFvAEhGvAEgGfEGgGTEGwCSEW8ASEa8ASAZ8QaAZMQbAJIRbwBIRrwBIBnxBoBkxBsAkhFvAEhGvAEgGfEGgGTEGwCSEW8ASEa8ASAZ8QaAZMQbAJIRbwBIRrwBIBnxBoBkxBsAkhFvAEhGvAEgGfEGgGTEGwCSEW8ASEa8ASAZ8QaAZBqK9+joaHR1dUVnZ2cMDAxccs0bb7wR3d3d0dPTE88888y0DgkA/MucegtqtVps3bo1BgcHo6WlJdauXRvt7e2xePHiyTWVSiUGBgbiZz/7WcyfPz/+9re/zejQAHAtq3vlPTY2FosWLYrW1tZoamqKnp6eGB4enrLm1VdfjXXr1sX8+fMjImLBggUzMy0AUD/e1Wo1yuXy5HFLS0tUq9UpayqVShw/fjweeOCBuP/++2N0dHT6JwUAIqKBt82Lorjo90ql0pTjWq0WH3zwQezatSvGx8dj3bp1sXfv3rjxxhunb1IAICIauPIul8sxPj4+eVytVqO5uXnKmpaWlujo6Ijrr78+Wltb45ZbbolKpTLtwwIADcS7ra0tKpVKnDhxIs6dOxdDQ0PR3t4+Zc3dd98dBw4ciIiITz75JCqVSrS2ts7MxABwjav7tvmcOXNiy5Yt0dfXF7VaLdasWRNLliyJ/v7+WLp0aXR0dMRdd90Vb731VnR3d8d1110XmzZtiptuumk25geAa06puNSH2gDAl5Y7rAFAMuINAMmINwAkI94AkIx4A0Ay4g0AyYg3ACQj3gCQjHgDQDLiDQDJiDcAJCPeAJCMeANAMuINAMmINwAkI94AkIx4A0Ay4g0AyYg3ACQj3gCQjHgDQDLiDQDJiDcAJCPeAJCMeANAMuINAMmINwAkI94AkIx4A0Ay4g0AyYg3ACQj3gCQjHgDQDLiDQDJiDcAJCPeAJCMeANAMuINAMmINwAkI94AkIx4A0Ay4g0AyYg3ACQj3gCQjHgDQDLiDQDJiDcAJCPeAJCMeANAMuINAMmINwAk01C8R0dHo6urKzo7O2NgYOBz17355ptx6623xjvvvDNtAwIAU9WNd61Wi61bt8bOnTtjaGgo9u7dG8eOHbto3ZkzZ2LXrl1x2223zcigAMBn6sZ7bGwsFi1aFK2trdHU1BQ9PT0xPDx80br+/v7o6+uLG264YUYGBQA+Uzfe1Wo1yuXy5HFLS0tUq9Upa44cORLj4+OxcuXK6Z8QAJiibryLorjo90ql0uSvL1y4EC+++GI8++yz0zsZAHBJdeNdLpdjfHx88rharUZzc/Pk8aeffhpHjx6Nhx9+ONrb2+PQoUOxYcMGX1oDgBkyp96Ctra2qFQqceLEiWhpaYmhoaF46aWXJs/PmzcvDhw4MHm8fv362LRpU7S1tc3MxABwjasb7zlz5sSWLVuir68varVarFmzJpYsWRL9/f2xdOnS6OjomI05AYD/Uyou9aE2APCl5Q5rAJCMeANAMuINAMmINwAkI94AkIx4A0Ay4g0AyYg3ACQj3gCQjHgDQDLiDQDJiDcAJCPeAJCMeANAMuINAMmINwAkI94AkIx4A0Ay4g0AyYg3ACQj3gCQjHgDQDLiDQDJiDcAJCPeAJCMeANAMuINAMmINwAkI94AkIx4A0Ay4g0AyYg3ACQj3gCQjHgDQDLiDQDJiDcAJCPeAJCMeANAMuINAMmINwAkI94AkIx4A0Ay4g0AyYg3ACQj3gCQjHgDQDLiDQDJiDcAJCPeAJCMeANAMg3Fe3R0NLq6uqKzszMGBgYuOj84OBjd3d3R29sbjzzySHz44YfTPigA8JlSURTFf1pQq9Wiq6srBgcHo6WlJdauXRsvv/xyLF68eHLN73//+7jtttti7ty58dOf/jQOHjwYP/7xj2d8eAC4FtW98h4bG4tFixZFa2trNDU1RU9PTwwPD09Zs3z58pg7d25ERCxbtizGx8dnZloAoH68q9VqlMvlyeOWlpaoVqufu37Pnj2xYsWK6ZkOALjInHoLLvWueqlUuuTa119/PQ4fPhy7d+++/MkAgEuqG+9yuTzlbfBqtRrNzc0XrXv77bdjx44dsXv37mhqapreKQGASXXfNm9ra4tKpRInTpyIc+fOxdDQULS3t09Zc+TIkdiyZUts3749FixYMGPDAgANfNs8ImJkZCS2bdsWtVot1qxZExs2bIj+/v5YunRpdHR0xKOPPhpHjx6Nr371qxERsXDhwtixY8eMDw8A16KG4g0AfHm4wxoAJCPeAJCMeANAMuINAMmINwAkI94AkIx4A0Ay4g0AyYg3ACQj3gCQjHgDQDLiDQDJiDcAJCPeAJCMeANAMuINAMmINwAkI94AkIx4A0Ay4g0AyYg3ACQj3gCQjHgDQDLiDQDJiDcAJCPeAJCMeANAMuINAMmINwAkI94AkIx4A0Ay4g0AyYg3ACQj3gCQjHgDQDLiDQDJiDcAJCPeAJCMeANAMuINAMmINwAkI94AkIx4A0Ay4g0AyYg3ACQj3gCQjHgDQDLiDQDJiDcAJCPeAJCMeANAMg3Fe3R0NLq6uqKzszMGBgYuOn/u3Ln49re/HZ2dnXHffffFn//852kfFAD4TN1412q12Lp1a+zcuTOGhoZi7969cezYsSlrfv7zn8eNN94Yv/nNb+LRRx+NH/7whzM2MABc6+rGe2xsLBYtWhStra3R1NQUPT09MTw8PGXNvn37YvXq1RER0dXVFfv374+iKGZmYgC4xtWNd7VajXK5PHnc0tIS1Wr1ojULFy6MiIg5c+bEvHnz4uTJk9M8KgAQ0UC8L3UFXSqVvvAaAGB61I13uVyO8fHxyeNqtRrNzc0XrfnLX/4SERHnz5+P06dPx1e+8pVpHhUAiGgg3m1tbVGpVOLEiRNx7ty5GBoaivb29ilr2tvb4xe/+EVERPzqV7+K5cuXu/IGgBlSKhr4ZtnIyEhs27YtarVarFmzJjZs2BD9/f2xdOnS6OjoiImJidi4cWP88Y9/jPnz58ePfvSjaG1tnY35AeCa01C8AYAvD3dYA4BkxBsAkhFvAEhGvAEgGfEGgGTEGwCSmdV4e7To7Ki3z4ODg9Hd3R29vb3xyCOPxIcffngFpsyt3h7/05tvvhm33nprvPPOO7M43dWhkT1+4403oru7O3p6euKZZ56Z5Qnzq7fHH330Uaxfvz5WrVoVvb29MTIycgWmzO25556LO++8M+69995Lni+KIl544YXo7OyM3t7eePfddxt74WKWnD9/vujo6Cj+9Kc/FRMTE0Vvb2/x3nvvTVmze/fu4vnnny+Koij27t1bPPXUU7M13lWjkX3ev39/cfbs2aIoiuKVV16xz19QI3tcFEVx+vTp4qGHHiruu+++Ymxs7ApMmlcje3z8+PHim9/8ZvH3v/+9KIqi+Otf/3olRk2rkT3evHlz8corrxRFURTvvfdesXLlyisxamoHDx4sDh8+XPT09Fzy/G9/+9viscceKy5cuFD84Q9/KNauXdvQ687albdHi86ORvZ5+fLlMXfu3IiIWLZs2ZR711NfI3scEdHf3x99fX1xww03XIEpc2tkj1999dVYt25dzJ8/PyIiFixYcCVGTauRPS6VSnHmzJmIiDh9+vRFz7WgvjvuuGPyZ/RShoeHY9WqVVEqlWLZsmVx6tSp+Pjjj+u+7qzF26NFZ0cj+/z/7dmzJ1asWDEbo101GtnjI0eOxPj4eKxcuXK2x7sqNLLHlUoljh8/Hg888EDcf//9MTo6OttjptbIHj/xxBPxy1/+MlasWBGPP/54bN68ebbHvOr9+99DuVz+j/9n/9OsxftSV9AeLTr9vsgevv7663H48OHo6+ub6bGuKvX2+MKFC/Hiiy/Gs88+O5tjXVUa+Tmu1WrxwQcfxK5du+Kll16KzZs3x6lTp2ZrxPQa2eOhoaFYvXp1jI6OxsDAQGzatCkuXLgwWyNeE/7b7s1avD1adHY0ss8REW+//Xbs2LEjtm/fHk1NTbM5Ynr19vjTTz+No0ePxsMPPxzt7e1x6NCh2LBhgy+tfQGN/By3tLRER0dHXH/99dHa2hq33HJLVCqVWZ40r0b2eM+ePXHPPfdERMTtt98eExMT3g2dZv/+9zA+Pt7QxxOzFm+PFp0djezzkSNHYsuWLbF9+3afE/4X6u3xvHnz4sCBA7Fv377Yt29fLFu2LLZv3x5tbW1XcOpcGvk5vvvuu+PAgQMREfHJJ59EpVLxNMMvoJE9XrhwYezfvz8iIt5///2YmJiIm2+++UqMe9Vqb2+P1157LYqiiEOHDsW8efMaivecWZjtsz9ozpzYsmVL9PX1TT5adMmSJVMeLbp27drYuHFjdHZ2Tj5alC+mkX3+wQ9+EGfPno2nnnoqIj77B7pjx44rPHkejewxl6eRPb7rrrvirbfeiu7u7rjuuuti06ZNcdNNN13p0dNoZI+/853vxObNm+MnP/lJlEql+P73v++C6gt6+umn4+DBg3Hy5MlYsWJFPPnkk3H+/PmIiHjwwQfjG9/4RoyMjERnZ2fMnTs3tm3b1tDreiQoACTjDmsAkIx4A0Ay4g0AyYg3ACQj3gCQjHgDQDLiDQDJ/C9zZ2stqKwToQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lS4D1RFvcQsk"
   },
   "outputs": [],
   "source": [
    "#Pentru seturi de date mari este indicat să folosim versiunea minimală a librăriei\n",
    "prof=ProfileReport(df,minimal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cshaUY4VcUKC"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4837eecef0c042a2a8e387d036a56182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Export report to file', max=1.0, style=ProgressStyle(desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prof.to_file(output_file='output-min.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UbnJVh7JcX1m"
   },
   "outputs": [],
   "source": [
    "prof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W_UUlJ8KfSRZ"
   },
   "outputs": [],
   "source": [
    "#Selectăm datele de intrare in retea eliminand ultima coloană din csv\n",
    "X = df.drop(\"target\", axis=1)\n",
    "#obținem etichetele pentru date salvand ultima coloana\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V-Cg9ewufSRh"
   },
   "outputs": [],
   "source": [
    "#folosim o functie din sklearn ce creaza seturi de date pentru antrenare si validare\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VzSoXozDfSRp"
   },
   "outputs": [],
   "source": [
    "#primim ca output seturile de date corespunzatoare.\n",
    "#Test size ne spune cat de mare procentual sa avem setul de validare\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oJHOFiIKjuhm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129, 242)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verificam distributia etichetelor de 0 si 1.\n",
    "#Suma etichetelor ne da numarul de intrari cu 1.\n",
    "#Ideal ar trebui sa avem o distributie echilibrata intre cele 2 valori\n",
    "sum(y_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xlqD0qTQjuj8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 61)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gNbCkEgrjumv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9rEU0qmIjupE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_2qY5O3UfSRu"
   },
   "outputs": [],
   "source": [
    "#Pentru normalizarea datelor folosim MinMaxScaler din sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4bfdh7k1s8lz"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>197</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>249</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>274</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>166</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>109</td>\n",
       "      <td>1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>244</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>154</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>212</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>140</td>\n",
       "      <td>335</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>120</td>\n",
       "      <td>231</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>182</td>\n",
       "      <td>1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>203</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>247</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>242 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "155   58    0   0       130   197    0        1      131      0      0.6   \n",
       "270   46    1   0       120   249    0        0      144      0      0.8   \n",
       "245   48    1   0       124   274    0        0      166      0      0.5   \n",
       "187   54    1   0       124   266    0        0      109      1      2.2   \n",
       "258   62    0   0       150   244    0        1      154      1      1.4   \n",
       "..   ...  ...  ..       ...   ...  ...      ...      ...    ...      ...   \n",
       "26    59    1   2       150   212    1        1      157      0      1.6   \n",
       "177   64    1   2       140   335    0        1      158      0      0.0   \n",
       "259   38    1   3       120   231    0        1      182      1      3.8   \n",
       "169   53    1   0       140   203    1        0      155      1      3.1   \n",
       "18    43    1   0       150   247    0        1      171      0      1.5   \n",
       "\n",
       "     slope  ca  thal  \n",
       "155      1   0     2  \n",
       "270      2   0     3  \n",
       "245      1   0     3  \n",
       "187      1   1     3  \n",
       "258      1   0     2  \n",
       "..     ...  ..   ...  \n",
       "26       2   0     2  \n",
       "177      2   0     2  \n",
       "259      1   0     3  \n",
       "169      0   0     3  \n",
       "18       2   0     2  \n",
       "\n",
       "[242 rows x 13 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U6UTX8sttBPn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155    1\n",
       "270    0\n",
       "245    0\n",
       "187    0\n",
       "258    0\n",
       "      ..\n",
       "26     1\n",
       "177    0\n",
       "259    0\n",
       "169    0\n",
       "18     1\n",
       "Name: target, Length: 242, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-CTwxoWZfSRy"
   },
   "outputs": [],
   "source": [
    "# Functia va translata fiecare feature in parte in intervalul (-1,1)\n",
    "# Funcția practic relizează următoarele\n",
    "# X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "# X_scaled = X_std * (max - min) + min\n",
    "\n",
    "\n",
    "sc = MinMaxScaler((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DeLHff3CfSR3"
   },
   "outputs": [],
   "source": [
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dEqnXDHmfSR9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.        , -1.        , -0.32075472, -0.05154639,\n",
       "        -1.        , -1.        ,  0.09923664, -1.        , -0.85714286,\n",
       "         0.        , -1.        ,  0.33333333],\n",
       "       [-0.45833333, -1.        , -1.        , -0.8490566 , -0.04467354,\n",
       "        -1.        , -1.        , -0.22137405, -1.        , -0.78571429,\n",
       "         0.        , -1.        ,  0.33333333],\n",
       "       [-0.41666667,  1.        , -1.        , -0.69811321, -0.41580756,\n",
       "        -1.        ,  0.        ,  0.3740458 , -1.        , -1.        ,\n",
       "         1.        , -1.        ,  1.        ],\n",
       "       [ 0.41666667, -1.        , -1.        , -0.43396226, -0.51202749,\n",
       "        -1.        ,  0.        , -0.00763359,  1.        , -1.        ,\n",
       "         0.        , -1.        ,  0.33333333],\n",
       "       [-0.375     ,  1.        ,  0.33333333, -0.32075472, -0.26460481,\n",
       "        -1.        ,  0.        ,  0.64885496,  1.        , -0.85714286,\n",
       "         1.        , -1.        ,  0.33333333],\n",
       "       [-0.41666667, -1.        , -1.        , -0.28301887,  0.47766323,\n",
       "         1.        , -1.        , -0.00763359,  1.        ,  0.07142857,\n",
       "         0.        , -1.        ,  1.        ],\n",
       "       [ 0.58333333, -1.        ,  0.33333333, -0.60377358,  2.01030928,\n",
       "        -1.        , -1.        ,  0.35877863, -1.        , -0.42857143,\n",
       "         0.        , -1.        ,  1.        ],\n",
       "       [ 0.54166667, -1.        ,  0.33333333, -0.01886792,  0.04467354,\n",
       "        -1.        , -1.        ,  0.23664122, -1.        , -1.        ,\n",
       "         0.        , -0.5       ,  0.33333333],\n",
       "       [-0.45833333,  1.        ,  1.        ,  0.01886792, -0.18900344,\n",
       "        -1.        , -1.        ,  0.63358779, -1.        , -0.71428571,\n",
       "         1.        ,  0.        ,  0.33333333],\n",
       "       [ 0.95833333, -1.        ,  0.33333333, -0.13207547, -0.51202749,\n",
       "        -1.        ,  1.        , -0.3129771 , -1.        , -0.60714286,\n",
       "         0.        , -1.        ,  0.33333333],\n",
       "       [-0.16666667,  1.        ,  0.33333333, -0.50943396, -0.57388316,\n",
       "        -1.        ,  0.        ,  0.03816794, -1.        , -0.28571429,\n",
       "         0.        ,  0.5       ,  1.        ],\n",
       "       [-0.16666667,  1.        , -0.33333333, -0.32075472, -0.03780069,\n",
       "        -1.        ,  0.        ,  0.52671756, -1.        , -0.78571429,\n",
       "         1.        , -1.        ,  0.33333333],\n",
       "       [ 0.29166667, -1.        ,  0.33333333, -0.8490566 ,  0.31958763,\n",
       "        -1.        ,  0.        ,  0.35877863, -1.        , -1.        ,\n",
       "         1.        , -0.5       ,  0.33333333],\n",
       "       [ 0.04166667, -1.        , -0.33333333, -0.28301887,  0.11340206,\n",
       "         1.        , -1.        ,  0.34351145,  1.        , -1.        ,\n",
       "         1.        , -0.5       ,  0.33333333],\n",
       "       [ 0.375     , -1.        , -1.        ,  0.24528302, -0.73883162,\n",
       "        -1.        , -1.        ,  0.12977099, -1.        ,  1.21428571,\n",
       "        -1.        ,  0.5       ,  1.        ],\n",
       "       [ 0.41666667,  1.        , -1.        , -0.32075472,  0.40206186,\n",
       "         1.        , -1.        , -0.06870229,  1.        , -0.35714286,\n",
       "         1.        ,  0.5       ,  1.        ],\n",
       "       [ 0.58333333,  1.        , -1.        ,  0.24528302,  0.09965636,\n",
       "        -1.        , -1.        , -0.4351145 ,  1.        , -0.46428571,\n",
       "         0.        ,  0.5       ,  0.33333333],\n",
       "       [ 0.125     ,  1.        , -1.        , -0.32075472,  0.0790378 ,\n",
       "         1.        , -1.        , -0.51145038,  1.        , -0.42857143,\n",
       "        -1.        , -1.        ,  1.        ],\n",
       "       [-0.33333333,  1.        , -1.        , -0.60377358, -0.0790378 ,\n",
       "        -1.        , -1.        ,  0.74045802, -1.        , -1.        ,\n",
       "         1.        , -1.        ,  0.33333333],\n",
       "       [ 0.58333333, -1.        ,  0.33333333,  0.09433962,  0.03780069,\n",
       "        -1.        ,  0.        ,  0.54198473, -1.        , -1.        ,\n",
       "         1.        , -0.5       ,  0.33333333],\n",
       "       [ 0.45833333,  1.        ,  1.        ,  0.43396226, -0.30584192,\n",
       "        -1.        , -1.        ,  0.28244275, -1.        , -0.78571429,\n",
       "         0.        , -1.        ,  1.        ],\n",
       "       [ 0.45833333,  1.        , -1.        , -0.35849057, -0.05841924,\n",
       "        -1.        ,  0.        , -0.48091603,  1.        , -0.92857143,\n",
       "         0.        , -0.5       ,  1.        ],\n",
       "       [-0.58333333,  1.        ,  0.33333333, -0.13207547,  0.34020619,\n",
       "        -1.        , -1.        ,  0.69465649, -1.        , -1.        ,\n",
       "         1.        , -1.        ,  0.33333333],\n",
       "       [-0.5       ,  1.        , -1.        , -0.69811321, -0.6838488 ,\n",
       "        -1.        , -1.        ,  0.32824427, -1.        , -1.        ,\n",
       "         1.        , -1.        ,  1.        ],\n",
       "       [ 0.375     ,  1.        , -0.33333333, -0.35849057, -0.43642612,\n",
       "         1.        , -1.        ,  0.05343511, -1.        , -1.        ,\n",
       "         1.        , -1.        ,  0.33333333],\n",
       "       [-0.08333333, -1.        ,  0.33333333, -0.13207547,  0.25085911,\n",
       "        -1.        , -1.        ,  0.08396947, -1.        , -0.46428571,\n",
       "         1.        , -0.5       ,  0.33333333],\n",
       "       [-0.08333333, -1.        , -1.        , -0.32075472,  0.23024055,\n",
       "        -1.        ,  0.        ,  0.08396947,  1.        , -0.57142857,\n",
       "         0.        , -1.        ,  1.        ],\n",
       "       [ 0.58333333,  1.        , -1.        , -0.50943396, -0.29209622,\n",
       "        -1.        , -1.        , -0.11450382,  1.        , -0.07142857,\n",
       "         0.        ,  0.        ,  1.        ],\n",
       "       [ 0.20833333,  1.        , -1.        , -0.88679245, -0.25773196,\n",
       "        -1.        ,  0.        ,  0.29770992, -1.        , -0.96428571,\n",
       "         1.        , -0.5       ,  1.        ],\n",
       "       [ 0.25      ,  1.        , -0.33333333, -0.13207547, -0.34707904,\n",
       "        -1.        ,  0.        ,  0.41984733,  1.        , -1.        ,\n",
       "         1.        , -1.        ,  0.33333333],\n",
       "       [ 0.33333333,  1.        ,  1.        , -0.24528302, -0.25773196,\n",
       "        -1.        ,  0.        ,  0.12977099, -1.        , -0.07142857,\n",
       "         0.        ,  0.        ,  0.33333333],\n",
       "       [-0.33333333,  1.        , -1.        , -0.09433962,  0.25773196,\n",
       "        -1.        , -1.        ,  0.16030534,  1.        , -1.        ,\n",
       "         0.        ,  0.5       ,  1.        ],\n",
       "       [ 0.        ,  1.        , -1.        , -0.45283019,  0.07216495,\n",
       "        -1.        ,  0.        , -0.63358779,  1.        , -0.28571429,\n",
       "         0.        ,  0.        ,  1.        ],\n",
       "       [ 0.54166667,  1.        , -1.        ,  0.24528302, -0.29896907,\n",
       "        -1.        , -1.        ,  0.02290076, -1.        , -0.17857143,\n",
       "         1.        , -1.        , -0.33333333],\n",
       "       [ 0.25      ,  1.        ,  1.        ,  0.24528302,  0.01030928,\n",
       "        -1.        , -1.        , -0.17557252, -1.        , -1.        ,\n",
       "         1.        , -1.        ,  0.33333333],\n",
       "       [-0.04166667,  1.        ,  1.        , -0.54716981, -0.58762887,\n",
       "        -1.        , -1.        ,  0.81679389, -1.        , -1.        ,\n",
       "         0.        , -1.        , -0.33333333],\n",
       "       [-0.08333333,  1.        , -1.        , -0.13207547,  0.18213058,\n",
       "        -1.        ,  0.        , -0.22137405,  1.        ,  0.5       ,\n",
       "         0.        ,  0.5       ,  1.        ],\n",
       "       [-0.5       ,  1.        , -0.33333333, -0.50943396, -0.78694158,\n",
       "        -1.        ,  0.        ,  0.69465649, -1.        , -1.        ,\n",
       "         1.        , -1.        ,  0.33333333],\n",
       "       [ 0.16666667,  1.        , -1.        , -0.69811321, -0.48453608,\n",
       "        -1.        ,  0.        , -0.16030534,  1.        , -0.46428571,\n",
       "         0.        , -1.        , -0.33333333],\n",
       "       [ 0.25      ,  1.        ,  1.        ,  0.43396226,  0.11340206,\n",
       "        -1.        , -1.        ,  0.34351145, -1.        , -0.92857143,\n",
       "         0.        , -1.        ,  1.        ],\n",
       "       [ 0.25      ,  1.        , -1.        , -0.13207547, -0.64948454,\n",
       "        -1.        ,  0.        ,  0.38931298,  1.        , -1.        ,\n",
       "         1.        , -0.5       ,  1.        ],\n",
       "       [ 0.04166667,  1.        , -1.        , -0.47169811,  0.09965636,\n",
       "        -1.        , -1.        , -0.3129771 ,  1.        ,  0.14285714,\n",
       "         0.        ,  0.        ,  0.33333333],\n",
       "       [ 0.25      ,  1.        , -1.        , -0.22641509, -0.25773196,\n",
       "        -1.        ,  0.        ,  0.3740458 , -1.        , -0.82142857,\n",
       "         0.        , -1.        ,  1.        ],\n",
       "       [ 0.20833333,  1.        , -0.33333333, -0.41509434, -0.35395189,\n",
       "        -1.        ,  0.        ,  0.11450382, -1.        , -0.85714286,\n",
       "         0.        ,  1.        ,  1.        ],\n",
       "       [ 0.20833333,  1.        , -0.33333333, -0.50943396,  0.08591065,\n",
       "        -1.        , -1.        ,  0.35877863, -1.        , -0.35714286,\n",
       "         0.        , -1.        ,  0.33333333],\n",
       "       [ 0.625     ,  1.        ,  0.33333333, -0.54716981,  0.03780069,\n",
       "        -1.        ,  0.        ,  0.22137405, -1.        , -0.64285714,\n",
       "         1.        , -0.5       ,  1.        ],\n",
       "       [ 0.625     ,  1.        ,  0.33333333,  0.62264151,  0.01718213,\n",
       "         1.        , -1.        ,  0.20610687,  1.        , -0.42857143,\n",
       "         0.        , -1.        ,  1.        ],\n",
       "       [-0.25      ,  1.        , -1.        , -0.69811321,  0.02405498,\n",
       "        -1.        , -1.        , -0.28244275,  1.        , -0.64285714,\n",
       "         0.        , -0.5       ,  0.33333333],\n",
       "       [-0.04166667, -1.        ,  0.33333333, -0.20754717, -0.51890034,\n",
       "        -1.        , -1.        ,  0.49618321, -1.        , -0.96428571,\n",
       "         0.        , -1.        ,  0.33333333],\n",
       "       [-0.04166667,  1.        ,  0.33333333, -0.16981132, -0.33333333,\n",
       "        -1.        ,  0.        ,  0.49618321, -1.        , -1.        ,\n",
       "         1.        ,  1.        ,  0.33333333],\n",
       "       [ 0.        ,  1.        ,  0.33333333, -0.32075472, -0.17525773,\n",
       "         1.        , -1.        ,  0.55725191, -1.        , -1.        ,\n",
       "         1.        ,  0.5       ,  0.33333333],\n",
       "       [-0.125     , -1.        ,  0.33333333, -0.50943396, -0.36082474,\n",
       "        -1.        ,  0.        ,  0.32824427, -1.        , -0.42857143,\n",
       "         0.        , -1.        ,  0.33333333],\n",
       "       [-0.5       ,  1.        ,  0.33333333, -0.66037736, -0.14776632,\n",
       "        -1.        ,  0.        ,  0.64885496, -1.        , -1.        ,\n",
       "         1.        , -1.        ,  0.33333333],\n",
       "       [-0.58333333, -1.        ,  0.33333333, -1.        , -0.49828179,\n",
       "        -1.        ,  0.        ,  0.64885496, -1.        , -1.        ,\n",
       "         1.        , -1.        ,  0.33333333],\n",
       "       [ 0.20833333, -1.        ,  0.33333333, -0.50943396,  0.47079038,\n",
       "        -1.        ,  0.        ,  0.54198473, -1.        , -1.        ,\n",
       "         1.        , -1.        ,  0.33333333],\n",
       "       [ 0.875     , -1.        , -0.33333333, -0.50943396, -0.01718213,\n",
       "        -1.        , -1.        , -0.23664122,  1.        , -0.92857143,\n",
       "         1.        , -0.5       ,  0.33333333],\n",
       "       [ 0.16666667,  1.        , -1.        ,  0.05660377,  0.03092784,\n",
       "        -1.        , -1.        , -0.3740458 ,  1.        , -0.78571429,\n",
       "         0.        , -0.5       , -0.33333333],\n",
       "       [ 0.29166667, -1.        , -1.        ,  0.05660377, -0.09278351,\n",
       "        -1.        , -1.        ,  0.3129771 , -1.        , -0.07142857,\n",
       "         0.        ,  0.        ,  1.        ],\n",
       "       [-0.54166667,  1.        , -1.        ,  0.09433962, -0.33333333,\n",
       "        -1.        ,  0.        ,  0.67938931, -1.        , -1.        ,\n",
       "         1.        , -1.        ,  1.        ],\n",
       "       [ 0.125     ,  1.        , -0.33333333, -0.50943396, -0.24398625,\n",
       "        -1.        ,  0.        ,  0.63358779, -1.        , -0.71428571,\n",
       "         1.        , -1.        ,  0.33333333],\n",
       "       [ 0.375     ,  1.        ,  0.33333333, -0.32075472, -0.27835052,\n",
       "        -1.        ,  0.        ,  0.14503817, -1.        , -0.35714286,\n",
       "         0.        ,  0.5       ,  1.        ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xa9U77VcfSSE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sDzfegLK4dDc"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kMSDboU8fSSQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VciPMpty4dGa"
   },
   "outputs": [],
   "source": [
    "#Dataset - o clasă din PyTorch foarte utilă gestionării seturilor de date\n",
    "class Dataset(Dataset):\n",
    "    \"\"\" Diabetes dataset.\"\"\"\n",
    "    # Initialize your data, download, etc.\n",
    "    def __init__(self, x, y):\n",
    "        #Citim setul de date\n",
    "        self.len = len(x)\n",
    "\n",
    "        self.x=torch.tensor(x).float()\n",
    "        self.y=torch.tensor(y.values).long()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2yf3t6Mh4dJA"
   },
   "outputs": [],
   "source": [
    "trainDataset=Dataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tb98rm-r4dMF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.2083, -1.0000, -1.0000, -0.3208, -0.5120, -1.0000,  0.0000, -0.0840,\n",
       "         -1.0000, -0.7857,  0.0000, -1.0000,  0.3333]), tensor(1))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JqEBq7LAfSSK"
   },
   "outputs": [],
   "source": [
    "trainLoader=DataLoader(dataset=trainDataset,\n",
    "                        batch_size=32,\n",
    "                        shuffle=True,\n",
    "                        num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oyWaEQ9Q5268"
   },
   "outputs": [],
   "source": [
    "validationDataset=Dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WSqWWH2953Jd"
   },
   "outputs": [],
   "source": [
    "validationLoader=DataLoader(dataset=validationDataset,\n",
    "                        batch_size=32,\n",
    "                        shuffle=True,\n",
    "                        num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oPIV0ruYuECM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nQjv5QsKfSSk"
   },
   "outputs": [],
   "source": [
    "class HeartDiseaseNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HeartDiseaseNN, self).__init__()\n",
    "\n",
    "        #Sequential oferă o alternativă mai estetică a codului\n",
    "        #Rețeaua noastră are 2 neuroni pentru output. \n",
    "        #Unul va prezice probabilitatea pentru cazul afirmativ al bolii, iar celălalt va prezice probabilitatea cazului negativ al bolii.\n",
    "        self.sequential= nn.Sequential(\n",
    "            nn.Linear(13,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 60),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(60, 2)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qI_tjxPWfSSn"
   },
   "outputs": [],
   "source": [
    "net = HeartDiseaseNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eI80PhVafSSt"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "# CrossEntropyLoss este folosit adeseori in problemele de clasificare\n",
    "# Acesta este compus din functia SoftMax și NLLLoss\n",
    "# Softmax - Mapează elementele din Tensor in intervalul [0,1] și face ca suma lor să fie 1. O functie foarte utilă atunci cand vrem sa calculam probabilitati dintr-un Tensor.\n",
    "# NLLLoss - negative log likelihood loss, functie folosită adeseori in problemele de clasificare\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RaqzqBOcfSSx"
   },
   "outputs": [],
   "source": [
    "#Colectăm loss-urile din antrenare pentru a le plota ulterior\n",
    "train_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7qt8b44MfSS3"
   },
   "outputs": [],
   "source": [
    "# Colectăm accuratetea pentru a o plota ulterior\n",
    "accuracies=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-2779h43uwuO"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K_woyjjffSTA"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(epoch):\n",
    "    # Setează câteva flaguri în rețeaua neurală. Specific activează Dropout-ul și BatchNormalization dacă este cazul.\n",
    "    # În exemplul nostru are un rol pur demonstrativ, nefiind necesar.\n",
    "    net.train()\n",
    "    losses=[]\n",
    "    for batch_idx, data in enumerate(trainLoader, 0):\n",
    "      inputs, labels =data\n",
    "      #Obținem predictii\n",
    "      outputs = net(inputs)\n",
    "      # Compute and print loss\n",
    "      loss = criterion(outputs, labels)\n",
    "\n",
    "      losses.append(loss.item())\n",
    "      # Zero gradients, perform a backward pass, and update the weights.\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      print(f\"[Train Epoch: {epoch}, Batch: {batch_idx+1}, Loss: {loss.item()}\")\n",
    "    mean_loss=sum(losses)/len(losses)\n",
    "    scheduler.step(mean_loss)\n",
    "    train_losses.append(mean_loss)\n",
    "    print(f\"[TRAIN] Epoch: {epoch} Loss:{mean_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pNNf-W4shrIH"
   },
   "outputs": [],
   "source": [
    "# Colectăm loss-ul din validare pentru a o plota ulterior\n",
    "test_losses=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8by5McSEfSTF"
   },
   "outputs": [],
   "source": [
    "def validation():\n",
    "    #Pune pe off flagurile setate in model.train()\n",
    "    #Din nou, în exemplul nostru e pur demonstrativ.\n",
    "    net.eval()\n",
    "\n",
    "    test_loss=[]\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(validationLoader, 0):\n",
    "          inputs, labels = data\n",
    "\n",
    "          output=net(inputs)\n",
    "\n",
    "          loss= criterion(output, labels)\n",
    "          test_loss.append(loss.item())\n",
    "\n",
    "          #Obținem predictiile pentru fiecare linie din setul de validare.\n",
    "          #Practic ne returnează rezultatul cu cea mai mare probabilitate pentru fiecare intrare din setul de validare \n",
    "          pred = output.data.max(1, keepdim=True)[1]\n",
    "\n",
    "          #Verificăm câte predicții sunt corecte și le însumăm numărul pentru a afla totalul de predicții corecte\n",
    "          correct += pred.eq(labels.data.view_as(pred)).sum()\n",
    "          current_correct=pred.eq(labels.data.view_as(pred)).sum()          \n",
    "          print(\"============\")\n",
    "          print(f\"[Validation set] Batch index: {batch_idx+1} Batch loss: {loss.item()}, Accuracy: {100. * current_correct/len(inputs)}%\")\n",
    "          print(\"============\")\n",
    "        mean_loss=sum(test_loss)/len(test_loss)\n",
    "        test_losses.append(mean_loss)\n",
    "        accuracy = 100. * correct/len(validationLoader.dataset)\n",
    "        print(f\"[Validation set] Loss: {mean_loss}, Accuracy: {accuracy}%\")\n",
    "          \n",
    "        accuracies.append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wuyLaJgafSTJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train Epoch: 0, Batch: 1, Loss: 0.6840814352035522\n",
      "[Train Epoch: 0, Batch: 2, Loss: 0.6904535889625549\n",
      "[Train Epoch: 0, Batch: 3, Loss: 0.6935496926307678\n",
      "[Train Epoch: 0, Batch: 4, Loss: 0.6955793499946594\n",
      "[Train Epoch: 0, Batch: 5, Loss: 0.7073678374290466\n",
      "[Train Epoch: 0, Batch: 6, Loss: 0.7050849795341492\n",
      "[Train Epoch: 0, Batch: 7, Loss: 0.7035866379737854\n",
      "[Train Epoch: 0, Batch: 8, Loss: 0.6887059807777405\n",
      "[TRAIN] Epoch: 0 Loss:0.696051187813282\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.6864393353462219, Accuracy: 62.5%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.6945700645446777, Accuracy: 51.72413635253906%\n",
      "============\n",
      "[Validation set] Loss: 0.6905046999454498, Accuracy: 57.37704849243164%\n",
      "[Train Epoch: 1, Batch: 1, Loss: 0.6892592310905457\n",
      "[Train Epoch: 1, Batch: 2, Loss: 0.695683479309082\n",
      "[Train Epoch: 1, Batch: 3, Loss: 0.6824958324432373\n",
      "[Train Epoch: 1, Batch: 4, Loss: 0.6927852630615234\n",
      "[Train Epoch: 1, Batch: 5, Loss: 0.6932699084281921\n",
      "[Train Epoch: 1, Batch: 6, Loss: 0.6950514912605286\n",
      "[Train Epoch: 1, Batch: 7, Loss: 0.6781613230705261\n",
      "[Train Epoch: 1, Batch: 8, Loss: 0.713239312171936\n",
      "[TRAIN] Epoch: 1 Loss:0.6924932301044464\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.6887689232826233, Accuracy: 56.25%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.6814182996749878, Accuracy: 62.068965911865234%\n",
      "============\n",
      "[Validation set] Loss: 0.6850936114788055, Accuracy: 59.01639175415039%\n",
      "[Train Epoch: 2, Batch: 1, Loss: 0.6926695108413696\n",
      "[Train Epoch: 2, Batch: 2, Loss: 0.6951425671577454\n",
      "[Train Epoch: 2, Batch: 3, Loss: 0.6793820261955261\n",
      "[Train Epoch: 2, Batch: 4, Loss: 0.6884442567825317\n",
      "[Train Epoch: 2, Batch: 5, Loss: 0.68358314037323\n",
      "[Train Epoch: 2, Batch: 6, Loss: 0.6860837936401367\n",
      "[Train Epoch: 2, Batch: 7, Loss: 0.6854224801063538\n",
      "[Train Epoch: 2, Batch: 8, Loss: 0.6715604662895203\n",
      "[TRAIN] Epoch: 2 Loss:0.6852860301733017\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.6778755784034729, Accuracy: 62.5%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.6828917860984802, Accuracy: 55.17241287231445%\n",
      "============\n",
      "[Validation set] Loss: 0.6803836822509766, Accuracy: 59.01639175415039%\n",
      "[Train Epoch: 3, Batch: 1, Loss: 0.6843332052230835\n",
      "[Train Epoch: 3, Batch: 2, Loss: 0.6823394894599915\n",
      "[Train Epoch: 3, Batch: 3, Loss: 0.6750075817108154\n",
      "[Train Epoch: 3, Batch: 4, Loss: 0.6687769293785095\n",
      "[Train Epoch: 3, Batch: 5, Loss: 0.6861335039138794\n",
      "[Train Epoch: 3, Batch: 6, Loss: 0.6920194625854492\n",
      "[Train Epoch: 3, Batch: 7, Loss: 0.6760002970695496\n",
      "[Train Epoch: 3, Batch: 8, Loss: 0.6860835552215576\n",
      "[TRAIN] Epoch: 3 Loss:0.6813367530703545\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.6716595888137817, Accuracy: 59.375%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.6798833012580872, Accuracy: 62.068965911865234%\n",
      "============\n",
      "[Validation set] Loss: 0.6757714450359344, Accuracy: 60.655738830566406%\n",
      "[Train Epoch: 4, Batch: 1, Loss: 0.6723600029945374\n",
      "[Train Epoch: 4, Batch: 2, Loss: 0.6774805784225464\n",
      "[Train Epoch: 4, Batch: 3, Loss: 0.6793195605278015\n",
      "[Train Epoch: 4, Batch: 4, Loss: 0.6671457290649414\n",
      "[Train Epoch: 4, Batch: 5, Loss: 0.6701611876487732\n",
      "[Train Epoch: 4, Batch: 6, Loss: 0.6698408722877502\n",
      "[Train Epoch: 4, Batch: 7, Loss: 0.6977539658546448\n",
      "[Train Epoch: 4, Batch: 8, Loss: 0.6762602925300598\n",
      "[TRAIN] Epoch: 4 Loss:0.6762902736663818\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.6725662350654602, Accuracy: 53.125%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.66901034116745, Accuracy: 68.96551513671875%\n",
      "============\n",
      "[Validation set] Loss: 0.6707882881164551, Accuracy: 60.655738830566406%\n",
      "[Train Epoch: 5, Batch: 1, Loss: 0.6668980717658997\n",
      "[Train Epoch: 5, Batch: 2, Loss: 0.6772061586380005\n",
      "[Train Epoch: 5, Batch: 3, Loss: 0.6766457557678223\n",
      "[Train Epoch: 5, Batch: 4, Loss: 0.6696863174438477\n",
      "[Train Epoch: 5, Batch: 5, Loss: 0.6774817705154419\n",
      "[Train Epoch: 5, Batch: 6, Loss: 0.6752825379371643\n",
      "[Train Epoch: 5, Batch: 7, Loss: 0.6532233953475952\n",
      "[Train Epoch: 5, Batch: 8, Loss: 0.6765975952148438\n",
      "[TRAIN] Epoch: 5 Loss:0.6716277003288269\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.65863037109375, Accuracy: 68.75%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.6747829914093018, Accuracy: 58.620689392089844%\n",
      "============\n",
      "[Validation set] Loss: 0.6667066812515259, Accuracy: 63.934425354003906%\n",
      "[Train Epoch: 6, Batch: 1, Loss: 0.652694582939148\n",
      "[Train Epoch: 6, Batch: 2, Loss: 0.6782986521720886\n",
      "[Train Epoch: 6, Batch: 3, Loss: 0.657317578792572\n",
      "[Train Epoch: 6, Batch: 4, Loss: 0.6638064384460449\n",
      "[Train Epoch: 6, Batch: 5, Loss: 0.6763439774513245\n",
      "[Train Epoch: 6, Batch: 6, Loss: 0.6625486016273499\n",
      "[Train Epoch: 6, Batch: 7, Loss: 0.6652054190635681\n",
      "[Train Epoch: 6, Batch: 8, Loss: 0.6819047927856445\n",
      "[TRAIN] Epoch: 6 Loss:0.6672650054097176\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.6665668487548828, Accuracy: 62.5%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.6568856239318848, Accuracy: 68.96551513671875%\n",
      "============\n",
      "[Validation set] Loss: 0.6617262363433838, Accuracy: 65.57376861572266%\n",
      "[Train Epoch: 7, Batch: 1, Loss: 0.6690950393676758\n",
      "[Train Epoch: 7, Batch: 2, Loss: 0.6624910831451416\n",
      "[Train Epoch: 7, Batch: 3, Loss: 0.6558899283409119\n",
      "[Train Epoch: 7, Batch: 4, Loss: 0.6574028134346008\n",
      "[Train Epoch: 7, Batch: 5, Loss: 0.653365433216095\n",
      "[Train Epoch: 7, Batch: 6, Loss: 0.6674336791038513\n",
      "[Train Epoch: 7, Batch: 7, Loss: 0.666569709777832\n",
      "[Train Epoch: 7, Batch: 8, Loss: 0.6602567434310913\n",
      "[TRAIN] Epoch: 7 Loss:0.66156305372715\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.6514607667922974, Accuracy: 65.625%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.6636548042297363, Accuracy: 72.4137954711914%\n",
      "============\n",
      "[Validation set] Loss: 0.6575577855110168, Accuracy: 68.85246276855469%\n",
      "[Train Epoch: 8, Batch: 1, Loss: 0.6567613482475281\n",
      "[Train Epoch: 8, Batch: 2, Loss: 0.6569780707359314\n",
      "[Train Epoch: 8, Batch: 3, Loss: 0.6692707538604736\n",
      "[Train Epoch: 8, Batch: 4, Loss: 0.654086709022522\n",
      "[Train Epoch: 8, Batch: 5, Loss: 0.6750622391700745\n",
      "[Train Epoch: 8, Batch: 6, Loss: 0.647137463092804\n",
      "[Train Epoch: 8, Batch: 7, Loss: 0.6424671411514282\n",
      "[Train Epoch: 8, Batch: 8, Loss: 0.6486256122589111\n",
      "[TRAIN] Epoch: 8 Loss:0.6562986671924591\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.6464487910270691, Accuracy: 75.0%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.6594329476356506, Accuracy: 65.51724243164062%\n",
      "============\n",
      "[Validation set] Loss: 0.6529408693313599, Accuracy: 70.49180603027344%\n",
      "[Train Epoch: 9, Batch: 1, Loss: 0.6706031560897827\n",
      "[Train Epoch: 9, Batch: 2, Loss: 0.6487873792648315\n",
      "[Train Epoch: 9, Batch: 3, Loss: 0.6497060060501099\n",
      "[Train Epoch: 9, Batch: 4, Loss: 0.6619201302528381\n",
      "[Train Epoch: 9, Batch: 5, Loss: 0.6441273093223572\n",
      "[Train Epoch: 9, Batch: 6, Loss: 0.6585872769355774\n",
      "[Train Epoch: 9, Batch: 7, Loss: 0.6428138613700867\n",
      "[Train Epoch: 9, Batch: 8, Loss: 0.6279281377792358\n",
      "[TRAIN] Epoch: 9 Loss:0.6505591571331024\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.6352052688598633, Accuracy: 78.125%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.6615416407585144, Accuracy: 65.51724243164062%\n",
      "============\n",
      "[Validation set] Loss: 0.6483734548091888, Accuracy: 72.13114929199219%\n",
      "[Train Epoch: 10, Batch: 1, Loss: 0.6588759422302246\n",
      "[Train Epoch: 10, Batch: 2, Loss: 0.6515233516693115\n",
      "[Train Epoch: 10, Batch: 3, Loss: 0.6172206401824951\n",
      "[Train Epoch: 10, Batch: 4, Loss: 0.6663435101509094\n",
      "[Train Epoch: 10, Batch: 5, Loss: 0.6452762484550476\n",
      "[Train Epoch: 10, Batch: 6, Loss: 0.6442061066627502\n",
      "[Train Epoch: 10, Batch: 7, Loss: 0.6475709676742554\n",
      "[Train Epoch: 10, Batch: 8, Loss: 0.639561116695404\n",
      "[TRAIN] Epoch: 10 Loss:0.6463222354650497\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.6262156367301941, Accuracy: 78.125%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.6617361903190613, Accuracy: 68.96551513671875%\n",
      "============\n",
      "[Validation set] Loss: 0.6439759135246277, Accuracy: 73.77049255371094%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train Epoch: 11, Batch: 1, Loss: 0.6507794857025146\n",
      "[Train Epoch: 11, Batch: 2, Loss: 0.6314398050308228\n",
      "[Train Epoch: 11, Batch: 3, Loss: 0.6425233483314514\n",
      "[Train Epoch: 11, Batch: 4, Loss: 0.6401515603065491\n",
      "[Train Epoch: 11, Batch: 5, Loss: 0.6605549454689026\n",
      "[Train Epoch: 11, Batch: 6, Loss: 0.6412620544433594\n",
      "[Train Epoch: 11, Batch: 7, Loss: 0.6338078379631042\n",
      "[Train Epoch: 11, Batch: 8, Loss: 0.6244063377380371\n",
      "[TRAIN] Epoch: 11 Loss:0.6406156718730927\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.6176984906196594, Accuracy: 84.375%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.6609209775924683, Accuracy: 62.068965911865234%\n",
      "============\n",
      "[Validation set] Loss: 0.6393097341060638, Accuracy: 73.77049255371094%\n",
      "[Train Epoch: 12, Batch: 1, Loss: 0.6894829869270325\n",
      "[Train Epoch: 12, Batch: 2, Loss: 0.6340404748916626\n",
      "[Train Epoch: 12, Batch: 3, Loss: 0.6566161513328552\n",
      "[Train Epoch: 12, Batch: 4, Loss: 0.605076789855957\n",
      "[Train Epoch: 12, Batch: 5, Loss: 0.6292316317558289\n",
      "[Train Epoch: 12, Batch: 6, Loss: 0.6325017809867859\n",
      "[Train Epoch: 12, Batch: 7, Loss: 0.6120739579200745\n",
      "[Train Epoch: 12, Batch: 8, Loss: 0.6297802925109863\n",
      "[TRAIN] Epoch: 12 Loss:0.6361005082726479\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.6392518281936646, Accuracy: 71.875%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.6272141933441162, Accuracy: 79.31034851074219%\n",
      "============\n",
      "[Validation set] Loss: 0.6332330107688904, Accuracy: 75.40983581542969%\n",
      "[Train Epoch: 13, Batch: 1, Loss: 0.6138061881065369\n",
      "[Train Epoch: 13, Batch: 2, Loss: 0.6456011533737183\n",
      "[Train Epoch: 13, Batch: 3, Loss: 0.648665726184845\n",
      "[Train Epoch: 13, Batch: 4, Loss: 0.6414118409156799\n",
      "[Train Epoch: 13, Batch: 5, Loss: 0.6136766672134399\n",
      "[Train Epoch: 13, Batch: 6, Loss: 0.6082618832588196\n",
      "[Train Epoch: 13, Batch: 7, Loss: 0.6320298910140991\n",
      "[Train Epoch: 13, Batch: 8, Loss: 0.6504804491996765\n",
      "[TRAIN] Epoch: 13 Loss:0.6317417249083519\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.6323625445365906, Accuracy: 71.875%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.6253700256347656, Accuracy: 79.31034851074219%\n",
      "============\n",
      "[Validation set] Loss: 0.6288662850856781, Accuracy: 75.40983581542969%\n",
      "[Train Epoch: 14, Batch: 1, Loss: 0.6369040608406067\n",
      "[Train Epoch: 14, Batch: 2, Loss: 0.6068434715270996\n",
      "[Train Epoch: 14, Batch: 3, Loss: 0.607464611530304\n",
      "[Train Epoch: 14, Batch: 4, Loss: 0.6327537894248962\n",
      "[Train Epoch: 14, Batch: 5, Loss: 0.6343443393707275\n",
      "[Train Epoch: 14, Batch: 6, Loss: 0.650263786315918\n",
      "[Train Epoch: 14, Batch: 7, Loss: 0.6158412098884583\n",
      "[Train Epoch: 14, Batch: 8, Loss: 0.6104837656021118\n",
      "[TRAIN] Epoch: 14 Loss:0.6243623793125153\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.6259255409240723, Accuracy: 71.875%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.6218411922454834, Accuracy: 82.75862121582031%\n",
      "============\n",
      "[Validation set] Loss: 0.6238833665847778, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 15, Batch: 1, Loss: 0.6128425598144531\n",
      "[Train Epoch: 15, Batch: 2, Loss: 0.5941680073738098\n",
      "[Train Epoch: 15, Batch: 3, Loss: 0.6098555326461792\n",
      "[Train Epoch: 15, Batch: 4, Loss: 0.6293254494667053\n",
      "[Train Epoch: 15, Batch: 5, Loss: 0.6247427463531494\n",
      "[Train Epoch: 15, Batch: 6, Loss: 0.6291013956069946\n",
      "[Train Epoch: 15, Batch: 7, Loss: 0.6406322717666626\n",
      "[Train Epoch: 15, Batch: 8, Loss: 0.607922375202179\n",
      "[TRAIN] Epoch: 15 Loss:0.6185737922787666\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.6157301068305969, Accuracy: 78.125%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.6220936179161072, Accuracy: 82.75862121582031%\n",
      "============\n",
      "[Validation set] Loss: 0.618911862373352, Accuracy: 80.32786560058594%\n",
      "[Train Epoch: 16, Batch: 1, Loss: 0.6016704440116882\n",
      "[Train Epoch: 16, Batch: 2, Loss: 0.5943199992179871\n",
      "[Train Epoch: 16, Batch: 3, Loss: 0.6089558601379395\n",
      "[Train Epoch: 16, Batch: 4, Loss: 0.6251164674758911\n",
      "[Train Epoch: 16, Batch: 5, Loss: 0.6230446100234985\n",
      "[Train Epoch: 16, Batch: 6, Loss: 0.6055726408958435\n",
      "[Train Epoch: 16, Batch: 7, Loss: 0.6039023399353027\n",
      "[Train Epoch: 16, Batch: 8, Loss: 0.663947343826294\n",
      "[TRAIN] Epoch: 16 Loss:0.6158162131905556\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5946141481399536, Accuracy: 81.25%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.635161280632019, Accuracy: 72.4137954711914%\n",
      "============\n",
      "[Validation set] Loss: 0.6148877143859863, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 17, Batch: 1, Loss: 0.5945139527320862\n",
      "[Train Epoch: 17, Batch: 2, Loss: 0.6230190992355347\n",
      "[Train Epoch: 17, Batch: 3, Loss: 0.5881791114807129\n",
      "[Train Epoch: 17, Batch: 4, Loss: 0.6166370511054993\n",
      "[Train Epoch: 17, Batch: 5, Loss: 0.6114767789840698\n",
      "[Train Epoch: 17, Batch: 6, Loss: 0.6210479736328125\n",
      "[Train Epoch: 17, Batch: 7, Loss: 0.5922954678535461\n",
      "[Train Epoch: 17, Batch: 8, Loss: 0.6124120354652405\n",
      "[TRAIN] Epoch: 17 Loss:0.6074476838111877\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.6147111654281616, Accuracy: 71.875%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.6015802025794983, Accuracy: 82.75862121582031%\n",
      "============\n",
      "[Validation set] Loss: 0.60814568400383, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 18, Batch: 1, Loss: 0.6324374675750732\n",
      "[Train Epoch: 18, Batch: 2, Loss: 0.5633739233016968\n",
      "[Train Epoch: 18, Batch: 3, Loss: 0.5947996377944946\n",
      "[Train Epoch: 18, Batch: 4, Loss: 0.592238187789917\n",
      "[Train Epoch: 18, Batch: 5, Loss: 0.6325426697731018\n",
      "[Train Epoch: 18, Batch: 6, Loss: 0.6135677099227905\n",
      "[Train Epoch: 18, Batch: 7, Loss: 0.5572234988212585\n",
      "[Train Epoch: 18, Batch: 8, Loss: 0.6363840103149414\n",
      "[TRAIN] Epoch: 18 Loss:0.6028208881616592\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.6143197417259216, Accuracy: 78.125%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.590462863445282, Accuracy: 75.86206817626953%\n",
      "============\n",
      "[Validation set] Loss: 0.6023913025856018, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 19, Batch: 1, Loss: 0.5763379335403442\n",
      "[Train Epoch: 19, Batch: 2, Loss: 0.5696382522583008\n",
      "[Train Epoch: 19, Batch: 3, Loss: 0.5663262605667114\n",
      "[Train Epoch: 19, Batch: 4, Loss: 0.6111415028572083\n",
      "[Train Epoch: 19, Batch: 5, Loss: 0.6152955889701843\n",
      "[Train Epoch: 19, Batch: 6, Loss: 0.6217774152755737\n",
      "[Train Epoch: 19, Batch: 7, Loss: 0.6233701109886169\n",
      "[Train Epoch: 19, Batch: 8, Loss: 0.5559982061386108\n",
      "[TRAIN] Epoch: 19 Loss:0.5924856588244438\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.580337643623352, Accuracy: 84.375%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.6160157322883606, Accuracy: 68.96551513671875%\n",
      "============\n",
      "[Validation set] Loss: 0.5981766879558563, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 20, Batch: 1, Loss: 0.6129041910171509\n",
      "[Train Epoch: 20, Batch: 2, Loss: 0.5959543585777283\n",
      "[Train Epoch: 20, Batch: 3, Loss: 0.5808691382408142\n",
      "[Train Epoch: 20, Batch: 4, Loss: 0.5791904926300049\n",
      "[Train Epoch: 20, Batch: 5, Loss: 0.5982608795166016\n",
      "[Train Epoch: 20, Batch: 6, Loss: 0.571983277797699\n",
      "[Train Epoch: 20, Batch: 7, Loss: 0.5714686512947083\n",
      "[Train Epoch: 20, Batch: 8, Loss: 0.5940700173377991\n",
      "[TRAIN] Epoch: 20 Loss:0.5880876258015633\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.6136226058006287, Accuracy: 75.0%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.5669932961463928, Accuracy: 79.31034851074219%\n",
      "============\n",
      "[Validation set] Loss: 0.5903079509735107, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 21, Batch: 1, Loss: 0.5693183541297913\n",
      "[Train Epoch: 21, Batch: 2, Loss: 0.5762814879417419\n",
      "[Train Epoch: 21, Batch: 3, Loss: 0.5914080739021301\n",
      "[Train Epoch: 21, Batch: 4, Loss: 0.5724261999130249\n",
      "[Train Epoch: 21, Batch: 5, Loss: 0.5859271883964539\n",
      "[Train Epoch: 21, Batch: 6, Loss: 0.5900923013687134\n",
      "[Train Epoch: 21, Batch: 7, Loss: 0.581651508808136\n",
      "[Train Epoch: 21, Batch: 8, Loss: 0.5811246037483215\n",
      "[TRAIN] Epoch: 21 Loss:0.5810287147760391\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.6101877689361572, Accuracy: 75.0%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.5576746463775635, Accuracy: 79.31034851074219%\n",
      "============\n",
      "[Validation set] Loss: 0.5839312076568604, Accuracy: 77.04917907714844%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train Epoch: 22, Batch: 1, Loss: 0.588164746761322\n",
      "[Train Epoch: 22, Batch: 2, Loss: 0.576110303401947\n",
      "[Train Epoch: 22, Batch: 3, Loss: 0.5794795155525208\n",
      "[Train Epoch: 22, Batch: 4, Loss: 0.5534857511520386\n",
      "[Train Epoch: 22, Batch: 5, Loss: 0.581067681312561\n",
      "[Train Epoch: 22, Batch: 6, Loss: 0.5923317670822144\n",
      "[Train Epoch: 22, Batch: 7, Loss: 0.5211610198020935\n",
      "[Train Epoch: 22, Batch: 8, Loss: 0.6214390397071838\n",
      "[TRAIN] Epoch: 22 Loss:0.5766549780964851\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5468953847885132, Accuracy: 81.25%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.6148467063903809, Accuracy: 72.4137954711914%\n",
      "============\n",
      "[Validation set] Loss: 0.580871045589447, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 23, Batch: 1, Loss: 0.5455483794212341\n",
      "[Train Epoch: 23, Batch: 2, Loss: 0.5663800239562988\n",
      "[Train Epoch: 23, Batch: 3, Loss: 0.5734190940856934\n",
      "[Train Epoch: 23, Batch: 4, Loss: 0.5753781795501709\n",
      "[Train Epoch: 23, Batch: 5, Loss: 0.5664842128753662\n",
      "[Train Epoch: 23, Batch: 6, Loss: 0.5906649827957153\n",
      "[Train Epoch: 23, Batch: 7, Loss: 0.5596031546592712\n",
      "[Train Epoch: 23, Batch: 8, Loss: 0.5561745166778564\n",
      "[TRAIN] Epoch: 23 Loss:0.5667065680027008\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5400089621543884, Accuracy: 87.5%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.6101500988006592, Accuracy: 65.51724243164062%\n",
      "============\n",
      "[Validation set] Loss: 0.5750795304775238, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 24, Batch: 1, Loss: 0.5677534341812134\n",
      "[Train Epoch: 24, Batch: 2, Loss: 0.5589720606803894\n",
      "[Train Epoch: 24, Batch: 3, Loss: 0.5432469248771667\n",
      "[Train Epoch: 24, Batch: 4, Loss: 0.5733248591423035\n",
      "[Train Epoch: 24, Batch: 5, Loss: 0.5823347568511963\n",
      "[Train Epoch: 24, Batch: 6, Loss: 0.5652328133583069\n",
      "[Train Epoch: 24, Batch: 7, Loss: 0.5165486931800842\n",
      "[Train Epoch: 24, Batch: 8, Loss: 0.5821924209594727\n",
      "[TRAIN] Epoch: 24 Loss:0.5612007454037666\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5340049862861633, Accuracy: 84.375%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.6045194864273071, Accuracy: 68.96551513671875%\n",
      "============\n",
      "[Validation set] Loss: 0.5692622363567352, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 25, Batch: 1, Loss: 0.5207478404045105\n",
      "[Train Epoch: 25, Batch: 2, Loss: 0.5041481256484985\n",
      "[Train Epoch: 25, Batch: 3, Loss: 0.5826467871665955\n",
      "[Train Epoch: 25, Batch: 4, Loss: 0.5884683728218079\n",
      "[Train Epoch: 25, Batch: 5, Loss: 0.5815626978874207\n",
      "[Train Epoch: 25, Batch: 6, Loss: 0.5616841316223145\n",
      "[Train Epoch: 25, Batch: 7, Loss: 0.5197640061378479\n",
      "[Train Epoch: 25, Batch: 8, Loss: 0.5742899179458618\n",
      "[TRAIN] Epoch: 25 Loss:0.5541639849543571\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5376749038696289, Accuracy: 84.375%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.5895055532455444, Accuracy: 72.4137954711914%\n",
      "============\n",
      "[Validation set] Loss: 0.5635902285575867, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 26, Batch: 1, Loss: 0.553080677986145\n",
      "[Train Epoch: 26, Batch: 2, Loss: 0.5390051603317261\n",
      "[Train Epoch: 26, Batch: 3, Loss: 0.5793653726577759\n",
      "[Train Epoch: 26, Batch: 4, Loss: 0.5433900952339172\n",
      "[Train Epoch: 26, Batch: 5, Loss: 0.5308473706245422\n",
      "[Train Epoch: 26, Batch: 6, Loss: 0.5245261192321777\n",
      "[Train Epoch: 26, Batch: 7, Loss: 0.5391961336135864\n",
      "[Train Epoch: 26, Batch: 8, Loss: 0.5645171403884888\n",
      "[TRAIN] Epoch: 26 Loss:0.5467410087585449\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5691826343536377, Accuracy: 78.125%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.5423553586006165, Accuracy: 79.31034851074219%\n",
      "============\n",
      "[Validation set] Loss: 0.5557689964771271, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 27, Batch: 1, Loss: 0.5656753182411194\n",
      "[Train Epoch: 27, Batch: 2, Loss: 0.5860322117805481\n",
      "[Train Epoch: 27, Batch: 3, Loss: 0.5284279584884644\n",
      "[Train Epoch: 27, Batch: 4, Loss: 0.5165459513664246\n",
      "[Train Epoch: 27, Batch: 5, Loss: 0.5252876281738281\n",
      "[Train Epoch: 27, Batch: 6, Loss: 0.5586011409759521\n",
      "[Train Epoch: 27, Batch: 7, Loss: 0.503407895565033\n",
      "[Train Epoch: 27, Batch: 8, Loss: 0.5125544667243958\n",
      "[TRAIN] Epoch: 27 Loss:0.5370665714144707\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5969967842102051, Accuracy: 68.75%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.49905627965927124, Accuracy: 89.6551742553711%\n",
      "============\n",
      "[Validation set] Loss: 0.5480265319347382, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 28, Batch: 1, Loss: 0.5714813470840454\n",
      "[Train Epoch: 28, Batch: 2, Loss: 0.5415859818458557\n",
      "[Train Epoch: 28, Batch: 3, Loss: 0.5289446115493774\n",
      "[Train Epoch: 28, Batch: 4, Loss: 0.456255167722702\n",
      "[Train Epoch: 28, Batch: 5, Loss: 0.5668767094612122\n",
      "[Train Epoch: 28, Batch: 6, Loss: 0.5271792411804199\n",
      "[Train Epoch: 28, Batch: 7, Loss: 0.5022737383842468\n",
      "[Train Epoch: 28, Batch: 8, Loss: 0.5767121911048889\n",
      "[TRAIN] Epoch: 28 Loss:0.5339136235415936\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.4921225905418396, Accuracy: 87.5%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.601666271686554, Accuracy: 68.96551513671875%\n",
      "============\n",
      "[Validation set] Loss: 0.5468944311141968, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 29, Batch: 1, Loss: 0.5045363306999207\n",
      "[Train Epoch: 29, Batch: 2, Loss: 0.5205310583114624\n",
      "[Train Epoch: 29, Batch: 3, Loss: 0.5622167587280273\n",
      "[Train Epoch: 29, Batch: 4, Loss: 0.4986594617366791\n",
      "[Train Epoch: 29, Batch: 5, Loss: 0.503653883934021\n",
      "[Train Epoch: 29, Batch: 6, Loss: 0.5362147688865662\n",
      "[Train Epoch: 29, Batch: 7, Loss: 0.5671623349189758\n",
      "[Train Epoch: 29, Batch: 8, Loss: 0.4890206754207611\n",
      "[TRAIN] Epoch: 29 Loss:0.5227494090795517\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.534027099609375, Accuracy: 78.125%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.5438365936279297, Accuracy: 75.86206817626953%\n",
      "============\n",
      "[Validation set] Loss: 0.5389318466186523, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 30, Batch: 1, Loss: 0.5473493933677673\n",
      "[Train Epoch: 30, Batch: 2, Loss: 0.5198177695274353\n",
      "[Train Epoch: 30, Batch: 3, Loss: 0.48574209213256836\n",
      "[Train Epoch: 30, Batch: 4, Loss: 0.4295507073402405\n",
      "[Train Epoch: 30, Batch: 5, Loss: 0.5132797360420227\n",
      "[Train Epoch: 30, Batch: 6, Loss: 0.5480130910873413\n",
      "[Train Epoch: 30, Batch: 7, Loss: 0.5986731052398682\n",
      "[Train Epoch: 30, Batch: 8, Loss: 0.48819154500961304\n",
      "[TRAIN] Epoch: 30 Loss:0.5163271799683571\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5152181386947632, Accuracy: 81.25%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.5521253943443298, Accuracy: 72.4137954711914%\n",
      "============\n",
      "[Validation set] Loss: 0.5336717665195465, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 31, Batch: 1, Loss: 0.5290210843086243\n",
      "[Train Epoch: 31, Batch: 2, Loss: 0.5246738195419312\n",
      "[Train Epoch: 31, Batch: 3, Loss: 0.4961574375629425\n",
      "[Train Epoch: 31, Batch: 4, Loss: 0.45092448592185974\n",
      "[Train Epoch: 31, Batch: 5, Loss: 0.49938321113586426\n",
      "[Train Epoch: 31, Batch: 6, Loss: 0.49363085627555847\n",
      "[Train Epoch: 31, Batch: 7, Loss: 0.5829266309738159\n",
      "[Train Epoch: 31, Batch: 8, Loss: 0.5083830952644348\n",
      "[TRAIN] Epoch: 31 Loss:0.5106375776231289\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5159589648246765, Accuracy: 81.25%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.5394288301467896, Accuracy: 72.4137954711914%\n",
      "============\n",
      "[Validation set] Loss: 0.527693897485733, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 32, Batch: 1, Loss: 0.49373286962509155\n",
      "[Train Epoch: 32, Batch: 2, Loss: 0.47055014967918396\n",
      "[Train Epoch: 32, Batch: 3, Loss: 0.4782171845436096\n",
      "[Train Epoch: 32, Batch: 4, Loss: 0.48597243428230286\n",
      "[Train Epoch: 32, Batch: 5, Loss: 0.5462930798530579\n",
      "[Train Epoch: 32, Batch: 6, Loss: 0.4753761887550354\n",
      "[Train Epoch: 32, Batch: 7, Loss: 0.5480935573577881\n",
      "[Train Epoch: 32, Batch: 8, Loss: 0.5542764067649841\n",
      "[TRAIN] Epoch: 32 Loss:0.5065639838576317\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.4991587996482849, Accuracy: 78.125%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.5482524633407593, Accuracy: 75.86206817626953%\n",
      "============\n",
      "[Validation set] Loss: 0.5237056314945221, Accuracy: 77.04917907714844%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train Epoch: 33, Batch: 1, Loss: 0.5422341823577881\n",
      "[Train Epoch: 33, Batch: 2, Loss: 0.5001336336135864\n",
      "[Train Epoch: 33, Batch: 3, Loss: 0.4840261936187744\n",
      "[Train Epoch: 33, Batch: 4, Loss: 0.45357295870780945\n",
      "[Train Epoch: 33, Batch: 5, Loss: 0.5264344811439514\n",
      "[Train Epoch: 33, Batch: 6, Loss: 0.5152301788330078\n",
      "[Train Epoch: 33, Batch: 7, Loss: 0.4928666353225708\n",
      "[Train Epoch: 33, Batch: 8, Loss: 0.43878379464149475\n",
      "[TRAIN] Epoch: 33 Loss:0.4941602572798729\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5441698431968689, Accuracy: 75.0%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.48722246289253235, Accuracy: 79.31034851074219%\n",
      "============\n",
      "[Validation set] Loss: 0.5156961530447006, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 34, Batch: 1, Loss: 0.4476688802242279\n",
      "[Train Epoch: 34, Batch: 2, Loss: 0.5626567006111145\n",
      "[Train Epoch: 34, Batch: 3, Loss: 0.44525980949401855\n",
      "[Train Epoch: 34, Batch: 4, Loss: 0.4805765151977539\n",
      "[Train Epoch: 34, Batch: 5, Loss: 0.465741366147995\n",
      "[Train Epoch: 34, Batch: 6, Loss: 0.5346574783325195\n",
      "[Train Epoch: 34, Batch: 7, Loss: 0.48602375388145447\n",
      "[Train Epoch: 34, Batch: 8, Loss: 0.516262412071228\n",
      "[TRAIN] Epoch: 34 Loss:0.492355864495039\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5884115695953369, Accuracy: 65.625%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.42890411615371704, Accuracy: 93.10344696044922%\n",
      "============\n",
      "[Validation set] Loss: 0.508657842874527, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 35, Batch: 1, Loss: 0.5410875082015991\n",
      "[Train Epoch: 35, Batch: 2, Loss: 0.5256112217903137\n",
      "[Train Epoch: 35, Batch: 3, Loss: 0.4413658380508423\n",
      "[Train Epoch: 35, Batch: 4, Loss: 0.5118632316589355\n",
      "[Train Epoch: 35, Batch: 5, Loss: 0.4832709729671478\n",
      "[Train Epoch: 35, Batch: 6, Loss: 0.41391807794570923\n",
      "[Train Epoch: 35, Batch: 7, Loss: 0.4505158066749573\n",
      "[Train Epoch: 35, Batch: 8, Loss: 0.5207556486129761\n",
      "[TRAIN] Epoch: 35 Loss:0.48604853823781013\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.48770642280578613, Accuracy: 84.375%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.5301012992858887, Accuracy: 72.4137954711914%\n",
      "============\n",
      "[Validation set] Loss: 0.5089038610458374, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 36, Batch: 1, Loss: 0.4733741283416748\n",
      "[Train Epoch: 36, Batch: 2, Loss: 0.553209662437439\n",
      "[Train Epoch: 36, Batch: 3, Loss: 0.474224328994751\n",
      "[Train Epoch: 36, Batch: 4, Loss: 0.508849561214447\n",
      "[Train Epoch: 36, Batch: 5, Loss: 0.44069409370422363\n",
      "[Train Epoch: 36, Batch: 6, Loss: 0.43893906474113464\n",
      "[Train Epoch: 36, Batch: 7, Loss: 0.49640220403671265\n",
      "[Train Epoch: 36, Batch: 8, Loss: 0.4042397439479828\n",
      "[TRAIN] Epoch: 36 Loss:0.4737415984272957\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.49494868516921997, Accuracy: 84.375%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.5113004446029663, Accuracy: 72.4137954711914%\n",
      "============\n",
      "[Validation set] Loss: 0.5031245648860931, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 37, Batch: 1, Loss: 0.453937828540802\n",
      "[Train Epoch: 37, Batch: 2, Loss: 0.485156774520874\n",
      "[Train Epoch: 37, Batch: 3, Loss: 0.4024018347263336\n",
      "[Train Epoch: 37, Batch: 4, Loss: 0.4612668752670288\n",
      "[Train Epoch: 37, Batch: 5, Loss: 0.46587371826171875\n",
      "[Train Epoch: 37, Batch: 6, Loss: 0.5033248066902161\n",
      "[Train Epoch: 37, Batch: 7, Loss: 0.5195692181587219\n",
      "[Train Epoch: 37, Batch: 8, Loss: 0.4866737723350525\n",
      "[TRAIN] Epoch: 37 Loss:0.47227560356259346\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.484718918800354, Accuracy: 78.125%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.513943612575531, Accuracy: 75.86206817626953%\n",
      "============\n",
      "[Validation set] Loss: 0.4993312656879425, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 38, Batch: 1, Loss: 0.4346120357513428\n",
      "[Train Epoch: 38, Batch: 2, Loss: 0.431520015001297\n",
      "[Train Epoch: 38, Batch: 3, Loss: 0.47035735845565796\n",
      "[Train Epoch: 38, Batch: 4, Loss: 0.4603111743927002\n",
      "[Train Epoch: 38, Batch: 5, Loss: 0.5610101222991943\n",
      "[Train Epoch: 38, Batch: 6, Loss: 0.37707003951072693\n",
      "[Train Epoch: 38, Batch: 7, Loss: 0.4751873314380646\n",
      "[Train Epoch: 38, Batch: 8, Loss: 0.5581320524215698\n",
      "[TRAIN] Epoch: 38 Loss:0.4710250161588192\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5259606838226318, Accuracy: 71.875%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.4604318141937256, Accuracy: 82.75862121582031%\n",
      "============\n",
      "[Validation set] Loss: 0.4931962490081787, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 39, Batch: 1, Loss: 0.483466237783432\n",
      "[Train Epoch: 39, Batch: 2, Loss: 0.42649751901626587\n",
      "[Train Epoch: 39, Batch: 3, Loss: 0.4373629689216614\n",
      "[Train Epoch: 39, Batch: 4, Loss: 0.5376430749893188\n",
      "[Train Epoch: 39, Batch: 5, Loss: 0.3126731514930725\n",
      "[Train Epoch: 39, Batch: 6, Loss: 0.5035839080810547\n",
      "[Train Epoch: 39, Batch: 7, Loss: 0.47473210096359253\n",
      "[Train Epoch: 39, Batch: 8, Loss: 0.5415890216827393\n",
      "[TRAIN] Epoch: 39 Loss:0.46469349786639214\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.46725940704345703, Accuracy: 84.375%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.5177251100540161, Accuracy: 68.96551513671875%\n",
      "============\n",
      "[Validation set] Loss: 0.4924922585487366, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 40, Batch: 1, Loss: 0.47938528656959534\n",
      "[Train Epoch: 40, Batch: 2, Loss: 0.5022767186164856\n",
      "[Train Epoch: 40, Batch: 3, Loss: 0.47390568256378174\n",
      "[Train Epoch: 40, Batch: 4, Loss: 0.4295615553855896\n",
      "[Train Epoch: 40, Batch: 5, Loss: 0.43783244490623474\n",
      "[Train Epoch: 40, Batch: 6, Loss: 0.39753445982933044\n",
      "[Train Epoch: 40, Batch: 7, Loss: 0.4500260055065155\n",
      "[Train Epoch: 40, Batch: 8, Loss: 0.4727393686771393\n",
      "[TRAIN] Epoch: 40 Loss:0.45540769025683403\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.4728783071041107, Accuracy: 71.875%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.5050533413887024, Accuracy: 82.75862121582031%\n",
      "============\n",
      "[Validation set] Loss: 0.48896582424640656, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 41, Batch: 1, Loss: 0.4274197816848755\n",
      "[Train Epoch: 41, Batch: 2, Loss: 0.46673038601875305\n",
      "[Train Epoch: 41, Batch: 3, Loss: 0.47950276732444763\n",
      "[Train Epoch: 41, Batch: 4, Loss: 0.3809559941291809\n",
      "[Train Epoch: 41, Batch: 5, Loss: 0.4429270327091217\n",
      "[Train Epoch: 41, Batch: 6, Loss: 0.44761160016059875\n",
      "[Train Epoch: 41, Batch: 7, Loss: 0.5163832902908325\n",
      "[Train Epoch: 41, Batch: 8, Loss: 0.417523056268692\n",
      "[TRAIN] Epoch: 41 Loss:0.44738173857331276\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.46008414030075073, Accuracy: 75.0%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.5116410255432129, Accuracy: 79.31034851074219%\n",
      "============\n",
      "[Validation set] Loss: 0.4858625829219818, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 42, Batch: 1, Loss: 0.4613460898399353\n",
      "[Train Epoch: 42, Batch: 2, Loss: 0.4473792016506195\n",
      "[Train Epoch: 42, Batch: 3, Loss: 0.4396497905254364\n",
      "[Train Epoch: 42, Batch: 4, Loss: 0.41165804862976074\n",
      "[Train Epoch: 42, Batch: 5, Loss: 0.4177221953868866\n",
      "[Train Epoch: 42, Batch: 6, Loss: 0.5297665596008301\n",
      "[Train Epoch: 42, Batch: 7, Loss: 0.4201565086841583\n",
      "[Train Epoch: 42, Batch: 8, Loss: 0.4027729034423828\n",
      "[TRAIN] Epoch: 42 Loss:0.4413064122200012\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.47037816047668457, Accuracy: 75.0%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.4927023947238922, Accuracy: 79.31034851074219%\n",
      "============\n",
      "[Validation set] Loss: 0.4815402776002884, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 43, Batch: 1, Loss: 0.40088191628456116\n",
      "[Train Epoch: 43, Batch: 2, Loss: 0.4520885646343231\n",
      "[Train Epoch: 43, Batch: 3, Loss: 0.45003119111061096\n",
      "[Train Epoch: 43, Batch: 4, Loss: 0.41291821002960205\n",
      "[Train Epoch: 43, Batch: 5, Loss: 0.479006290435791\n",
      "[Train Epoch: 43, Batch: 6, Loss: 0.5222439765930176\n",
      "[Train Epoch: 43, Batch: 7, Loss: 0.34577393531799316\n",
      "[Train Epoch: 43, Batch: 8, Loss: 0.4511096179485321\n",
      "[TRAIN] Epoch: 43 Loss:0.4392567127943039\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5422985553741455, Accuracy: 78.125%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.4082638919353485, Accuracy: 75.86206817626953%\n",
      "============\n",
      "[Validation set] Loss: 0.475281223654747, Accuracy: 77.04917907714844%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train Epoch: 44, Batch: 1, Loss: 0.4422488510608673\n",
      "[Train Epoch: 44, Batch: 2, Loss: 0.42953795194625854\n",
      "[Train Epoch: 44, Batch: 3, Loss: 0.45017313957214355\n",
      "[Train Epoch: 44, Batch: 4, Loss: 0.41997030377388\n",
      "[Train Epoch: 44, Batch: 5, Loss: 0.42752766609191895\n",
      "[Train Epoch: 44, Batch: 6, Loss: 0.4635224938392639\n",
      "[Train Epoch: 44, Batch: 7, Loss: 0.40224790573120117\n",
      "[Train Epoch: 44, Batch: 8, Loss: 0.43394893407821655\n",
      "[TRAIN] Epoch: 44 Loss:0.43364715576171875\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.47797250747680664, Accuracy: 75.0%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.4716910123825073, Accuracy: 79.31034851074219%\n",
      "============\n",
      "[Validation set] Loss: 0.474831759929657, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 45, Batch: 1, Loss: 0.40873944759368896\n",
      "[Train Epoch: 45, Batch: 2, Loss: 0.445040225982666\n",
      "[Train Epoch: 45, Batch: 3, Loss: 0.42205387353897095\n",
      "[Train Epoch: 45, Batch: 4, Loss: 0.375879168510437\n",
      "[Train Epoch: 45, Batch: 5, Loss: 0.4034380614757538\n",
      "[Train Epoch: 45, Batch: 6, Loss: 0.49886763095855713\n",
      "[Train Epoch: 45, Batch: 7, Loss: 0.4455421566963196\n",
      "[Train Epoch: 45, Batch: 8, Loss: 0.43724700808525085\n",
      "[TRAIN] Epoch: 45 Loss:0.42960094660520554\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5196155309677124, Accuracy: 71.875%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.4193090498447418, Accuracy: 82.75862121582031%\n",
      "============\n",
      "[Validation set] Loss: 0.4694622904062271, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 46, Batch: 1, Loss: 0.43377551436424255\n",
      "[Train Epoch: 46, Batch: 2, Loss: 0.4903874099254608\n",
      "[Train Epoch: 46, Batch: 3, Loss: 0.40069255232810974\n",
      "[Train Epoch: 46, Batch: 4, Loss: 0.37778377532958984\n",
      "[Train Epoch: 46, Batch: 5, Loss: 0.48376429080963135\n",
      "[Train Epoch: 46, Batch: 6, Loss: 0.39773672819137573\n",
      "[Train Epoch: 46, Batch: 7, Loss: 0.43412503600120544\n",
      "[Train Epoch: 46, Batch: 8, Loss: 0.34312281012535095\n",
      "[TRAIN] Epoch: 46 Loss:0.4201735146343708\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5077431797981262, Accuracy: 75.0%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.42834028601646423, Accuracy: 79.31034851074219%\n",
      "============\n",
      "[Validation set] Loss: 0.4680417329072952, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 47, Batch: 1, Loss: 0.44167426228523254\n",
      "[Train Epoch: 47, Batch: 2, Loss: 0.5389770865440369\n",
      "[Train Epoch: 47, Batch: 3, Loss: 0.4852946698665619\n",
      "[Train Epoch: 47, Batch: 4, Loss: 0.3807661831378937\n",
      "[Train Epoch: 47, Batch: 5, Loss: 0.36187803745269775\n",
      "[Train Epoch: 47, Batch: 6, Loss: 0.43931615352630615\n",
      "[Train Epoch: 47, Batch: 7, Loss: 0.3741302490234375\n",
      "[Train Epoch: 47, Batch: 8, Loss: 0.277646005153656\n",
      "[TRAIN] Epoch: 47 Loss:0.4124603308737278\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5062289237976074, Accuracy: 71.875%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.4237350821495056, Accuracy: 82.75862121582031%\n",
      "============\n",
      "[Validation set] Loss: 0.4649820029735565, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 48, Batch: 1, Loss: 0.46414247155189514\n",
      "[Train Epoch: 48, Batch: 2, Loss: 0.3802887201309204\n",
      "[Train Epoch: 48, Batch: 3, Loss: 0.37333202362060547\n",
      "[Train Epoch: 48, Batch: 4, Loss: 0.4368061125278473\n",
      "[Train Epoch: 48, Batch: 5, Loss: 0.37505704164505005\n",
      "[Train Epoch: 48, Batch: 6, Loss: 0.508665919303894\n",
      "[Train Epoch: 48, Batch: 7, Loss: 0.3741183280944824\n",
      "[Train Epoch: 48, Batch: 8, Loss: 0.412904292345047\n",
      "[TRAIN] Epoch: 48 Loss:0.4156643636524677\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.49000176787376404, Accuracy: 78.125%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.43741706013679504, Accuracy: 75.86206817626953%\n",
      "============\n",
      "[Validation set] Loss: 0.46370941400527954, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 49, Batch: 1, Loss: 0.4973560571670532\n",
      "[Train Epoch: 49, Batch: 2, Loss: 0.4196065664291382\n",
      "[Train Epoch: 49, Batch: 3, Loss: 0.32734593749046326\n",
      "[Train Epoch: 49, Batch: 4, Loss: 0.36659738421440125\n",
      "[Train Epoch: 49, Batch: 5, Loss: 0.5562787055969238\n",
      "[Train Epoch: 49, Batch: 6, Loss: 0.3255327045917511\n",
      "[Train Epoch: 49, Batch: 7, Loss: 0.3940768837928772\n",
      "[Train Epoch: 49, Batch: 8, Loss: 0.41136327385902405\n",
      "[TRAIN] Epoch: 49 Loss:0.412269689142704\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.4064875543117523, Accuracy: 81.25%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.5274136066436768, Accuracy: 72.4137954711914%\n",
      "============\n",
      "[Validation set] Loss: 0.46695058047771454, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 50, Batch: 1, Loss: 0.45105093717575073\n",
      "[Train Epoch: 50, Batch: 2, Loss: 0.4161165654659271\n",
      "[Train Epoch: 50, Batch: 3, Loss: 0.44018664956092834\n",
      "[Train Epoch: 50, Batch: 4, Loss: 0.4563628137111664\n",
      "[Train Epoch: 50, Batch: 5, Loss: 0.37951886653900146\n",
      "[Train Epoch: 50, Batch: 6, Loss: 0.31884056329727173\n",
      "[Train Epoch: 50, Batch: 7, Loss: 0.4264450669288635\n",
      "[Train Epoch: 50, Batch: 8, Loss: 0.34771716594696045\n",
      "[TRAIN] Epoch: 50 Loss:0.4045298285782337\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5628030896186829, Accuracy: 62.5%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.35136738419532776, Accuracy: 93.10344696044922%\n",
      "============\n",
      "[Validation set] Loss: 0.4570852369070053, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 51, Batch: 1, Loss: 0.4057648479938507\n",
      "[Train Epoch: 51, Batch: 2, Loss: 0.39818739891052246\n",
      "[Train Epoch: 51, Batch: 3, Loss: 0.4197964072227478\n",
      "[Train Epoch: 51, Batch: 4, Loss: 0.4482392370700836\n",
      "[Train Epoch: 51, Batch: 5, Loss: 0.29111072421073914\n",
      "[Train Epoch: 51, Batch: 6, Loss: 0.34044769406318665\n",
      "[Train Epoch: 51, Batch: 7, Loss: 0.4948329031467438\n",
      "[Train Epoch: 51, Batch: 8, Loss: 0.4555715024471283\n",
      "[TRAIN] Epoch: 51 Loss:0.4067438393831253\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5357136130332947, Accuracy: 75.0%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.3795434236526489, Accuracy: 79.31034851074219%\n",
      "============\n",
      "[Validation set] Loss: 0.4576285183429718, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 52, Batch: 1, Loss: 0.4529566168785095\n",
      "[Train Epoch: 52, Batch: 2, Loss: 0.3995804190635681\n",
      "[Train Epoch: 52, Batch: 3, Loss: 0.3844813406467438\n",
      "[Train Epoch: 52, Batch: 4, Loss: 0.40242189168930054\n",
      "[Train Epoch: 52, Batch: 5, Loss: 0.43418800830841064\n",
      "[Train Epoch: 52, Batch: 6, Loss: 0.3077591359615326\n",
      "[Train Epoch: 52, Batch: 7, Loss: 0.4241293668746948\n",
      "[Train Epoch: 52, Batch: 8, Loss: 0.39905208349227905\n",
      "[TRAIN] Epoch: 52 Loss:0.4005711078643799\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5044529438018799, Accuracy: 75.0%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.4134255647659302, Accuracy: 79.31034851074219%\n",
      "============\n",
      "[Validation set] Loss: 0.45893925428390503, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 53, Batch: 1, Loss: 0.4835529029369354\n",
      "[Train Epoch: 53, Batch: 2, Loss: 0.3507140576839447\n",
      "[Train Epoch: 53, Batch: 3, Loss: 0.28916800022125244\n",
      "[Train Epoch: 53, Batch: 4, Loss: 0.40238887071609497\n",
      "[Train Epoch: 53, Batch: 5, Loss: 0.4766871929168701\n",
      "[Train Epoch: 53, Batch: 6, Loss: 0.3700992465019226\n",
      "[Train Epoch: 53, Batch: 7, Loss: 0.3891596496105194\n",
      "[Train Epoch: 53, Batch: 8, Loss: 0.43014833331108093\n",
      "[TRAIN] Epoch: 53 Loss:0.3989897817373276\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.4895649254322052, Accuracy: 78.125%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.42501527070999146, Accuracy: 75.86206817626953%\n",
      "============\n",
      "[Validation set] Loss: 0.4572900980710983, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 54, Batch: 1, Loss: 0.524128794670105\n",
      "[Train Epoch: 54, Batch: 2, Loss: 0.40678977966308594\n",
      "[Train Epoch: 54, Batch: 3, Loss: 0.3172765076160431\n",
      "[Train Epoch: 54, Batch: 4, Loss: 0.34747278690338135\n",
      "[Train Epoch: 54, Batch: 5, Loss: 0.32680097222328186\n",
      "[Train Epoch: 54, Batch: 6, Loss: 0.497309148311615\n",
      "[Train Epoch: 54, Batch: 7, Loss: 0.315630167722702\n",
      "[Train Epoch: 54, Batch: 8, Loss: 0.4310993552207947\n",
      "[TRAIN] Epoch: 54 Loss:0.3958134390413761\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.47305822372436523, Accuracy: 75.0%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.4414253830909729, Accuracy: 79.31034851074219%\n",
      "============\n",
      "[Validation set] Loss: 0.45724180340766907, Accuracy: 77.04917907714844%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train Epoch: 55, Batch: 1, Loss: 0.3978613615036011\n",
      "[Train Epoch: 55, Batch: 2, Loss: 0.3278072774410248\n",
      "[Train Epoch: 55, Batch: 3, Loss: 0.4006359875202179\n",
      "[Train Epoch: 55, Batch: 4, Loss: 0.3441579043865204\n",
      "[Train Epoch: 55, Batch: 5, Loss: 0.5193377137184143\n",
      "[Train Epoch: 55, Batch: 6, Loss: 0.30276787281036377\n",
      "[Train Epoch: 55, Batch: 7, Loss: 0.3457747995853424\n",
      "[Train Epoch: 55, Batch: 8, Loss: 0.5603852272033691\n",
      "[TRAIN] Epoch: 55 Loss:0.3998410180211067\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5612154006958008, Accuracy: 68.75%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.3431686758995056, Accuracy: 86.20689392089844%\n",
      "============\n",
      "[Validation set] Loss: 0.4521920382976532, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 56, Batch: 1, Loss: 0.39172399044036865\n",
      "[Train Epoch: 56, Batch: 2, Loss: 0.33303096890449524\n",
      "[Train Epoch: 56, Batch: 3, Loss: 0.4150550067424774\n",
      "[Train Epoch: 56, Batch: 4, Loss: 0.34560561180114746\n",
      "[Train Epoch: 56, Batch: 5, Loss: 0.3546912372112274\n",
      "[Train Epoch: 56, Batch: 6, Loss: 0.4100472033023834\n",
      "[Train Epoch: 56, Batch: 7, Loss: 0.39944204688072205\n",
      "[Train Epoch: 56, Batch: 8, Loss: 0.4996456503868103\n",
      "[TRAIN] Epoch: 56 Loss:0.393655214458704\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.43328672647476196, Accuracy: 84.375%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.48289722204208374, Accuracy: 68.96551513671875%\n",
      "============\n",
      "[Validation set] Loss: 0.45809197425842285, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 57, Batch: 1, Loss: 0.37513965368270874\n",
      "[Train Epoch: 57, Batch: 2, Loss: 0.3695511519908905\n",
      "[Train Epoch: 57, Batch: 3, Loss: 0.33873310685157776\n",
      "[Train Epoch: 57, Batch: 4, Loss: 0.4893955588340759\n",
      "[Train Epoch: 57, Batch: 5, Loss: 0.5055180191993713\n",
      "[Train Epoch: 57, Batch: 6, Loss: 0.2688434422016144\n",
      "[Train Epoch: 57, Batch: 7, Loss: 0.4088067412376404\n",
      "[Train Epoch: 57, Batch: 8, Loss: 0.2825677990913391\n",
      "[TRAIN] Epoch: 57 Loss:0.37981943413615227\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.43914905190467834, Accuracy: 75.0%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.4735473692417145, Accuracy: 79.31034851074219%\n",
      "============\n",
      "[Validation set] Loss: 0.4563482105731964, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 58, Batch: 1, Loss: 0.44769737124443054\n",
      "[Train Epoch: 58, Batch: 2, Loss: 0.34965747594833374\n",
      "[Train Epoch: 58, Batch: 3, Loss: 0.27968573570251465\n",
      "[Train Epoch: 58, Batch: 4, Loss: 0.37047749757766724\n",
      "[Train Epoch: 58, Batch: 5, Loss: 0.4345068633556366\n",
      "[Train Epoch: 58, Batch: 6, Loss: 0.4696688950061798\n",
      "[Train Epoch: 58, Batch: 7, Loss: 0.26562461256980896\n",
      "[Train Epoch: 58, Batch: 8, Loss: 0.4807901382446289\n",
      "[TRAIN] Epoch: 58 Loss:0.38726357370615005\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.390890508890152, Accuracy: 87.5%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.5244594812393188, Accuracy: 65.51724243164062%\n",
      "============\n",
      "[Validation set] Loss: 0.4576749950647354, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 59, Batch: 1, Loss: 0.3591844141483307\n",
      "[Train Epoch: 59, Batch: 2, Loss: 0.38513389229774475\n",
      "[Train Epoch: 59, Batch: 3, Loss: 0.33161038160324097\n",
      "[Train Epoch: 59, Batch: 4, Loss: 0.33798202872276306\n",
      "[Train Epoch: 59, Batch: 5, Loss: 0.4321257472038269\n",
      "[Train Epoch: 59, Batch: 6, Loss: 0.3615413010120392\n",
      "[Train Epoch: 59, Batch: 7, Loss: 0.4528096914291382\n",
      "[Train Epoch: 59, Batch: 8, Loss: 0.3690284490585327\n",
      "[TRAIN] Epoch: 59 Loss:0.37867698818445206\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5078417658805847, Accuracy: 68.75%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.394623339176178, Accuracy: 86.20689392089844%\n",
      "============\n",
      "[Validation set] Loss: 0.45123255252838135, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 60, Batch: 1, Loss: 0.43785908818244934\n",
      "[Train Epoch: 60, Batch: 2, Loss: 0.28673118352890015\n",
      "[Train Epoch: 60, Batch: 3, Loss: 0.38158246874809265\n",
      "[Train Epoch: 60, Batch: 4, Loss: 0.41549748182296753\n",
      "[Train Epoch: 60, Batch: 5, Loss: 0.3493162989616394\n",
      "[Train Epoch: 60, Batch: 6, Loss: 0.42371082305908203\n",
      "[Train Epoch: 60, Batch: 7, Loss: 0.2983608543872833\n",
      "[Train Epoch: 60, Batch: 8, Loss: 0.45745861530303955\n",
      "[TRAIN] Epoch: 60 Loss:0.38131460174918175\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.39787089824676514, Accuracy: 84.375%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.518772304058075, Accuracy: 72.4137954711914%\n",
      "============\n",
      "[Validation set] Loss: 0.45832160115242004, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 61, Batch: 1, Loss: 0.2859557569026947\n",
      "[Train Epoch: 61, Batch: 2, Loss: 0.49345314502716064\n",
      "[Train Epoch: 61, Batch: 3, Loss: 0.33505886793136597\n",
      "[Train Epoch: 61, Batch: 4, Loss: 0.38367000222206116\n",
      "[Train Epoch: 61, Batch: 5, Loss: 0.35748395323753357\n",
      "[Train Epoch: 61, Batch: 6, Loss: 0.34251707792282104\n",
      "[Train Epoch: 61, Batch: 7, Loss: 0.31823691725730896\n",
      "[Train Epoch: 61, Batch: 8, Loss: 0.5629616379737854\n",
      "[TRAIN] Epoch: 61 Loss:0.38491716980934143\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5756099820137024, Accuracy: 68.75%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.3226155638694763, Accuracy: 86.20689392089844%\n",
      "============\n",
      "[Validation set] Loss: 0.44911277294158936, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 62, Batch: 1, Loss: 0.3348834812641144\n",
      "[Train Epoch: 62, Batch: 2, Loss: 0.3411261737346649\n",
      "[Train Epoch: 62, Batch: 3, Loss: 0.3262118399143219\n",
      "[Train Epoch: 62, Batch: 4, Loss: 0.3729078769683838\n",
      "[Train Epoch: 62, Batch: 5, Loss: 0.3555219769477844\n",
      "[Train Epoch: 62, Batch: 6, Loss: 0.3930990993976593\n",
      "[Train Epoch: 62, Batch: 7, Loss: 0.41810494661331177\n",
      "[Train Epoch: 62, Batch: 8, Loss: 0.4835040271282196\n",
      "[TRAIN] Epoch: 62 Loss:0.3781699277460575\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5260663032531738, Accuracy: 75.0%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.37935182452201843, Accuracy: 79.31034851074219%\n",
      "============\n",
      "[Validation set] Loss: 0.45270906388759613, Accuracy: 77.04917907714844%\n",
      "[Train Epoch: 63, Batch: 1, Loss: 0.4363330006599426\n",
      "[Train Epoch: 63, Batch: 2, Loss: 0.3589983582496643\n",
      "[Train Epoch: 63, Batch: 3, Loss: 0.477911114692688\n",
      "[Train Epoch: 63, Batch: 4, Loss: 0.3628222942352295\n",
      "[Train Epoch: 63, Batch: 5, Loss: 0.33510521054267883\n",
      "[Train Epoch: 63, Batch: 6, Loss: 0.3634292483329773\n",
      "[Train Epoch: 63, Batch: 7, Loss: 0.3173063397407532\n",
      "[Train Epoch: 63, Batch: 8, Loss: 0.2593330144882202\n",
      "[TRAIN] Epoch: 63 Loss:0.36390482261776924\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5717571973800659, Accuracy: 75.0%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.3235241174697876, Accuracy: 86.20689392089844%\n",
      "============\n",
      "[Validation set] Loss: 0.44764065742492676, Accuracy: 80.32786560058594%\n",
      "[Train Epoch: 64, Batch: 1, Loss: 0.2972995340824127\n",
      "[Train Epoch: 64, Batch: 2, Loss: 0.41867202520370483\n",
      "[Train Epoch: 64, Batch: 3, Loss: 0.3231636583805084\n",
      "[Train Epoch: 64, Batch: 4, Loss: 0.5040197968482971\n",
      "[Train Epoch: 64, Batch: 5, Loss: 0.39268559217453003\n",
      "[Train Epoch: 64, Batch: 6, Loss: 0.47619593143463135\n",
      "[Train Epoch: 64, Batch: 7, Loss: 0.24391353130340576\n",
      "[Train Epoch: 64, Batch: 8, Loss: 0.2182203233242035\n",
      "[TRAIN] Epoch: 64 Loss:0.3592712990939617\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.558707594871521, Accuracy: 75.0%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.33568745851516724, Accuracy: 86.20689392089844%\n",
      "============\n",
      "[Validation set] Loss: 0.4471975266933441, Accuracy: 80.32786560058594%\n",
      "[Train Epoch: 65, Batch: 1, Loss: 0.43731316924095154\n",
      "[Train Epoch: 65, Batch: 2, Loss: 0.23235589265823364\n",
      "[Train Epoch: 65, Batch: 3, Loss: 0.25574991106987\n",
      "[Train Epoch: 65, Batch: 4, Loss: 0.435356080532074\n",
      "[Train Epoch: 65, Batch: 5, Loss: 0.42244476079940796\n",
      "[Train Epoch: 65, Batch: 6, Loss: 0.4124259948730469\n",
      "[Train Epoch: 65, Batch: 7, Loss: 0.3308936655521393\n",
      "[Train Epoch: 65, Batch: 8, Loss: 0.41505980491638184\n",
      "[TRAIN] Epoch: 65 Loss:0.36769990995526314\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5341407656669617, Accuracy: 71.875%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.3614119589328766, Accuracy: 86.20689392089844%\n",
      "============\n",
      "[Validation set] Loss: 0.44777636229991913, Accuracy: 78.68852233886719%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train Epoch: 66, Batch: 1, Loss: 0.3554040193557739\n",
      "[Train Epoch: 66, Batch: 2, Loss: 0.36450424790382385\n",
      "[Train Epoch: 66, Batch: 3, Loss: 0.48182058334350586\n",
      "[Train Epoch: 66, Batch: 4, Loss: 0.2891848385334015\n",
      "[Train Epoch: 66, Batch: 5, Loss: 0.26598021388053894\n",
      "[Train Epoch: 66, Batch: 6, Loss: 0.3269721567630768\n",
      "[Train Epoch: 66, Batch: 7, Loss: 0.43857017159461975\n",
      "[Train Epoch: 66, Batch: 8, Loss: 0.40622058510780334\n",
      "[TRAIN] Epoch: 66 Loss:0.366082102060318\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.4224633276462555, Accuracy: 78.125%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.4869309365749359, Accuracy: 79.31034851074219%\n",
      "============\n",
      "[Validation set] Loss: 0.4546971321105957, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 67, Batch: 1, Loss: 0.2959178388118744\n",
      "[Train Epoch: 67, Batch: 2, Loss: 0.45222586393356323\n",
      "[Train Epoch: 67, Batch: 3, Loss: 0.3899866044521332\n",
      "[Train Epoch: 67, Batch: 4, Loss: 0.388837993144989\n",
      "[Train Epoch: 67, Batch: 5, Loss: 0.2650936543941498\n",
      "[Train Epoch: 67, Batch: 6, Loss: 0.5022802352905273\n",
      "[Train Epoch: 67, Batch: 7, Loss: 0.33492663502693176\n",
      "[Train Epoch: 67, Batch: 8, Loss: 0.1759803295135498\n",
      "[TRAIN] Epoch: 67 Loss:0.3506561443209648\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.4479976296424866, Accuracy: 78.125%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.45978379249572754, Accuracy: 79.31034851074219%\n",
      "============\n",
      "[Validation set] Loss: 0.45389071106910706, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 68, Batch: 1, Loss: 0.3011051416397095\n",
      "[Train Epoch: 68, Batch: 2, Loss: 0.4104990065097809\n",
      "[Train Epoch: 68, Batch: 3, Loss: 0.46407949924468994\n",
      "[Train Epoch: 68, Batch: 4, Loss: 0.4200068712234497\n",
      "[Train Epoch: 68, Batch: 5, Loss: 0.2444145381450653\n",
      "[Train Epoch: 68, Batch: 6, Loss: 0.2790582776069641\n",
      "[Train Epoch: 68, Batch: 7, Loss: 0.34230175614356995\n",
      "[Train Epoch: 68, Batch: 8, Loss: 0.4518880844116211\n",
      "[TRAIN] Epoch: 68 Loss:0.3641691468656063\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.3613901436328888, Accuracy: 84.375%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.5575968027114868, Accuracy: 72.4137954711914%\n",
      "============\n",
      "[Validation set] Loss: 0.4594934731721878, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 69, Batch: 1, Loss: 0.493435800075531\n",
      "[Train Epoch: 69, Batch: 2, Loss: 0.2952100932598114\n",
      "[Train Epoch: 69, Batch: 3, Loss: 0.3372187614440918\n",
      "[Train Epoch: 69, Batch: 4, Loss: 0.36394768953323364\n",
      "[Train Epoch: 69, Batch: 5, Loss: 0.3590773046016693\n",
      "[Train Epoch: 69, Batch: 6, Loss: 0.3079393804073334\n",
      "[Train Epoch: 69, Batch: 7, Loss: 0.36582139134407043\n",
      "[Train Epoch: 69, Batch: 8, Loss: 0.3158913850784302\n",
      "[TRAIN] Epoch: 69 Loss:0.3548177257180214\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.45366379618644714, Accuracy: 81.25%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.456875741481781, Accuracy: 75.86206817626953%\n",
      "============\n",
      "[Validation set] Loss: 0.4552697688341141, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 70, Batch: 1, Loss: 0.23700642585754395\n",
      "[Train Epoch: 70, Batch: 2, Loss: 0.4961966872215271\n",
      "[Train Epoch: 70, Batch: 3, Loss: 0.3155529499053955\n",
      "[Train Epoch: 70, Batch: 4, Loss: 0.27812740206718445\n",
      "[Train Epoch: 70, Batch: 5, Loss: 0.39468687772750854\n",
      "[Train Epoch: 70, Batch: 6, Loss: 0.34894517064094543\n",
      "[Train Epoch: 70, Batch: 7, Loss: 0.2998245358467102\n",
      "[Train Epoch: 70, Batch: 8, Loss: 0.5601279735565186\n",
      "[TRAIN] Epoch: 70 Loss:0.3663085028529167\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.42249876260757446, Accuracy: 75.0%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.49294525384902954, Accuracy: 82.75862121582031%\n",
      "============\n",
      "[Validation set] Loss: 0.457722008228302, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 71, Batch: 1, Loss: 0.3920099139213562\n",
      "[Train Epoch: 71, Batch: 2, Loss: 0.38145458698272705\n",
      "[Train Epoch: 71, Batch: 3, Loss: 0.26432308554649353\n",
      "[Train Epoch: 71, Batch: 4, Loss: 0.30967605113983154\n",
      "[Train Epoch: 71, Batch: 5, Loss: 0.5557706952095032\n",
      "[Train Epoch: 71, Batch: 6, Loss: 0.2623157501220703\n",
      "[Train Epoch: 71, Batch: 7, Loss: 0.3607226610183716\n",
      "[Train Epoch: 71, Batch: 8, Loss: 0.27519094944000244\n",
      "[TRAIN] Epoch: 71 Loss:0.3501829616725445\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.48286816477775574, Accuracy: 78.125%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.42670953273773193, Accuracy: 79.31034851074219%\n",
      "============\n",
      "[Validation set] Loss: 0.45478884875774384, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 72, Batch: 1, Loss: 0.5210983157157898\n",
      "[Train Epoch: 72, Batch: 2, Loss: 0.3712317943572998\n",
      "[Train Epoch: 72, Batch: 3, Loss: 0.2832413911819458\n",
      "[Train Epoch: 72, Batch: 4, Loss: 0.2993491291999817\n",
      "[Train Epoch: 72, Batch: 5, Loss: 0.37571829557418823\n",
      "[Train Epoch: 72, Batch: 6, Loss: 0.2561277747154236\n",
      "[Train Epoch: 72, Batch: 7, Loss: 0.3698805272579193\n",
      "[Train Epoch: 72, Batch: 8, Loss: 0.33074894547462463\n",
      "[TRAIN] Epoch: 72 Loss:0.3509245216846466\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.42197543382644653, Accuracy: 84.375%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.4915800392627716, Accuracy: 72.4137954711914%\n",
      "============\n",
      "[Validation set] Loss: 0.45677773654460907, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 73, Batch: 1, Loss: 0.29663437604904175\n",
      "[Train Epoch: 73, Batch: 2, Loss: 0.5099995136260986\n",
      "[Train Epoch: 73, Batch: 3, Loss: 0.36873123049736023\n",
      "[Train Epoch: 73, Batch: 4, Loss: 0.2704697847366333\n",
      "[Train Epoch: 73, Batch: 5, Loss: 0.33217108249664307\n",
      "[Train Epoch: 73, Batch: 6, Loss: 0.3424326479434967\n",
      "[Train Epoch: 73, Batch: 7, Loss: 0.39015477895736694\n",
      "[Train Epoch: 73, Batch: 8, Loss: 0.24102777242660522\n",
      "[TRAIN] Epoch: 73 Loss:0.34395264834165573\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.3855447471141815, Accuracy: 84.375%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.5335189700126648, Accuracy: 72.4137954711914%\n",
      "============\n",
      "[Validation set] Loss: 0.45953185856342316, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 74, Batch: 1, Loss: 0.2349402904510498\n",
      "[Train Epoch: 74, Batch: 2, Loss: 0.3411494195461273\n",
      "[Train Epoch: 74, Batch: 3, Loss: 0.31698212027549744\n",
      "[Train Epoch: 74, Batch: 4, Loss: 0.3942944407463074\n",
      "[Train Epoch: 74, Batch: 5, Loss: 0.4520037770271301\n",
      "[Train Epoch: 74, Batch: 6, Loss: 0.2916457951068878\n",
      "[Train Epoch: 74, Batch: 7, Loss: 0.2964959144592285\n",
      "[Train Epoch: 74, Batch: 8, Loss: 0.5534893274307251\n",
      "[TRAIN] Epoch: 74 Loss:0.3601251356303692\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.48829904198646545, Accuracy: 81.25%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.4261789619922638, Accuracy: 75.86206817626953%\n",
      "============\n",
      "[Validation set] Loss: 0.4572390019893646, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 75, Batch: 1, Loss: 0.3849950432777405\n",
      "[Train Epoch: 75, Batch: 2, Loss: 0.2986789345741272\n",
      "[Train Epoch: 75, Batch: 3, Loss: 0.4614238739013672\n",
      "[Train Epoch: 75, Batch: 4, Loss: 0.2889818549156189\n",
      "[Train Epoch: 75, Batch: 5, Loss: 0.3426138460636139\n",
      "[Train Epoch: 75, Batch: 6, Loss: 0.3693573474884033\n",
      "[Train Epoch: 75, Batch: 7, Loss: 0.30666372179985046\n",
      "[Train Epoch: 75, Batch: 8, Loss: 0.30739691853523254\n",
      "[TRAIN] Epoch: 75 Loss:0.34501394256949425\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.40542346239089966, Accuracy: 81.25%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.5128603577613831, Accuracy: 75.86206817626953%\n",
      "============\n",
      "[Validation set] Loss: 0.45914191007614136, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 76, Batch: 1, Loss: 0.2924274504184723\n",
      "[Train Epoch: 76, Batch: 2, Loss: 0.3867841362953186\n",
      "[Train Epoch: 76, Batch: 3, Loss: 0.34244996309280396\n",
      "[Train Epoch: 76, Batch: 4, Loss: 0.34772318601608276\n",
      "[Train Epoch: 76, Batch: 5, Loss: 0.3060286045074463\n",
      "[Train Epoch: 76, Batch: 6, Loss: 0.37793102860450745\n",
      "[Train Epoch: 76, Batch: 7, Loss: 0.26673391461372375\n",
      "[Train Epoch: 76, Batch: 8, Loss: 0.5224334001541138\n",
      "[TRAIN] Epoch: 76 Loss:0.3553139604628086\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.46837013959884644, Accuracy: 78.125%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.443205326795578, Accuracy: 79.31034851074219%\n",
      "============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation set] Loss: 0.4557877331972122, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 77, Batch: 1, Loss: 0.3588218688964844\n",
      "[Train Epoch: 77, Batch: 2, Loss: 0.2945292890071869\n",
      "[Train Epoch: 77, Batch: 3, Loss: 0.38469862937927246\n",
      "[Train Epoch: 77, Batch: 4, Loss: 0.31719037890434265\n",
      "[Train Epoch: 77, Batch: 5, Loss: 0.35508111119270325\n",
      "[Train Epoch: 77, Batch: 6, Loss: 0.31152406334877014\n",
      "[Train Epoch: 77, Batch: 7, Loss: 0.3455851674079895\n",
      "[Train Epoch: 77, Batch: 8, Loss: 0.41523224115371704\n",
      "[TRAIN] Epoch: 77 Loss:0.3478328436613083\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.3983181416988373, Accuracy: 84.375%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.5215581059455872, Accuracy: 72.4137954711914%\n",
      "============\n",
      "[Validation set] Loss: 0.4599381238222122, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 78, Batch: 1, Loss: 0.4816962778568268\n",
      "[Train Epoch: 78, Batch: 2, Loss: 0.2668308913707733\n",
      "[Train Epoch: 78, Batch: 3, Loss: 0.4518175721168518\n",
      "[Train Epoch: 78, Batch: 4, Loss: 0.3008721172809601\n",
      "[Train Epoch: 78, Batch: 5, Loss: 0.2910068929195404\n",
      "[Train Epoch: 78, Batch: 6, Loss: 0.30632469058036804\n",
      "[Train Epoch: 78, Batch: 7, Loss: 0.3215228319168091\n",
      "[Train Epoch: 78, Batch: 8, Loss: 0.3047674298286438\n",
      "[TRAIN] Epoch: 78 Loss:0.34060483798384666\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5439919233322144, Accuracy: 78.125%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.3628194332122803, Accuracy: 79.31034851074219%\n",
      "============\n",
      "[Validation set] Loss: 0.4534056782722473, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 79, Batch: 1, Loss: 0.34119269251823425\n",
      "[Train Epoch: 79, Batch: 2, Loss: 0.2261342853307724\n",
      "[Train Epoch: 79, Batch: 3, Loss: 0.41163894534111023\n",
      "[Train Epoch: 79, Batch: 4, Loss: 0.294545441865921\n",
      "[Train Epoch: 79, Batch: 5, Loss: 0.2712675929069519\n",
      "[Train Epoch: 79, Batch: 6, Loss: 0.38810160756111145\n",
      "[Train Epoch: 79, Batch: 7, Loss: 0.45117008686065674\n",
      "[Train Epoch: 79, Batch: 8, Loss: 0.35010001063346863\n",
      "[TRAIN] Epoch: 79 Loss:0.34176883287727833\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.43003806471824646, Accuracy: 75.0%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.49045130610466003, Accuracy: 82.75862121582031%\n",
      "============\n",
      "[Validation set] Loss: 0.46024468541145325, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 80, Batch: 1, Loss: 0.28386279940605164\n",
      "[Train Epoch: 80, Batch: 2, Loss: 0.29814469814300537\n",
      "[Train Epoch: 80, Batch: 3, Loss: 0.41493451595306396\n",
      "[Train Epoch: 80, Batch: 4, Loss: 0.39517930150032043\n",
      "[Train Epoch: 80, Batch: 5, Loss: 0.3427748680114746\n",
      "[Train Epoch: 80, Batch: 6, Loss: 0.3863562345504761\n",
      "[Train Epoch: 80, Batch: 7, Loss: 0.33245420455932617\n",
      "[Train Epoch: 80, Batch: 8, Loss: 0.21101662516593933\n",
      "[TRAIN] Epoch: 80 Loss:0.3330904059112072\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5308188199996948, Accuracy: 75.0%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.37920302152633667, Accuracy: 82.75862121582031%\n",
      "============\n",
      "[Validation set] Loss: 0.45501092076301575, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 81, Batch: 1, Loss: 0.2445947825908661\n",
      "[Train Epoch: 81, Batch: 2, Loss: 0.3521704375743866\n",
      "[Train Epoch: 81, Batch: 3, Loss: 0.5091279149055481\n",
      "[Train Epoch: 81, Batch: 4, Loss: 0.3276607394218445\n",
      "[Train Epoch: 81, Batch: 5, Loss: 0.3358316421508789\n",
      "[Train Epoch: 81, Batch: 6, Loss: 0.22785310447216034\n",
      "[Train Epoch: 81, Batch: 7, Loss: 0.2778530716896057\n",
      "[Train Epoch: 81, Batch: 8, Loss: 0.5080915093421936\n",
      "[TRAIN] Epoch: 81 Loss:0.3478979002684355\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.46367400884628296, Accuracy: 71.875%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.4548260569572449, Accuracy: 86.20689392089844%\n",
      "============\n",
      "[Validation set] Loss: 0.4592500329017639, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 82, Batch: 1, Loss: 0.3581492602825165\n",
      "[Train Epoch: 82, Batch: 2, Loss: 0.4153625965118408\n",
      "[Train Epoch: 82, Batch: 3, Loss: 0.30077314376831055\n",
      "[Train Epoch: 82, Batch: 4, Loss: 0.2448975294828415\n",
      "[Train Epoch: 82, Batch: 5, Loss: 0.36801204085350037\n",
      "[Train Epoch: 82, Batch: 6, Loss: 0.3240705728530884\n",
      "[Train Epoch: 82, Batch: 7, Loss: 0.36587411165237427\n",
      "[Train Epoch: 82, Batch: 8, Loss: 0.3131142854690552\n",
      "[TRAIN] Epoch: 82 Loss:0.33628169260919094\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.6465832591056824, Accuracy: 68.75%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.25564637780189514, Accuracy: 89.6551742553711%\n",
      "============\n",
      "[Validation set] Loss: 0.45111481845378876, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 83, Batch: 1, Loss: 0.4602913558483124\n",
      "[Train Epoch: 83, Batch: 2, Loss: 0.40930402278900146\n",
      "[Train Epoch: 83, Batch: 3, Loss: 0.2316150665283203\n",
      "[Train Epoch: 83, Batch: 4, Loss: 0.3017682731151581\n",
      "[Train Epoch: 83, Batch: 5, Loss: 0.35414382815361023\n",
      "[Train Epoch: 83, Batch: 6, Loss: 0.27459219098091125\n",
      "[Train Epoch: 83, Batch: 7, Loss: 0.2605062425136566\n",
      "[Train Epoch: 83, Batch: 8, Loss: 0.44516319036483765\n",
      "[TRAIN] Epoch: 83 Loss:0.342173021286726\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.49795860052108765, Accuracy: 81.25%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.4214886426925659, Accuracy: 75.86206817626953%\n",
      "============\n",
      "[Validation set] Loss: 0.4597236216068268, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 84, Batch: 1, Loss: 0.24120080471038818\n",
      "[Train Epoch: 84, Batch: 2, Loss: 0.23223885893821716\n",
      "[Train Epoch: 84, Batch: 3, Loss: 0.3649349808692932\n",
      "[Train Epoch: 84, Batch: 4, Loss: 0.3700229227542877\n",
      "[Train Epoch: 84, Batch: 5, Loss: 0.5126294493675232\n",
      "[Train Epoch: 84, Batch: 6, Loss: 0.2732027769088745\n",
      "[Train Epoch: 84, Batch: 7, Loss: 0.2580224871635437\n",
      "[Train Epoch: 84, Batch: 8, Loss: 0.5022043585777283\n",
      "[TRAIN] Epoch: 84 Loss:0.344307079911232\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.6516531705856323, Accuracy: 68.75%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.24874497950077057, Accuracy: 89.6551742553711%\n",
      "============\n",
      "[Validation set] Loss: 0.45019907504320145, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 85, Batch: 1, Loss: 0.261886864900589\n",
      "[Train Epoch: 85, Batch: 2, Loss: 0.2963787615299225\n",
      "[Train Epoch: 85, Batch: 3, Loss: 0.37799495458602905\n",
      "[Train Epoch: 85, Batch: 4, Loss: 0.2766948640346527\n",
      "[Train Epoch: 85, Batch: 5, Loss: 0.3720943033695221\n",
      "[Train Epoch: 85, Batch: 6, Loss: 0.3661535084247589\n",
      "[Train Epoch: 85, Batch: 7, Loss: 0.3159671425819397\n",
      "[Train Epoch: 85, Batch: 8, Loss: 0.4680587947368622\n",
      "[TRAIN] Epoch: 85 Loss:0.3419036492705345\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.4921160042285919, Accuracy: 81.25%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.42971932888031006, Accuracy: 75.86206817626953%\n",
      "============\n",
      "[Validation set] Loss: 0.460917666554451, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 86, Batch: 1, Loss: 0.5073651671409607\n",
      "[Train Epoch: 86, Batch: 2, Loss: 0.3283112645149231\n",
      "[Train Epoch: 86, Batch: 3, Loss: 0.3176920413970947\n",
      "[Train Epoch: 86, Batch: 4, Loss: 0.2919492721557617\n",
      "[Train Epoch: 86, Batch: 5, Loss: 0.33838486671447754\n",
      "[Train Epoch: 86, Batch: 6, Loss: 0.2854977250099182\n",
      "[Train Epoch: 86, Batch: 7, Loss: 0.30615097284317017\n",
      "[Train Epoch: 86, Batch: 8, Loss: 0.27206987142562866\n",
      "[TRAIN] Epoch: 86 Loss:0.33092764765024185\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.4339181184768677, Accuracy: 78.125%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.49478965997695923, Accuracy: 79.31034851074219%\n",
      "============\n",
      "[Validation set] Loss: 0.46435388922691345, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 87, Batch: 1, Loss: 0.35112282633781433\n",
      "[Train Epoch: 87, Batch: 2, Loss: 0.23912781476974487\n",
      "[Train Epoch: 87, Batch: 3, Loss: 0.35654258728027344\n",
      "[Train Epoch: 87, Batch: 4, Loss: 0.39872434735298157\n",
      "[Train Epoch: 87, Batch: 5, Loss: 0.27002429962158203\n",
      "[Train Epoch: 87, Batch: 6, Loss: 0.40995603799819946\n",
      "[Train Epoch: 87, Batch: 7, Loss: 0.24459213018417358\n",
      "[Train Epoch: 87, Batch: 8, Loss: 0.4257623553276062\n",
      "[TRAIN] Epoch: 87 Loss:0.33698154985904694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.6157645583152771, Accuracy: 71.875%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.2957271337509155, Accuracy: 86.20689392089844%\n",
      "============\n",
      "[Validation set] Loss: 0.4557458460330963, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 88, Batch: 1, Loss: 0.2725224494934082\n",
      "[Train Epoch: 88, Batch: 2, Loss: 0.30247482657432556\n",
      "[Train Epoch: 88, Batch: 3, Loss: 0.3212919533252716\n",
      "[Train Epoch: 88, Batch: 4, Loss: 0.3327321708202362\n",
      "[Train Epoch: 88, Batch: 5, Loss: 0.3603232800960541\n",
      "[Train Epoch: 88, Batch: 6, Loss: 0.36291489005088806\n",
      "[Train Epoch: 88, Batch: 7, Loss: 0.3153742551803589\n",
      "[Train Epoch: 88, Batch: 8, Loss: 0.4218718409538269\n",
      "[TRAIN] Epoch: 88 Loss:0.3361882083117962\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5764745473861694, Accuracy: 78.125%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.33989468216896057, Accuracy: 79.31034851074219%\n",
      "============\n",
      "[Validation set] Loss: 0.458184614777565, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 89, Batch: 1, Loss: 0.25519245862960815\n",
      "[Train Epoch: 89, Batch: 2, Loss: 0.22603750228881836\n",
      "[Train Epoch: 89, Batch: 3, Loss: 0.32323622703552246\n",
      "[Train Epoch: 89, Batch: 4, Loss: 0.3469974100589752\n",
      "[Train Epoch: 89, Batch: 5, Loss: 0.34573233127593994\n",
      "[Train Epoch: 89, Batch: 6, Loss: 0.25102096796035767\n",
      "[Train Epoch: 89, Batch: 7, Loss: 0.43772467970848083\n",
      "[Train Epoch: 89, Batch: 8, Loss: 0.5561649799346924\n",
      "[TRAIN] Epoch: 89 Loss:0.3427633196115494\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.35054105520248413, Accuracy: 84.375%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.5948435664176941, Accuracy: 72.4137954711914%\n",
      "============\n",
      "[Validation set] Loss: 0.4726923108100891, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 90, Batch: 1, Loss: 0.3373691737651825\n",
      "[Train Epoch: 90, Batch: 2, Loss: 0.20395493507385254\n",
      "[Train Epoch: 90, Batch: 3, Loss: 0.3987901508808136\n",
      "[Train Epoch: 90, Batch: 4, Loss: 0.3788180351257324\n",
      "[Train Epoch: 90, Batch: 5, Loss: 0.46127721667289734\n",
      "[Train Epoch: 90, Batch: 6, Loss: 0.29389533400535583\n",
      "[Train Epoch: 90, Batch: 7, Loss: 0.2791600525379181\n",
      "[Train Epoch: 90, Batch: 8, Loss: 0.2378767877817154\n",
      "[TRAIN] Epoch: 90 Loss:0.32389271073043346\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.2961949408054352, Accuracy: 87.5%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.6577858328819275, Accuracy: 68.96551513671875%\n",
      "============\n",
      "[Validation set] Loss: 0.47699038684368134, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 91, Batch: 1, Loss: 0.28574419021606445\n",
      "[Train Epoch: 91, Batch: 2, Loss: 0.3242107331752777\n",
      "[Train Epoch: 91, Batch: 3, Loss: 0.3955269455909729\n",
      "[Train Epoch: 91, Batch: 4, Loss: 0.3618413805961609\n",
      "[Train Epoch: 91, Batch: 5, Loss: 0.26874077320098877\n",
      "[Train Epoch: 91, Batch: 6, Loss: 0.2413354367017746\n",
      "[Train Epoch: 91, Batch: 7, Loss: 0.3075290620326996\n",
      "[Train Epoch: 91, Batch: 8, Loss: 0.5250616669654846\n",
      "[TRAIN] Epoch: 91 Loss:0.33874877355992794\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5725208520889282, Accuracy: 78.125%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.35093480348587036, Accuracy: 79.31034851074219%\n",
      "============\n",
      "[Validation set] Loss: 0.4617278277873993, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 92, Batch: 1, Loss: 0.22339853644371033\n",
      "[Train Epoch: 92, Batch: 2, Loss: 0.26578089594841003\n",
      "[Train Epoch: 92, Batch: 3, Loss: 0.2719535827636719\n",
      "[Train Epoch: 92, Batch: 4, Loss: 0.38257133960723877\n",
      "[Train Epoch: 92, Batch: 5, Loss: 0.29056644439697266\n",
      "[Train Epoch: 92, Batch: 6, Loss: 0.521083652973175\n",
      "[Train Epoch: 92, Batch: 7, Loss: 0.33137404918670654\n",
      "[Train Epoch: 92, Batch: 8, Loss: 0.34009280800819397\n",
      "[TRAIN] Epoch: 92 Loss:0.3283526636660099\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5259100794792175, Accuracy: 78.125%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.4049692451953888, Accuracy: 79.31034851074219%\n",
      "============\n",
      "[Validation set] Loss: 0.46543966233730316, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 93, Batch: 1, Loss: 0.30444493889808655\n",
      "[Train Epoch: 93, Batch: 2, Loss: 0.21775974333286285\n",
      "[Train Epoch: 93, Batch: 3, Loss: 0.36598044633865356\n",
      "[Train Epoch: 93, Batch: 4, Loss: 0.35011744499206543\n",
      "[Train Epoch: 93, Batch: 5, Loss: 0.3058171570301056\n",
      "[Train Epoch: 93, Batch: 6, Loss: 0.3321862816810608\n",
      "[Train Epoch: 93, Batch: 7, Loss: 0.38289394974708557\n",
      "[Train Epoch: 93, Batch: 8, Loss: 0.37282177805900574\n",
      "[TRAIN] Epoch: 93 Loss:0.32900271750986576\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.4385567605495453, Accuracy: 71.875%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.5016175508499146, Accuracy: 86.20689392089844%\n",
      "============\n",
      "[Validation set] Loss: 0.4700871556997299, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 94, Batch: 1, Loss: 0.37123903632164\n",
      "[Train Epoch: 94, Batch: 2, Loss: 0.23278653621673584\n",
      "[Train Epoch: 94, Batch: 3, Loss: 0.24253979325294495\n",
      "[Train Epoch: 94, Batch: 4, Loss: 0.4034654498100281\n",
      "[Train Epoch: 94, Batch: 5, Loss: 0.42016667127609253\n",
      "[Train Epoch: 94, Batch: 6, Loss: 0.3209962546825409\n",
      "[Train Epoch: 94, Batch: 7, Loss: 0.30509576201438904\n",
      "[Train Epoch: 94, Batch: 8, Loss: 0.29212987422943115\n",
      "[TRAIN] Epoch: 94 Loss:0.3235524222254753\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.3903740346431732, Accuracy: 84.375%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.555188775062561, Accuracy: 72.4137954711914%\n",
      "============\n",
      "[Validation set] Loss: 0.4727814048528671, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 95, Batch: 1, Loss: 0.3806813359260559\n",
      "[Train Epoch: 95, Batch: 2, Loss: 0.36589929461479187\n",
      "[Train Epoch: 95, Batch: 3, Loss: 0.316264808177948\n",
      "[Train Epoch: 95, Batch: 4, Loss: 0.3671606481075287\n",
      "[Train Epoch: 95, Batch: 5, Loss: 0.2738068699836731\n",
      "[Train Epoch: 95, Batch: 6, Loss: 0.25863116979599\n",
      "[Train Epoch: 95, Batch: 7, Loss: 0.2920362651348114\n",
      "[Train Epoch: 95, Batch: 8, Loss: 0.3524717092514038\n",
      "[TRAIN] Epoch: 95 Loss:0.32586901262402534\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5890148282051086, Accuracy: 75.0%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.3364546298980713, Accuracy: 82.75862121582031%\n",
      "============\n",
      "[Validation set] Loss: 0.46273472905158997, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 96, Batch: 1, Loss: 0.42446839809417725\n",
      "[Train Epoch: 96, Batch: 2, Loss: 0.3335320055484772\n",
      "[Train Epoch: 96, Batch: 3, Loss: 0.30003267526626587\n",
      "[Train Epoch: 96, Batch: 4, Loss: 0.2897481620311737\n",
      "[Train Epoch: 96, Batch: 5, Loss: 0.29179877042770386\n",
      "[Train Epoch: 96, Batch: 6, Loss: 0.25934138894081116\n",
      "[Train Epoch: 96, Batch: 7, Loss: 0.35753974318504333\n",
      "[Train Epoch: 96, Batch: 8, Loss: 0.34044796228408813\n",
      "[TRAIN] Epoch: 96 Loss:0.32461363822221756\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.42133891582489014, Accuracy: 81.25%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.5217815041542053, Accuracy: 75.86206817626953%\n",
      "============\n",
      "[Validation set] Loss: 0.47156020998954773, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 97, Batch: 1, Loss: 0.34129709005355835\n",
      "[Train Epoch: 97, Batch: 2, Loss: 0.2573919892311096\n",
      "[Train Epoch: 97, Batch: 3, Loss: 0.35167980194091797\n",
      "[Train Epoch: 97, Batch: 4, Loss: 0.3380039632320404\n",
      "[Train Epoch: 97, Batch: 5, Loss: 0.3859349489212036\n",
      "[Train Epoch: 97, Batch: 6, Loss: 0.44139352440834045\n",
      "[Train Epoch: 97, Batch: 7, Loss: 0.19895093142986298\n",
      "[Train Epoch: 97, Batch: 8, Loss: 0.22293895483016968\n",
      "[TRAIN] Epoch: 97 Loss:0.3171989005059004\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5416885614395142, Accuracy: 71.875%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.3941870331764221, Accuracy: 86.20689392089844%\n",
      "============\n",
      "[Validation set] Loss: 0.46793779730796814, Accuracy: 78.68852233886719%\n",
      "[Train Epoch: 98, Batch: 1, Loss: 0.45561859011650085\n",
      "[Train Epoch: 98, Batch: 2, Loss: 0.41450756788253784\n",
      "[Train Epoch: 98, Batch: 3, Loss: 0.30715566873550415\n",
      "[Train Epoch: 98, Batch: 4, Loss: 0.25534358620643616\n",
      "[Train Epoch: 98, Batch: 5, Loss: 0.3635222017765045\n",
      "[Train Epoch: 98, Batch: 6, Loss: 0.20945623517036438\n",
      "[Train Epoch: 98, Batch: 7, Loss: 0.27698662877082825\n",
      "[Train Epoch: 98, Batch: 8, Loss: 0.27160125970840454\n",
      "[TRAIN] Epoch: 98 Loss:0.3192739672958851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.6215940713882446, Accuracy: 71.875%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.3115118145942688, Accuracy: 89.6551742553711%\n",
      "============\n",
      "[Validation set] Loss: 0.4665529429912567, Accuracy: 80.32786560058594%\n",
      "[Train Epoch: 99, Batch: 1, Loss: 0.19623126089572906\n",
      "[Train Epoch: 99, Batch: 2, Loss: 0.4478978216648102\n",
      "[Train Epoch: 99, Batch: 3, Loss: 0.3219566345214844\n",
      "[Train Epoch: 99, Batch: 4, Loss: 0.2738143801689148\n",
      "[Train Epoch: 99, Batch: 5, Loss: 0.28803807497024536\n",
      "[Train Epoch: 99, Batch: 6, Loss: 0.20859335362911224\n",
      "[Train Epoch: 99, Batch: 7, Loss: 0.3842211365699768\n",
      "[Train Epoch: 99, Batch: 8, Loss: 0.5546473860740662\n",
      "[TRAIN] Epoch: 99 Loss:0.3344250060617924\n",
      "============\n",
      "[Validation set] Batch index: 1 Batch loss: 0.5406233668327332, Accuracy: 84.375%\n",
      "============\n",
      "============\n",
      "[Validation set] Batch index: 2 Batch loss: 0.4027944505214691, Accuracy: 75.86206817626953%\n",
      "============\n",
      "[Validation set] Loss: 0.47170890867710114, Accuracy: 80.32786560058594%\n"
     ]
    }
   ],
   "source": [
    "#după fiecare epocă de train() verificăm rezultatele pe setul de validare\n",
    "for epoch in range(100):\n",
    "  train(epoch)\n",
    "  validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CNO72JVEjVy1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f961ed74d90>,\n",
       " <matplotlib.lines.Line2D at 0x7f961ed7ea50>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Printăm comparativ cu roșu loss-ul de pe setul de validare și cu albastru loss-ul de pe setul de validare.\n",
    "plt.plot(train_losses, \"r-\", test_losses, \"b-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJk5Y0GatLMr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f961649f7d0>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a9oNbLYZfSTU"
   },
   "outputs": [],
   "source": [
    "#Let's try and feed a single example in the neural network and see if it gets it right\n",
    "def try_a_single_example_with_the_network(index_from_the_validation_set):\n",
    "  with torch.no_grad():\n",
    "    pred_test = net(validationDataset[index_from_the_validation_set][0].view(1, -1))\n",
    "    _, preds_y = torch.max(pred_test, 1)\n",
    "    return preds_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FGj9CYP6yfZz"
   },
   "outputs": [],
   "source": [
    "index_of_test=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UnsqqEqMuI67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network response is: 0\n",
      "Actual response is: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Neural network response is: {try_a_single_example_with_the_network(index_of_test).item()}\")\n",
    "print(f\"Actual response is: {validationDataset[index_of_test][1].view(-1).item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pVbvWrE2uI9q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UwjpjFuifSTi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "MiniBatchHeartDiseasePytorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
