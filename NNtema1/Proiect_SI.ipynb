{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rFg7hNhp2nra"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "  # Our activation function: f(x) = 1 / (1 + e^(-x))\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class Neuron:\n",
    "  def __init__(self, weights, bias):\n",
    "    self.weights = weights\n",
    "    self.bias = bias\n",
    "\n",
    "  def feedforward(self, inputs):\n",
    "    # Weight inputs, add bias, then use the activation function\n",
    "    total = np.dot(self.weights, inputs) + self.bias\n",
    "    return sigmoid(total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hJAhWLrY4RL1",
    "outputId": "380d601c-adc9-4849-8c64-2bd41849aca9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7216325609518421\n"
     ]
    }
   ],
   "source": [
    "class OurNeuralNetwork:\n",
    "  '''\n",
    "  A neural network with:\n",
    "    - 2 inputs\n",
    "    - a hidden layer with 2 neurons (h1, h2)\n",
    "    - an output layer with 1 neuron (o1)\n",
    "  Each neuron has the same weights and bias:\n",
    "    - w = [0, 1]\n",
    "    - b = 0\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    weights = np.array([0, 1])\n",
    "    bias = 0\n",
    "\n",
    "    # The Neuron class here is from the previous section\n",
    "    self.h1 = Neuron(weights, bias)\n",
    "    self.h2 = Neuron(weights, bias)\n",
    "    self.o1 = Neuron(weights, bias)\n",
    "\n",
    "  def feedforward(self, x):\n",
    "    out_h1 = self.h1.feedforward(x)\n",
    "    out_h2 = self.h2.feedforward(x)\n",
    "\n",
    "    # The inputs for o1 are the outputs from h1 and h2\n",
    "    out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "\n",
    "    return out_o1\n",
    "\n",
    "network = OurNeuralNetwork()\n",
    "x = np.array([2, 3])\n",
    "print(network.feedforward(x)) # 0.7216325609518421"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1MNsTWlsQ9vS"
   },
   "source": [
    "Relu activation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pWPPuMD-emy7"
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "  return max(0, x)\n",
    "\n",
    "def deriv_relu(x):\n",
    "  k  = relu(x)\n",
    "  if(k > 0):\n",
    "    return 1\n",
    "  return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EjYR16CUQ66i"
   },
   "source": [
    "Define a small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kX3_VBx8Q5_U"
   },
   "outputs": [],
   "source": [
    "# Define dataset\n",
    "data = np.array([\n",
    "  [-2, -1],  # Alice\n",
    "  [25, 6],   # Bob\n",
    "  [17, 4],   # Charlie\n",
    "  [-15, -6], # Diana\n",
    "])\n",
    "all_y_trues = np.array([\n",
    "  1, # Alice\n",
    "  0, # Bob\n",
    "  0, # Charlie\n",
    "  1, # Diana\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4OgoIvmBRFUr"
   },
   "source": [
    "Sigmoid and mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rlscy6xsREkc"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  # Sigmoid activation function: f(x) = 1 / (1 + e^(-x))\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "  # Derivative of sigmoid: f'(x) = f(x) * (1 - f(x))\n",
    "  fx = sigmoid(x)\n",
    "  return fx * (1 - fx)\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "  # y_true and y_pred are numpy arrays of the same length.\n",
    "  return ((y_true - y_pred) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ffYQB8flRH3y"
   },
   "source": [
    "Default neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6cCzIXrt4BG_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class OurNeuralNetwork:\n",
    "  '''\n",
    "  A neural network with:\n",
    "    - 2 inputs\n",
    "    - a hidden layer with 2 neurons (h1, h2)\n",
    "    - an output layer with 1 neuron (o1)\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    # Weights\n",
    "    self.w1 = np.random.normal()\n",
    "    self.w2 = np.random.normal()\n",
    "    self.w3 = np.random.normal()\n",
    "    self.w4 = np.random.normal()\n",
    "    self.w5 = np.random.normal()\n",
    "    self.w6 = np.random.normal()\n",
    "\n",
    "    # Biases\n",
    "    self.b1 = np.random.normal()\n",
    "    self.b2 = np.random.normal()\n",
    "    self.b3 = np.random.normal()\n",
    "\n",
    "  def feedforward(self, x):\n",
    "    # x is a numpy array with 2 elements.\n",
    "    h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
    "    h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
    "    o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
    "    return o1\n",
    "\n",
    "  def train(self, data, all_y_trues):\n",
    "    '''\n",
    "    - data is a (n x 2) numpy array, n = # of samples in the dataset.\n",
    "    - all_y_trues is a numpy array with n elements.\n",
    "      Elements in all_y_trues correspond to those in data.\n",
    "    '''\n",
    "    learn_rate = 0.1\n",
    "    epochs = 1000 # number of times to loop through the entire dataset\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      for x, y_true in zip(data, all_y_trues):\n",
    "        # --- Do a feedforward (we'll need these values later)\n",
    "        sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
    "        h1 = sigmoid(sum_h1)\n",
    "\n",
    "        sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
    "        h2 = sigmoid(sum_h2)\n",
    "\n",
    "        sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
    "        o1 = sigmoid(sum_o1)\n",
    "        y_pred = o1\n",
    "\n",
    "        # --- Calculate partial derivatives.\n",
    "        # --- Naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
    "        d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "\n",
    "        # Neuron o1\n",
    "        d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
    "\n",
    "        d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
    "\n",
    "        # Neuron h1\n",
    "        d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
    "        d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
    "        d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
    "\n",
    "        # Neuron h2\n",
    "        d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
    "        d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
    "        d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
    "\n",
    "        # --- Update weights and biases\n",
    "        # Neuron h1\n",
    "        self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
    "        self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
    "        self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
    "\n",
    "        # Neuron h2\n",
    "        self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
    "        self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
    "        self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
    "\n",
    "        # Neuron o1\n",
    "        self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
    "        self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6\n",
    "        self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
    "\n",
    "      # --- Calculate total loss at the end of each epoch\n",
    "      if epoch % 10 == 0:\n",
    "        y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "        loss = mse_loss(all_y_trues, y_preds)\n",
    "        print(\"Epoch %d loss: %.3f\" % (epoch, loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KeGHbFmkRSIT"
   },
   "outputs": [],
   "source": [
    "# Train our neural network!\n",
    "network = OurNeuralNetwork()\n",
    "network.train(data, all_y_trues)\n",
    "emily = np.array([-7, -3]) # 128 pounds, 63 inches\n",
    "frank = np.array([20, 2])  # 155 pounds, 68 inches\n",
    "claudia = np.array([-12, 2])\n",
    "jon = np.array([10, 7])\n",
    "print(\"Emily: %.3f\" % network.feedforward(emily)) # 0.951 - F\n",
    "print(\"Frank: %.3f\" % network.feedforward(frank)) # 0.039 - M\n",
    "print(\"Claudi: %.3f\" % network.feedforward(claudia)) # 0.951 - F\n",
    "print(\"Jon: %.3f\" % network.feedforward(jon)) # 0.039 - M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a07v9AG3RLOq"
   },
   "source": [
    "Neural network with relu activation function for the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i79C61_tfQxB"
   },
   "outputs": [],
   "source": [
    "class OurNeuralNetwork2:\n",
    "  '''\n",
    "    The relu activation can be used only inside the network (in the hidden layers). \n",
    "    Considering that the output should be a value in [0, 1] that represents the probability of the person\n",
    "    being a woman, relu output is inapropriate. I used relu for the hidden layer and sigmoid for the output layer\n",
    "\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    # Weights\n",
    "    self.w1 = np.random.normal()\n",
    "    self.w2 = np.random.normal()\n",
    "    self.w3 = np.random.normal()\n",
    "    self.w4 = np.random.normal()\n",
    "    self.w5 = np.random.normal()\n",
    "    self.w6 = np.random.normal()\n",
    "\n",
    "    # Biases\n",
    "    self.b1 = np.random.normal()\n",
    "    self.b2 = np.random.normal()\n",
    "    self.b3 = np.random.normal()\n",
    "\n",
    "  def feedforward(self, x):\n",
    "    # x is a numpy array with 2 elements.\n",
    "    h1 = relu(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
    "    h2 = relu(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
    "    o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
    "    return o1\n",
    "\n",
    "  def train(self, data, all_y_trues):\n",
    "    learn_rate = 0.10\n",
    "    epochs = 500 # number of times to loop through the entire dataset\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      for x, y_true in zip(data, all_y_trues):\n",
    "        # --- Do a feedforward (we'll need these values later)\n",
    "        sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
    "        h1 =relu(sum_h1)\n",
    "\n",
    "        sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
    "        h2 =relu(sum_h2)\n",
    "\n",
    "        sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
    "        o1 =sigmoid(sum_o1)\n",
    "        y_pred = o1\n",
    "\n",
    "        # --- Calculate partial derivatives.\n",
    "        # --- Naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
    "        d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "\n",
    "        # Neuron o1\n",
    "        d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
    "\n",
    "        d_ypred_d_h1 = self.w5 * deriv_relu(sum_o1)\n",
    "        d_ypred_d_h2 = self.w6 * deriv_relu(sum_o1)\n",
    "\n",
    "        # Neuron h1\n",
    "        d_h1_d_w1 = x[0] * deriv_relu(sum_h1)\n",
    "        d_h1_d_w2 = x[1] * deriv_relu(sum_h1)\n",
    "        d_h1_d_b1 = deriv_relu(sum_h1)\n",
    "\n",
    "        # Neuron h2\n",
    "        d_h2_d_w3 = x[0] * deriv_relu(sum_h2)\n",
    "        d_h2_d_w4 = x[1] * deriv_relu(sum_h2)\n",
    "        d_h2_d_b2 = deriv_relu(sum_h2)\n",
    "\n",
    "        # --- Update weights and biases\n",
    "        # Neuron h1\n",
    "        self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
    "        self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
    "        self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
    "\n",
    "        # Neuron h2\n",
    "        self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
    "        self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
    "        self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
    "\n",
    "        # Neuron o1\n",
    "        self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
    "        self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6\n",
    "        self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
    "\n",
    "      # --- Calculate total loss at the end of each epoch\n",
    "      if epoch % 10 == 0:\n",
    "        y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "        #print(y_preds)\n",
    "        #print(all_y_trues)\n",
    "        loss = mse_loss(all_y_trues, y_preds)\n",
    "        #print(str(self.w1) + \" \" + str(self.w2) + \" \" + str(self.w3) + \" \" + str(self.w4)+ \" \" + str(self.w5)+ \" \" + str(self.w6)+ \" \")\n",
    "        print(\"Epoch %d loss: %.3f\" % (epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 935
    },
    "colab_type": "code",
    "id": "c7qMPMgWf97z",
    "outputId": "fa3a8038-7fc5-47ef-db17-a7059c30159e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.395\n",
      "Epoch 10 loss: 0.357\n",
      "Epoch 20 loss: 0.297\n",
      "Epoch 30 loss: 0.217\n",
      "Epoch 40 loss: 0.140\n",
      "Epoch 50 loss: 0.087\n",
      "Epoch 60 loss: 0.057\n",
      "Epoch 70 loss: 0.040\n",
      "Epoch 80 loss: 0.030\n",
      "Epoch 90 loss: 0.023\n",
      "Epoch 100 loss: 0.019\n",
      "Epoch 110 loss: 0.016\n",
      "Epoch 120 loss: 0.013\n",
      "Epoch 130 loss: 0.012\n",
      "Epoch 140 loss: 0.010\n",
      "Epoch 150 loss: 0.009\n",
      "Epoch 160 loss: 0.008\n",
      "Epoch 170 loss: 0.008\n",
      "Epoch 180 loss: 0.007\n",
      "Epoch 190 loss: 0.006\n",
      "Epoch 200 loss: 0.006\n",
      "Epoch 210 loss: 0.005\n",
      "Epoch 220 loss: 0.005\n",
      "Epoch 230 loss: 0.005\n",
      "Epoch 240 loss: 0.005\n",
      "Epoch 250 loss: 0.004\n",
      "Epoch 260 loss: 0.004\n",
      "Epoch 270 loss: 0.004\n",
      "Epoch 280 loss: 0.004\n",
      "Epoch 290 loss: 0.003\n",
      "Epoch 300 loss: 0.003\n",
      "Epoch 310 loss: 0.003\n",
      "Epoch 320 loss: 0.003\n",
      "Epoch 330 loss: 0.003\n",
      "Epoch 340 loss: 0.003\n",
      "Epoch 350 loss: 0.003\n",
      "Epoch 360 loss: 0.003\n",
      "Epoch 370 loss: 0.003\n",
      "Epoch 380 loss: 0.002\n",
      "Epoch 390 loss: 0.002\n",
      "Epoch 400 loss: 0.002\n",
      "Epoch 410 loss: 0.002\n",
      "Epoch 420 loss: 0.002\n",
      "Epoch 430 loss: 0.002\n",
      "Epoch 440 loss: 0.002\n",
      "Epoch 450 loss: 0.002\n",
      "Epoch 460 loss: 0.002\n",
      "Epoch 470 loss: 0.002\n",
      "Epoch 480 loss: 0.002\n",
      "Epoch 490 loss: 0.002\n",
      "Emily: 1.000\n",
      "Frank: 0.059\n",
      "Claudi: 1.000\n",
      "Jon: 0.002\n"
     ]
    }
   ],
   "source": [
    "network2 = OurNeuralNetwork2()\n",
    "network2.train(data, all_y_trues)\n",
    "print(\"Emily: %.3f\" % network2.feedforward(emily)) # 0.951 - F\n",
    "print(\"Frank: %.3f\" % network2.feedforward(frank)) # 0.039 - M\n",
    "print(\"Claudi: %.3f\" % network2.feedforward(claudia)) # 0.951 - F\n",
    "print(\"Jon: %.3f\" % network2.feedforward(jon)) # 0.039 - M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zJBq2CLpRcX7"
   },
   "source": [
    "Add one more neuron in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l-vWGKu30l2l"
   },
   "outputs": [],
   "source": [
    "class OurNeuralNetwork3:\n",
    "  '''\n",
    "  This neural network has an extra neuron in the hidden layer\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    # Weights\n",
    "    self.w1 = np.random.normal()\n",
    "    self.w2 = np.random.normal()\n",
    "    self.w3 = np.random.normal()\n",
    "    self.w4 = np.random.normal()\n",
    "    self.w5 = np.random.normal()\n",
    "    self.w6 = np.random.normal()\n",
    "    self.w7 = np.random.normal()\n",
    "    self.w8 = np.random.normal()\n",
    "    self.w9 = np.random.normal()\n",
    "\n",
    "    # Biases\n",
    "    self.b1 = np.random.normal()\n",
    "    self.b2 = np.random.normal()\n",
    "    self.b3 = np.random.normal()\n",
    "    self.b4 = np.random.normal()\n",
    "\n",
    "  def feedforward(self, x):\n",
    "    # x is a numpy array with 2 elements.\n",
    "    h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
    "    h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
    "    h3 = sigmoid(self.w7 * x[0] + self.w8 * x[1] + self.b4)\n",
    "    o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.w9 * h3 + self.b3)\n",
    "    return o1\n",
    "\n",
    "  def train(self, data, all_y_trues):\n",
    "    '''\n",
    "    - data is a (n x 2) numpy array, n = # of samples in the dataset.\n",
    "    - all_y_trues is a numpy array with n elements.\n",
    "      Elements in all_y_trues correspond to those in data.\n",
    "    '''\n",
    "    learn_rate = 0.001\n",
    "    epochs = 1000 # number of times to loop through the entire dataset\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      for x, y_true in zip(data, all_y_trues):\n",
    "        # --- Do a feedforward (we'll need these values later)\n",
    "        sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
    "        h1 =sigmoid(sum_h1)\n",
    "\n",
    "        sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
    "        h2 =sigmoid(sum_h2)\n",
    "\n",
    "        sum_h3 = self.w7 * x[0] + self.w8 * x[1] + self.b4\n",
    "        h3 =sigmoid(sum_h3)\n",
    "\n",
    "        sum_o1 = self.w5 * h1 + self.w6 * h2 + self.w8 * h3 + self.b3\n",
    "        o1 =sigmoid(sum_o1)\n",
    "        y_pred = o1\n",
    "\n",
    "        # --- Calculate partial derivatives.\n",
    "        # --- Naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
    "        d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "\n",
    "        # Neuron o1\n",
    "        d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_w9 = h3 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
    "\n",
    "        d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_h3 = self.w9 * deriv_sigmoid(sum_o1)\n",
    "\n",
    "        # Neuron h1\n",
    "        d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
    "        d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
    "        d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
    "\n",
    "        # Neuron h2\n",
    "        d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
    "        d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
    "        d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
    "\n",
    "\n",
    "        # Neuron h3\n",
    "        d_h3_d_w7 = x[0] * deriv_sigmoid(sum_h3)\n",
    "        d_h3_d_w8 = x[1] * deriv_sigmoid(sum_h3)\n",
    "        d_h3_d_b4 = deriv_sigmoid(sum_h3)\n",
    "\n",
    "        # --- Update weights and biases\n",
    "        # Neuron h1\n",
    "        self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
    "        self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
    "        self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
    "\n",
    "        # Neuron h2\n",
    "        self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
    "        self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
    "        self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
    "\n",
    "        # Neuron h3\n",
    "        self.w7 -= learn_rate * d_L_d_ypred * d_ypred_d_h3 * d_h3_d_w7\n",
    "        self.w8 -= learn_rate * d_L_d_ypred * d_ypred_d_h3 * d_h3_d_w8\n",
    "        self.b4 -= learn_rate * d_L_d_ypred * d_ypred_d_h3 * d_h3_d_b4\n",
    "\n",
    "        # Neuron o1\n",
    "        self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
    "        self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6\n",
    "        self.w9 -= learn_rate * d_L_d_ypred * d_ypred_d_w9\n",
    "        self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
    "\n",
    "      # --- Calculate total loss at the end of each epoch\n",
    "      if epoch % 10 == 0:\n",
    "        y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "        loss = mse_loss(all_y_trues, y_preds)\n",
    "        print(\"Epoch %d loss: %.3f\" % (epoch, loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Uk6EaJwN2t2W",
    "outputId": "a7768a96-f57b-4baf-c5c3-d300476553ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.782\n",
      "Epoch 10 loss: 0.780\n",
      "Epoch 20 loss: 0.779\n",
      "Epoch 30 loss: 0.778\n",
      "Epoch 40 loss: 0.776\n",
      "Epoch 50 loss: 0.775\n",
      "Epoch 60 loss: 0.773\n",
      "Epoch 70 loss: 0.772\n",
      "Epoch 80 loss: 0.771\n",
      "Epoch 90 loss: 0.769\n",
      "Epoch 100 loss: 0.768\n",
      "Epoch 110 loss: 0.767\n",
      "Epoch 120 loss: 0.765\n",
      "Epoch 130 loss: 0.764\n",
      "Epoch 140 loss: 0.762\n",
      "Epoch 150 loss: 0.761\n",
      "Epoch 160 loss: 0.760\n",
      "Epoch 170 loss: 0.758\n",
      "Epoch 180 loss: 0.757\n",
      "Epoch 190 loss: 0.756\n",
      "Epoch 200 loss: 0.754\n",
      "Epoch 210 loss: 0.753\n",
      "Epoch 220 loss: 0.751\n",
      "Epoch 230 loss: 0.750\n",
      "Epoch 240 loss: 0.748\n",
      "Epoch 250 loss: 0.747\n",
      "Epoch 260 loss: 0.746\n",
      "Epoch 270 loss: 0.744\n",
      "Epoch 280 loss: 0.743\n",
      "Epoch 290 loss: 0.741\n",
      "Epoch 300 loss: 0.740\n",
      "Epoch 310 loss: 0.738\n",
      "Epoch 320 loss: 0.737\n",
      "Epoch 330 loss: 0.735\n",
      "Epoch 340 loss: 0.734\n",
      "Epoch 350 loss: 0.733\n",
      "Epoch 360 loss: 0.731\n",
      "Epoch 370 loss: 0.730\n",
      "Epoch 380 loss: 0.728\n",
      "Epoch 390 loss: 0.727\n",
      "Epoch 400 loss: 0.725\n",
      "Epoch 410 loss: 0.724\n",
      "Epoch 420 loss: 0.722\n",
      "Epoch 430 loss: 0.721\n",
      "Epoch 440 loss: 0.719\n",
      "Epoch 450 loss: 0.718\n",
      "Epoch 460 loss: 0.716\n",
      "Epoch 470 loss: 0.715\n",
      "Epoch 480 loss: 0.713\n",
      "Epoch 490 loss: 0.712\n",
      "Epoch 500 loss: 0.710\n",
      "Epoch 510 loss: 0.709\n",
      "Epoch 520 loss: 0.707\n",
      "Epoch 530 loss: 0.706\n",
      "Epoch 540 loss: 0.704\n",
      "Epoch 550 loss: 0.703\n",
      "Epoch 560 loss: 0.701\n",
      "Epoch 570 loss: 0.700\n",
      "Epoch 580 loss: 0.698\n",
      "Epoch 590 loss: 0.697\n",
      "Epoch 600 loss: 0.695\n",
      "Epoch 610 loss: 0.694\n",
      "Epoch 620 loss: 0.692\n",
      "Epoch 630 loss: 0.690\n",
      "Epoch 640 loss: 0.689\n",
      "Epoch 650 loss: 0.687\n",
      "Epoch 660 loss: 0.686\n",
      "Epoch 670 loss: 0.684\n",
      "Epoch 680 loss: 0.683\n",
      "Epoch 690 loss: 0.681\n",
      "Epoch 700 loss: 0.680\n",
      "Epoch 710 loss: 0.678\n",
      "Epoch 720 loss: 0.677\n",
      "Epoch 730 loss: 0.675\n",
      "Epoch 740 loss: 0.674\n",
      "Epoch 750 loss: 0.672\n",
      "Epoch 760 loss: 0.671\n",
      "Epoch 770 loss: 0.669\n",
      "Epoch 780 loss: 0.668\n",
      "Epoch 790 loss: 0.667\n",
      "Epoch 800 loss: 0.665\n",
      "Epoch 810 loss: 0.664\n",
      "Epoch 820 loss: 0.662\n",
      "Epoch 830 loss: 0.661\n",
      "Epoch 840 loss: 0.659\n",
      "Epoch 850 loss: 0.658\n",
      "Epoch 860 loss: 0.656\n",
      "Epoch 870 loss: 0.655\n",
      "Epoch 880 loss: 0.653\n",
      "Epoch 890 loss: 0.652\n",
      "Epoch 900 loss: 0.651\n",
      "Epoch 910 loss: 0.649\n",
      "Epoch 920 loss: 0.648\n",
      "Epoch 930 loss: 0.646\n",
      "Epoch 940 loss: 0.645\n",
      "Epoch 950 loss: 0.644\n",
      "Epoch 960 loss: 0.642\n",
      "Epoch 970 loss: 0.641\n",
      "Epoch 980 loss: 0.640\n",
      "Epoch 990 loss: 0.638\n",
      "Emily: 0.415\n",
      "Frank: 0.968\n",
      "Claudi: 0.406\n",
      "Jon: 0.966\n"
     ]
    }
   ],
   "source": [
    "network3 = OurNeuralNetwork3()\n",
    "network3.train(data, all_y_trues)\n",
    "print(\"Emily: %.3f\" % network3.feedforward(emily)) # 0.951 - F\n",
    "print(\"Frank: %.3f\" % network3.feedforward(frank)) # 0.039 - M\n",
    "print(\"Claudi: %.3f\" % network3.feedforward(claudia)) # 0.951 - F\n",
    "print(\"Jon: %.3f\" % network3.feedforward(jon)) # 0.039 - M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UdYM6p_ARh7S"
   },
   "source": [
    "Add an extra hidden layer. For the begining, I used the same model as above, writing the functions for each neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wJRE7C4GMDUl"
   },
   "outputs": [],
   "source": [
    "class OurNeuralNetwork41:\n",
    "  def __init__(self):\n",
    "    # Weights\n",
    "    self.w1 = np.random.normal()\n",
    "    self.w2 = np.random.normal()\n",
    "    self.w3 = np.random.normal()\n",
    "    self.w4 = np.random.normal()\n",
    "    self.w5 = np.random.normal()\n",
    "    self.w6 = np.random.normal()\n",
    "    self.w7 = np.random.normal()\n",
    "    self.w8 = np.random.normal()\n",
    "    self.w9 = np.random.normal()\n",
    "    self.w10 = np.random.normal()\n",
    "\n",
    "    # Biases\n",
    "    self.b1 = np.random.normal()\n",
    "    self.b2 = np.random.normal()\n",
    "    self.b3 = np.random.normal()\n",
    "    self.b4 = np.random.normal()\n",
    "    self.b5 = np.random.normal()\n",
    "\n",
    "  def feedforward(self, x):\n",
    "    # x is a numpy array with 2 elements.\n",
    "    h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
    "    h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
    "    h3 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
    "    h4 = sigmoid(self.w7 * h1 + self.w8 * h2 + self.b4)\n",
    "    o1 = sigmoid(self.w9 * h3 + self.w10 * h4 + self.b5)\n",
    "    return o1\n",
    "\n",
    "  def train(self, data, all_y_trues):\n",
    "    \n",
    "    learn_rate = 0.1\n",
    "    epochs = 1000 # number of times to loop through the entire dataset\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      for x, y_true in zip(data, all_y_trues):\n",
    "        # --- Do a feedforward (we'll need these values later)\n",
    "        sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
    "        h1 = sigmoid(sum_h1)\n",
    "\n",
    "        sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
    "        h2 = sigmoid(sum_h2)\n",
    "\n",
    "        sum_h3 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
    "        h3 = sigmoid(sum_h3)\n",
    "\n",
    "        sum_h4 = self.w7 * h1 + self.w8 * h2 + self.b4\n",
    "        h4 = sigmoid(sum_h4)\n",
    "\n",
    "        sum_o1 = self.w9 * h3 + self.w10 * h4 + self.b5\n",
    "        o1 = sigmoid(sum_o1)\n",
    "        y_pred = o1\n",
    "\n",
    "        # --- Calculate partial derivatives.\n",
    "        # --- Naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
    "        d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "\n",
    "        # Neuron o1\n",
    "        d_ypred_d_w9 = h3 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_w10 = h4 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_b5 = deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_h3 = self.w9 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_h4 = self.w10 * deriv_sigmoid(sum_o1)\n",
    "\n",
    "        #neuron h3\n",
    "        d_h3_d_w5 = h1 * deriv_sigmoid(sum_h3)\n",
    "        d_h3_d_w6 = h2 * deriv_sigmoid(sum_h3)\n",
    "        d_h3_d_h1 = self.w5 * deriv_sigmoid(sum_h3)\n",
    "        d_h3_d_h2 = self.w6 * deriv_sigmoid(sum_h3)\n",
    "        d_h3_d_b3 = deriv_sigmoid(sum_h3)\n",
    "\n",
    "        #neuron h4\n",
    "        d_h4_d_w7 = h1 * deriv_sigmoid(sum_h4)\n",
    "        d_h4_d_w8 = h2 * deriv_sigmoid(sum_h4)\n",
    "        d_h4_d_h1 = self.w7 * deriv_sigmoid(sum_h4)\n",
    "        d_h4_d_h2 = self.w8 * deriv_sigmoid(sum_h4)\n",
    "        d_h4_d_b4 = deriv_sigmoid(sum_h4)\n",
    "        \n",
    "        # Neuron h1\n",
    "        d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
    "        d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
    "        d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
    "\n",
    "        # Neuron h2\n",
    "        d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
    "        d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
    "        d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
    "\n",
    "        # --- Update weights and biases\n",
    "        # Neuron h1\n",
    "        self.w1 -= learn_rate * d_L_d_ypred * (d_ypred_d_h3 * d_h3_d_h1 +  d_ypred_d_h4 * d_h4_d_h1) * d_h1_d_w1 \n",
    "        self.w2 -= learn_rate * d_L_d_ypred *(d_ypred_d_h3 * d_h3_d_h1 +  d_ypred_d_h4 * d_h4_d_h1) * d_h1_d_w2\n",
    "        self.b1 -= learn_rate * d_L_d_ypred * (d_ypred_d_h3 * d_h3_d_h1 +  d_ypred_d_h4 * d_h4_d_h1) * d_h1_d_b1\n",
    "        # Neuron h2\n",
    "        self.w3 -= learn_rate * d_L_d_ypred * (d_ypred_d_h3 * d_h3_d_h2 +  d_ypred_d_h4 * d_h4_d_h2) * d_h2_d_w3 \n",
    "        self.w4 -= learn_rate * d_L_d_ypred *  (d_ypred_d_h3 * d_h3_d_h2 +  d_ypred_d_h4 * d_h4_d_h2) * d_h2_d_w4\n",
    "        self.b2 -= learn_rate * d_L_d_ypred *  (d_ypred_d_h3 * d_h3_d_h2 +  d_ypred_d_h4 * d_h4_d_h2) * d_h2_d_b2\n",
    "\n",
    "        # Neuron h3\n",
    "        self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_h3 * d_h3_d_w5\n",
    "        self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_h3 * d_h3_d_w6\n",
    "        self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_h3 * d_h3_d_b3\n",
    "\n",
    "        # Neuron h4\n",
    "        self.w7 -= learn_rate * d_L_d_ypred * d_ypred_d_h4 * d_h4_d_w7\n",
    "        self.w8 -= learn_rate * d_L_d_ypred * d_ypred_d_h4 * d_h4_d_w8\n",
    "        self.b4 -= learn_rate * d_L_d_ypred * d_ypred_d_h4 * d_h4_d_b4\n",
    "\n",
    "        # Neuron o1\n",
    "        self.w9 -= learn_rate * d_L_d_ypred * d_ypred_d_w9\n",
    "        self.w10 -= learn_rate * d_L_d_ypred * d_ypred_d_w10\n",
    "        self.b5 -= learn_rate * d_L_d_ypred * d_ypred_d_b5\n",
    "\n",
    "      # --- Calculate total loss at the end of each epoch\n",
    "      if epoch % 10 == 0:\n",
    "        y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "        loss = mse_loss(all_y_trues, y_preds)\n",
    "        print(\"Epoch %d loss: %.3f\" % (epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_w1VRG60TEk_",
    "outputId": "669ebbd7-df34-47a1-8cff-0704a64ae8d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.386\n",
      "Epoch 10 loss: 0.336\n",
      "Epoch 20 loss: 0.285\n",
      "Epoch 30 loss: 0.257\n",
      "Epoch 40 loss: 0.246\n",
      "Epoch 50 loss: 0.240\n",
      "Epoch 60 loss: 0.234\n",
      "Epoch 70 loss: 0.227\n",
      "Epoch 80 loss: 0.219\n",
      "Epoch 90 loss: 0.208\n",
      "Epoch 100 loss: 0.195\n",
      "Epoch 110 loss: 0.181\n",
      "Epoch 120 loss: 0.165\n",
      "Epoch 130 loss: 0.149\n",
      "Epoch 140 loss: 0.133\n",
      "Epoch 150 loss: 0.117\n",
      "Epoch 160 loss: 0.103\n",
      "Epoch 170 loss: 0.090\n",
      "Epoch 180 loss: 0.079\n",
      "Epoch 190 loss: 0.069\n",
      "Epoch 200 loss: 0.061\n",
      "Epoch 210 loss: 0.054\n",
      "Epoch 220 loss: 0.048\n",
      "Epoch 230 loss: 0.043\n",
      "Epoch 240 loss: 0.038\n",
      "Epoch 250 loss: 0.034\n",
      "Epoch 260 loss: 0.031\n",
      "Epoch 270 loss: 0.028\n",
      "Epoch 280 loss: 0.026\n",
      "Epoch 290 loss: 0.024\n",
      "Epoch 300 loss: 0.022\n",
      "Epoch 310 loss: 0.021\n",
      "Epoch 320 loss: 0.019\n",
      "Epoch 330 loss: 0.018\n",
      "Epoch 340 loss: 0.017\n",
      "Epoch 350 loss: 0.016\n",
      "Epoch 360 loss: 0.015\n",
      "Epoch 370 loss: 0.014\n",
      "Epoch 380 loss: 0.013\n",
      "Epoch 390 loss: 0.012\n",
      "Epoch 400 loss: 0.012\n",
      "Epoch 410 loss: 0.011\n",
      "Epoch 420 loss: 0.011\n",
      "Epoch 430 loss: 0.010\n",
      "Epoch 440 loss: 0.010\n",
      "Epoch 450 loss: 0.009\n",
      "Epoch 460 loss: 0.009\n",
      "Epoch 470 loss: 0.009\n",
      "Epoch 480 loss: 0.008\n",
      "Epoch 490 loss: 0.008\n",
      "Epoch 500 loss: 0.008\n",
      "Epoch 510 loss: 0.007\n",
      "Epoch 520 loss: 0.007\n",
      "Epoch 530 loss: 0.007\n",
      "Epoch 540 loss: 0.007\n",
      "Epoch 550 loss: 0.007\n",
      "Epoch 560 loss: 0.006\n",
      "Epoch 570 loss: 0.006\n",
      "Epoch 580 loss: 0.006\n",
      "Epoch 590 loss: 0.006\n",
      "Epoch 600 loss: 0.006\n",
      "Epoch 610 loss: 0.005\n",
      "Epoch 620 loss: 0.005\n",
      "Epoch 630 loss: 0.005\n",
      "Epoch 640 loss: 0.005\n",
      "Epoch 650 loss: 0.005\n",
      "Epoch 660 loss: 0.005\n",
      "Epoch 670 loss: 0.005\n",
      "Epoch 680 loss: 0.005\n",
      "Epoch 690 loss: 0.004\n",
      "Epoch 700 loss: 0.004\n",
      "Epoch 710 loss: 0.004\n",
      "Epoch 720 loss: 0.004\n",
      "Epoch 730 loss: 0.004\n",
      "Epoch 740 loss: 0.004\n",
      "Epoch 750 loss: 0.004\n",
      "Epoch 760 loss: 0.004\n",
      "Epoch 770 loss: 0.004\n",
      "Epoch 780 loss: 0.004\n",
      "Epoch 790 loss: 0.004\n",
      "Epoch 800 loss: 0.004\n",
      "Epoch 810 loss: 0.003\n",
      "Epoch 820 loss: 0.003\n",
      "Epoch 830 loss: 0.003\n",
      "Epoch 840 loss: 0.003\n",
      "Epoch 850 loss: 0.003\n",
      "Epoch 860 loss: 0.003\n",
      "Epoch 870 loss: 0.003\n",
      "Epoch 880 loss: 0.003\n",
      "Epoch 890 loss: 0.003\n",
      "Epoch 900 loss: 0.003\n",
      "Epoch 910 loss: 0.003\n",
      "Epoch 920 loss: 0.003\n",
      "Epoch 930 loss: 0.003\n",
      "Epoch 940 loss: 0.003\n",
      "Epoch 950 loss: 0.003\n",
      "Epoch 960 loss: 0.003\n",
      "Epoch 970 loss: 0.003\n",
      "Epoch 980 loss: 0.003\n",
      "Epoch 990 loss: 0.003\n",
      "Emily: 0.952\n",
      "Frank: 0.051\n",
      "Claudi: 0.868\n",
      "Jon: 0.051\n"
     ]
    }
   ],
   "source": [
    "network41 = OurNeuralNetwork41()\n",
    "network41.train(data, all_y_trues)\n",
    "emily = np.array([-7, -3]) # 128 pounds, 63 inches\n",
    "frank = np.array([20, 2])  # 155 pounds, 68 inches\n",
    "claudia = np.array([-12, 2])\n",
    "jon = np.array([10, 7])\n",
    "print(\"Emily: %.3f\" % network41.feedforward(emily)) # 0.951 - F\n",
    "print(\"Frank: %.3f\" % network41.feedforward(frank)) # 0.039 - M\n",
    "print(\"Claudi: %.3f\" % network41.feedforward(claudia)) # 0.951 - F\n",
    "print(\"Jon: %.3f\" % network41\n",
    "      .feedforward(jon)) # 0.039 - M\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XSmP_TVCRuzP"
   },
   "source": [
    "I tried to implement the neural network with matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ruaqRCw0_JA7"
   },
   "outputs": [],
   "source": [
    "class OurNeuralNetwork4:\n",
    "  '''\n",
    "  This neural network has 2 hidden layers\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    # Weights\n",
    "    self.w1 = np.random.rand(2, 2)\n",
    "    self.w2 = np.random.rand(2, 2)\n",
    "    self.w3 = np.random.rand(1, 2)\n",
    "    \n",
    "    # Biases\n",
    "    self.b1 = np.random.rand(2, 1)\n",
    "    self.b2 = np.random.rand(2, 1)\n",
    "    self.b3 = np.random.normal()\n",
    "\n",
    "  def feedforward(self, x):\n",
    "    # x is a numpy array with 2 elements.\n",
    "    x = x.reshape(2,1)\n",
    "    l1 = sigmoid(np.dot(self.w1 , x) + self.b1)\n",
    "    l2 = sigmoid(np.dot(self.w2 , l1) + self.b2)\n",
    "    o1 = sigmoid(np.dot(self.w3 , l2) + self.b3)\n",
    "    o1 = o1[0][0]\n",
    "\n",
    "    return o1\n",
    "  def train(self, data, all_y_trues):\n",
    "    learn_rate = 0.1\n",
    "    epochs = 1000 # number of times to loop through the entire dataset\n",
    "    for epoch in range(epochs):\n",
    "      for x, y_true in zip(data, all_y_trues):\n",
    "        x = x.reshape(2,1)\n",
    "        sum_l1 = np.dot(self.w1 , x) + self.b1\n",
    "        l1 = sigmoid(sum_l1)\n",
    "        sum_l2 = np.dot(self.w2 , l1) + self.b2\n",
    "        l2 = sigmoid(sum_l2)\n",
    "        sum_o1 = np.dot(self.w3 , l2) + self.b3\n",
    "        o1 = sigmoid(sum_o1)\n",
    "        y_pred = o1[0][0]\n",
    "        d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "        d_ypred_d_w3 = np.dot(l2 , deriv_sigmoid(sum_o1))\n",
    "        d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_l2 = self.w3 * deriv_sigmoid(sum_o1)\n",
    "\n",
    "        d_l2_d_w2 = np.dot(deriv_sigmoid(sum_l2), l1.transpose())\n",
    "        d_l2_d_l1 = self.w2 * deriv_sigmoid(sum_l2)\n",
    "        d_l2_d_b2 = deriv_sigmoid(sum_l2)\n",
    "        \n",
    "        d_l1_d_w1 = np.dot(deriv_sigmoid(sum_l1), x.transpose())\n",
    "        d_l1_d_b1 = deriv_sigmoid(sum_l1)\n",
    "    \n",
    "        self.w3  -= (learn_rate * d_L_d_ypred * d_ypred_d_w3).transpose();\n",
    "        self.b3  -= learn_rate * d_L_d_ypred * d_ypred_d_b3;\n",
    "        self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_l2.transpose() * d_l2_d_w2\n",
    "        self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_l2.transpose() * d_l2_d_b2\n",
    "        self.w1 -= learn_rate * d_L_d_ypred * np.dot(d_ypred_d_l2 , d_l2_d_l1) * d_l1_d_w1\n",
    "        self.b1 -= learn_rate * d_L_d_ypred * np.dot(d_ypred_d_l2 , d_l2_d_l1).transpose() * d_l1_d_b1\n",
    "\n",
    "        '''if epoch % 10 == 0:\n",
    "          y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "          loss = mse_loss(all_y_trues, y_preds)\n",
    "          print(\"Epoch %d loss: %.3f\" % (epoch, loss))'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "HS6k0TwzJ1WR",
    "outputId": "28773d1e-9803-4b91-9394-9d8cfbb07438"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emily: 0.945\n",
      "Frank: 0.064\n",
      "Claudi: 0.945\n",
      "Jon: 0.051\n"
     ]
    }
   ],
   "source": [
    "network4 = OurNeuralNetwork4()\n",
    "network4.train(data, all_y_trues)\n",
    "emily = np.array([-7, -3]) # 128 pounds, 63 inches\n",
    "frank = np.array([20, 2])  # 155 pounds, 68 inches\n",
    "claudia = np.array([-12, 2])\n",
    "jon = np.array([10, 7])\n",
    "print(\"Emily: %.3f\" % network4.feedforward(emily)) # 0.951 - F\n",
    "print(\"Frank: %.3f\" % network4.feedforward(frank)) # 0.039 - M\n",
    "print(\"Claudi: %.3f\" % network4.feedforward(claudia)) # 0.951 - F\n",
    "print(\"Jon: %.3f\" % network41\n",
    "      .feedforward(jon)) # 0.039 - M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cv-knWrsR2rN"
   },
   "source": [
    "Generate dataset. Height and weight are generated as distributions. Labels are generated considering height and weight. If a person is above the average, that means he is probably a male, and if not, is probably a female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWMqUu_mS2vk"
   },
   "outputs": [],
   "source": [
    "height= np.round( np.random.normal(1.75, 0.20, 100), 2)\n",
    "weight= np.round( np.random.normal(60.32, 15, 100), 2)\n",
    "np_people=np.column_stack((height, weight))\n",
    "label = np.where(np.logical_and(height < 1.75, weight < 60) , 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "qyihSB8AIvhc",
    "outputId": "0f8cfe9b-2466-4932-99cd-ebf6edce302b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "0.16975212928336433\n",
      "nr predictii gresite\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "network4 = OurNeuralNetwork4()\n",
    "network4.train(np_people[0:80], label[:80])\n",
    "y_preds = np.apply_along_axis(network4.feedforward, 1, np_people[80:])\n",
    "print(\"loss\")\n",
    "loss = mse_loss(label[80:], y_preds)\n",
    "print(loss)\n",
    "y_pred = np.where(y_preds < 0.5, 0, 1)\n",
    "print(\"nr predictii gresite\")\n",
    "print(np.count_nonzero(y_pred==label[80:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A8bP5_YvSQAo"
   },
   "source": [
    "Added the minibatch.\n",
    "Before recalculationg the weights, the NN takes a random number between 0 and the (length of the training - no of the batch size).\n",
    "Batch size is 20% of total number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lu02y4I4LWJs"
   },
   "outputs": [],
   "source": [
    "class OurNeuralNetwork5:\n",
    "  def __init__(self):\n",
    "    # Weights\n",
    "    self.w1 = np.random.normal()\n",
    "    self.w2 = np.random.normal()\n",
    "    self.w3 = np.random.normal()\n",
    "    self.w4 = np.random.normal()\n",
    "    self.w5 = np.random.normal()\n",
    "    self.w6 = np.random.normal()\n",
    "\n",
    "    # Biases\n",
    "    self.b1 = np.random.normal()\n",
    "    self.b2 = np.random.normal()\n",
    "    self.b3 = np.random.normal()\n",
    "\n",
    "  def feedforward(self, x):\n",
    "    # x is a numpy array with 2 elements.\n",
    "    h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
    "    h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
    "    o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
    "    return o1\n",
    "\n",
    "  def train(self, my_data, my_all_y_trues):\n",
    "    '''\n",
    "    - data is a (n x 2) numpy array, n = # of samples in the dataset.\n",
    "    - all_y_trues is a numpy array with n elements.\n",
    "      Elements in all_y_trues correspond to those in data.\n",
    "    '''\n",
    "    learn_rate = 0.1\n",
    "    epochs = 1000 # number of times to loop through the entire dataset\n",
    "    data_len = my_data.shape[0]\n",
    "    compute_nb = int(0.2 * data_len)\n",
    "    for epoch in range(epochs):\n",
    "      mini_batch_data = np.random.randint(0, data_len - compute_nb)\n",
    "      data = my_data[mini_batch_data: mini_batch_data + compute_nb]\n",
    "      all_y_trues = my_all_y_trues[mini_batch_data: mini_batch_data + compute_nb]\n",
    "      for x, y_true in zip(data, all_y_trues):\n",
    "        # --- Do a feedforward (we'll need these values later)\n",
    "        sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
    "        h1 = sigmoid(sum_h1)\n",
    "\n",
    "        sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
    "        h2 = sigmoid(sum_h2)\n",
    "\n",
    "        sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
    "        o1 = sigmoid(sum_o1)\n",
    "        y_pred = o1\n",
    "\n",
    "        # --- Calculate partial derivatives.\n",
    "        # --- Naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
    "        d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "\n",
    "        # Neuron o1\n",
    "        d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
    "\n",
    "        d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
    "\n",
    "        # Neuron h1\n",
    "        d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
    "        d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
    "        d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
    "\n",
    "        # Neuron h2\n",
    "        d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
    "        d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
    "        d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
    "\n",
    "        # --- Update weights and biases\n",
    "        # Neuron h1\n",
    "        self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
    "        self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
    "        self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
    "\n",
    "        # Neuron h2\n",
    "        self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
    "        self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
    "        self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
    "\n",
    "        # Neuron o1\n",
    "        self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
    "        self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6\n",
    "        self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
    "\n",
    "      # --- Calculate total loss at the end of each epoch\n",
    "      if epoch % 10 == 0:\n",
    "        y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "        loss = mse_loss(all_y_trues, y_preds)\n",
    "        print(\"Epoch %d loss: %.3f\" % (epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b0LSg-VwMi4-"
   },
   "outputs": [],
   "source": [
    "\n",
    "'''print(np_people)\n",
    "print(label)'''\n",
    "network5 = OurNeuralNetwork5()\n",
    "network5.train(np_people[0:80], label[:80])\n",
    "y_preds = np.apply_along_axis(network5.feedforward, 1, np_people[80:])\n",
    "print(\"loss\")\n",
    "loss = mse_loss(label[80:], y_preds)\n",
    "print(loss)\n",
    "y_pred = np.where(y_preds < 0.5, 0, 1)\n",
    "print(\"nr predictii gresite\")\n",
    "print(np.count_nonzero(y_pred==label[80:]))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Proiect_SI.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
