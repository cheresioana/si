{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP on Titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {},
    "colab_type": "code",
    "id": "tVm68ThnfSQw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {},
    "colab_type": "code",
    "id": "DKMMh-YnfSQ_"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pasul 1: se citeste setul de date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XytQWckMfSRK"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"csv/titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ph1yGMQAfSRS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Name</th>\n",
       "      <th>Parch</th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Title</th>\n",
       "      <th>Family_Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>Miss</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.0</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>113803</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>373450</td>\n",
       "      <td>Mr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age Cabin Embarked     Fare  \\\n",
       "0  22.0   NaN        S   7.2500   \n",
       "1  38.0   C85        C  71.2833   \n",
       "2  26.0   NaN        S   7.9250   \n",
       "3  35.0  C123        S  53.1000   \n",
       "4  35.0   NaN        S   8.0500   \n",
       "\n",
       "                                                Name  Parch  PassengerId  \\\n",
       "0                            Braund, Mr. Owen Harris      0            1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...      0            2   \n",
       "2                             Heikkinen, Miss. Laina      0            3   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)      0            4   \n",
       "4                           Allen, Mr. William Henry      0            5   \n",
       "\n",
       "   Pclass     Sex  SibSp  Survived            Ticket Title  Family_Size  \n",
       "0       3    male      1       0.0         A/5 21171    Mr            1  \n",
       "1       1  female      1       1.0          PC 17599   Mrs            1  \n",
       "2       3  female      0       1.0  STON/O2. 3101282  Miss            0  \n",
       "3       1  female      1       1.0            113803   Mrs            1  \n",
       "4       3    male      0       0.0            373450    Mr            0  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pas2: Se genereaza statistici despre date\n",
    "\n",
    "Aceste statistici se pot folosi pentru a vedea care feature-uri sunt relevante, pe ce coloane sunt missing values, distributia valorilor etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ntTgqB42XRSl"
   },
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lS4D1RFvcQsk"
   },
   "outputs": [],
   "source": [
    "#Pentru seturi de date mari este indicat să folosim versiunea minimală a librăriei\n",
    "prof=ProfileReport(df,minimal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cshaUY4VcUKC"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f973be0cae04665aeaf7127954776ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Summarize dataset', max=21.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4978c9bd5a44f4b815daca17570d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Generate report structure', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a4f31d79014e8bb2d75f13f3cc2143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Render HTML', max=1.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a5c289d69840d09bc21c6362e8709d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Export report to file', max=1.0, style=ProgressStyle(desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prof.to_file(output_file='html_stats/titanic-min.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pas3: selectarea feature-urilor\n",
    "\n",
    "Unul dintre feature-urile relevante este sexul persoanei. In dataset acesta este exprimat printr-un String (\"male\" sau \"female\")\n",
    "\n",
    "Pentru procesare stringul trebuie convertit in valoare numerica. Am ales sa reprezint \"femal\" ca si 1 si \"male\" ca si 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W_UUlJ8KfSRZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "df = pd.read_csv(\"csv/titanic.csv\")\n",
    "df[\"Sex\"] = np.where(df[\"Sex\"] == \"female\", 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un alt element important sunt featur-urile irelevante sau cele care au multe valori Nan.\n",
    "Feature-ul cabin are 77% missing values motiv pentru care nu este luat in considerare.\n",
    "Feature-ul Name, Ticket, Embarked si PassengerId nu aduc plus de informatii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"Cabin\", \"Survived\", \"Name\", \"Title\", \"Embarked\", \"Ticket\", \"PassengerId\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un feature relevant este cel de Title. \n",
    "\n",
    "Acesta are 6 vlaori distincte si este reprezentat printr-un string (Mr, Miss, Mr, Mrs, Master, Dr). \n",
    "\n",
    "Pentru transformare acestui feature in valori numerice foloses un labelBinarizer. Acesta va genera o martice de 6xlen(df) valori. Fiecare linie contine feature-ul transformat intr-o insirurire de 0 si 1. Exista doar un singur 1 pe linie si pozitia acestia corespunde cu Titlul pe care il inlocuieste.\n",
    "\n",
    "La final concatenam matricea in care am encodat feature-ul de Title cu dataframe-ul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Age     Fare  Parch  Pclass  Sex  SibSp  Family_Size  0  1  2  3  4  5\n",
      "0    22.0   7.2500      0       3    0      1            1  0  0  0  1  0  0\n",
      "1    38.0  71.2833      0       1    1      1            1  0  0  0  0  1  0\n",
      "2    26.0   7.9250      0       3    1      0            0  0  0  1  0  0  0\n",
      "3    35.0  53.1000      0       1    1      1            1  0  0  0  0  1  0\n",
      "4    35.0   8.0500      0       3    0      0            0  0  0  0  1  0  0\n",
      "..    ...      ...    ...     ...  ...    ...          ... .. .. .. .. .. ..\n",
      "886  27.0  13.0000      0       2    0      0            0  0  0  0  0  0  1\n",
      "887  19.0  30.0000      0       1    1      0            0  0  0  1  0  0  0\n",
      "888  22.0  23.4500      2       3    1      1            3  0  0  1  0  0  0\n",
      "889  26.0  30.0000      0       1    0      0            0  0  0  0  1  0  0\n",
      "890  32.0   7.7500      0       3    0      0            0  0  0  0  1  0  0\n",
      "\n",
      "[891 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelBinarizer()\n",
    "title = encoder.fit_transform(df[\"Title\"])\n",
    "X = pd.concat([X, pd.DataFrame(title)], axis=1)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Survived reprezinta outputul de care avem nevoie, modiv pentru care il mutem in variabila y si il stergen dintre feature-uri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pentru rezultate bune avem nevoie de normalizarea valorilor. \n",
    "\n",
    "EX: Daca avem feature-ul A cu valori intre 0 si 1 si feature-ul B cu valori intre 2000 si 3000, la training feature-ul B va influenta puternic rezultatul datorita valorilor sale foarte mari. Cu toate acestea se poate ca feature-ul A sa fie mult mai relevant, dar algoritmului ii va fi foarte greu sa tina cont datorita range-urilor diferite. Din acest motiv se foloseste un MinMaxScaler.\n",
    "\n",
    "Functia dupa care se normalizeaza este\n",
    "\n",
    "X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "\n",
    "X_scaled = X_std * (max - min) + min\n",
    "\n",
    "Si toate valorile vor fi in intervalul (-1 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V-Cg9ewufSRh"
   },
   "outputs": [],
   "source": [
    "# Functia va translata fiecare feature in parte in intervalul (-1,1)\n",
    "# Funcția practic relizează următoarele\n",
    "# X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "# X_scaled = X_std * (max - min) + min\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler((-1, 1))\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pas 4 Impartirea setului de date\n",
    "\n",
    "Setul de date se imparte in 80% pentru train si 20% pentru validation.\n",
    "De asemenea se face shuffle la date astfel incat daca in fisier aceste sunt cumva ordonate, pentru training sa fie in ordine aleatorie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VzSoXozDfSRp"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dEqnXDHmfSR9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.33199296, -0.9459008 , -1.        , ..., -1.        ,\n",
       "        -1.        , -1.        ],\n",
       "       [-0.03040965, -0.96906286, -1.        , ...,  1.        ,\n",
       "        -1.        , -1.        ],\n",
       "       [-0.65870822, -0.84507032, -0.66666667, ...,  1.        ,\n",
       "        -1.        , -1.        ],\n",
       "       ...,\n",
       "       [-0.10580548, -0.6921901 , -1.        , ...,  1.        ,\n",
       "        -1.        , -1.        ],\n",
       "       [-0.20633325, -0.88093593, -1.        , ..., -1.        ,\n",
       "        -1.        , -1.        ],\n",
       "       [ 0.84920834, -0.96964842, -1.        , ...,  1.        ,\n",
       "        -1.        , -1.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kMSDboU8fSSQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8CBOO0C3fSSV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(712, 13)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8MPgMngsfSSa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(712,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jZv_HSpDtU2v"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 13)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversie valori in tensori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cE4YqjFxfSSg"
   },
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_train = torch.tensor(y_train.values).long()\n",
    "y_test = torch.tensor(y_test.values).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oPIV0ruYuECM"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "class Dataset(Dataset):\n",
    "    # Initialize your data, download, etc.\n",
    "    def __init__(self, x, y):\n",
    "        #Citim setul de date\n",
    "        self.len = len(x)\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creare Loader pentru Validation si train. Cele 2 loadere impart datele in batch-uri de 32 si fac shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset=Dataset(X_train, y_train)\n",
    "trainLoader=DataLoader(dataset=trainDataset,\n",
    "                        batch_size=32,\n",
    "                        shuffle=True,\n",
    "                        num_workers=1)\n",
    "validationDataset=Dataset(X_test, y_test)\n",
    "validationLoader=DataLoader(dataset=validationDataset,\n",
    "                        batch_size=32,\n",
    "                        shuffle=True,\n",
    "                        num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reteaua neuronala\n",
    "\n",
    "Se defineste structura retelei neuronale. Reteaua de fata are 4 layere: 1 de input, 2 hidden si 1 de output. De asemenea se definesc functiile de activare a neuronilor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nQjv5QsKfSSk"
   },
   "outputs": [],
   "source": [
    "class TitanicNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TitanicNN, self).__init__()\n",
    "\n",
    "        #Sequential oferă o alternativă mai estetică a codului\n",
    "        #Rețeaua noastră are 2 neuroni pentru output. \n",
    "        #Unul va prezice probabilitatea pentru cazul afirmativ iar celălalt va prezice probabilitatea cazului negativ.\n",
    "        self.sequential= nn.Sequential(\n",
    "            nn.Linear(13,30),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(30, 60),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(60, 2)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qI_tjxPWfSSn"
   },
   "outputs": [],
   "source": [
    "net = TitanicNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class.\n",
    "\n",
    "It is useful when training a classification problem with C classes. If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set.\n",
    "\n",
    "softmax function or normalized exponential function is a function that takes as input a vector of K real numbers, and normalizes it into a probability distribution consisting of K probabilities proportional to the exponentials of the input numbers. \n",
    "\n",
    "np.exp(a) / np.sum(np.exp(a))\n",
    "\n",
    "a = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0] \n",
    "\n",
    "-> array([0.02364054, 0.06426166, 0.1746813, 0.474833, 0.02364054,\n",
    "       0.06426166, 0.1746813])\n",
    "       \n",
    "       \n",
    "In practice, the softmax function is used in tandem with the negative log-likelihood (NLL). This loss function is very interesting if we interpret it in relation to the behavior of softmax. First, let’s write down our loss function:\n",
    "\n",
    "L(y) = − log(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eI80PhVafSSt"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RaqzqBOcfSSx"
   },
   "outputs": [],
   "source": [
    "#Colectăm loss-urile din antrenare pentru a le plota ulterior\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7qt8b44MfSS3"
   },
   "outputs": [],
   "source": [
    "# Colectăm accuratetea pentru a o plota ulterior\n",
    "accuracies=[]\n",
    "acc_validation = []\n",
    "acc_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-2779h43uwuO"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tainingul se face cu minibatch. pentru fiecare minibatch se fac predictii, se calculeaza loss-ul si apoi se face update la weight-uri.\n",
    "De asemenea se salveaza si accuracy-ul pentru training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K_woyjjffSTA"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    net.train()\n",
    "    losses=[]\n",
    "    correct = 0\n",
    "    for batch_idx, data in enumerate(trainLoader, 0):\n",
    "      inputs, labels =data\n",
    "      outputs = net(inputs)\n",
    "      # Compute and print loss\n",
    "      loss = criterion(outputs, labels)\n",
    "      losses.append(loss.item())\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      pred = outputs.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(labels.data.view_as(pred)).sum()\n",
    "      #print(f\"[Train Epoch: {epoch}, Batch: {batch_idx+1}, Loss: {loss.item()}\")\n",
    "    mean_loss=sum(losses)/len(losses)\n",
    "    accuracy = 100. * correct/len(trainLoader.dataset)\n",
    "    scheduler.step(mean_loss)\n",
    "    train_losses.append(mean_loss)\n",
    "    acc_train.append(accuracy)\n",
    "    print(f\"[TRAIN] Epoch: {epoch} Loss:{mean_loss}, Accuracy: {accuracy}%\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pNNf-W4shrIH"
   },
   "outputs": [],
   "source": [
    "# Colectăm loss-ul din validare pentru a o plota ulterior\n",
    "test_losses=[]\n",
    "train_losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dupa fiecare train se face masuratorile si pe setul de validation. Fara a modifica weight-urile, se evalueaza accuracy-ul si loss-ul pe setul de validare. Si pe setul de validare calculele se fac tot folosind mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8by5McSEfSTF"
   },
   "outputs": [],
   "source": [
    "def validation():\n",
    "    #Pune pe off flagurile setate in model.train()\n",
    "    net.eval()\n",
    "    test_loss=[]\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(validationLoader, 0):\n",
    "          inputs, labels = data\n",
    "\n",
    "          output=net(inputs)\n",
    "          loss= criterion(output, labels)\n",
    "          test_loss.append(loss.item())\n",
    "\n",
    "          pred = output.data.max(1, keepdim=True)[1]\n",
    "\n",
    "          #Verificăm câte predicții sunt corecte și le însumăm numărul pentru a afla totalul de predicții corecte\n",
    "          correct += pred.eq(labels.data.view_as(pred)).sum()\n",
    "          current_correct=pred.eq(labels.data.view_as(pred)).sum()          \n",
    "        mean_loss=sum(test_loss)/len(test_loss)\n",
    "        test_losses.append(mean_loss)\n",
    "        accuracy = 100. * correct/len(validationLoader.dataset)\n",
    "        print(f\"[Validation set] Loss: {mean_loss}, Accuracy: {accuracy}%\")\n",
    "        accuracies.append(accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wuyLaJgafSTJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Epoch: 0 Loss:0.7043146853861602, Accuracy: 42.134830474853516%\n",
      "[Validation set] Loss: 0.6908387839794159, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 1 Loss:0.6773598738338636, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.6817559997240702, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 2 Loss:0.6646205990210824, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.6754622856775919, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 3 Loss:0.6647694758746935, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.67206339041392, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 4 Loss:0.6605227719182554, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.6687128047148386, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 5 Loss:0.6583451680515123, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.6754536430040995, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 6 Loss:0.6607990783193837, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.6768260498841604, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 7 Loss:0.6527888152910315, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.6669430732727051, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 8 Loss:0.6508846801260243, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.670503189166387, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 9 Loss:0.6510855348213859, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.6638420323530833, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 10 Loss:0.6497941250386445, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.6632234752178192, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 11 Loss:0.6450172714565111, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.663596898317337, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 12 Loss:0.6456954427387404, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.6599155763785044, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 13 Loss:0.6438022229982459, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.657679001490275, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 14 Loss:0.6402325837508492, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.6627063453197479, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 15 Loss:0.6421147584915161, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.657207727432251, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 16 Loss:0.6370755667271821, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.646935393412908, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 17 Loss:0.6354973445767942, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.6513921519120535, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 18 Loss:0.6318178436030513, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.6407842437426249, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 19 Loss:0.6313686915065931, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.6417297621568044, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 20 Loss:0.6299384884212328, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.6392507354418436, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 21 Loss:0.6248415889947311, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.6329607764879862, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 22 Loss:0.6287611334220223, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.6372641921043396, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 23 Loss:0.620779169642407, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.6295788884162903, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 24 Loss:0.6172206116759259, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.6392259299755096, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 25 Loss:0.6125224621399589, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.6269339521725973, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 26 Loss:0.6123075433399366, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.6193987826506296, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 27 Loss:0.608172797638437, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.6188926498095194, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 28 Loss:0.6092859921248063, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.6119255820910136, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 29 Loss:0.6048022249470586, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.6077809035778046, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 30 Loss:0.6021513187366984, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.6011296411355337, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 31 Loss:0.5985119109568389, Accuracy: 62.21910095214844%\n",
      "[Validation set] Loss: 0.6041122078895569, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 32 Loss:0.5871271439220594, Accuracy: 63.76404571533203%\n",
      "[Validation set] Loss: 0.5934942662715912, Accuracy: 59.21787643432617%\n",
      "[TRAIN] Epoch: 33 Loss:0.5857756189678026, Accuracy: 62.921348571777344%\n",
      "[Validation set] Loss: 0.5897014240423838, Accuracy: 64.8044662475586%\n",
      "[TRAIN] Epoch: 34 Loss:0.5797729090504025, Accuracy: 66.71348571777344%\n",
      "[Validation set] Loss: 0.5852275888125101, Accuracy: 65.36312866210938%\n",
      "[TRAIN] Epoch: 35 Loss:0.5745019342588342, Accuracy: 66.994384765625%\n",
      "[Validation set] Loss: 0.5846202274163564, Accuracy: 66.4804458618164%\n",
      "[TRAIN] Epoch: 36 Loss:0.5712439288263735, Accuracy: 70.22471618652344%\n",
      "[Validation set] Loss: 0.5727788408597311, Accuracy: 70.94972229003906%\n",
      "[TRAIN] Epoch: 37 Loss:0.5665777442247971, Accuracy: 73.59550476074219%\n",
      "[Validation set] Loss: 0.5687277615070343, Accuracy: 72.0670394897461%\n",
      "[TRAIN] Epoch: 38 Loss:0.5590906648532205, Accuracy: 74.2977523803711%\n",
      "[Validation set] Loss: 0.5629440347353617, Accuracy: 72.0670394897461%\n",
      "[TRAIN] Epoch: 39 Loss:0.5542686568654102, Accuracy: 76.40449523925781%\n",
      "[Validation set] Loss: 0.5526134868462881, Accuracy: 72.0670394897461%\n",
      "[TRAIN] Epoch: 40 Loss:0.5507678726445073, Accuracy: 76.12359619140625%\n",
      "[Validation set] Loss: 0.5483022828896841, Accuracy: 74.30167388916016%\n",
      "[TRAIN] Epoch: 41 Loss:0.5429298683353092, Accuracy: 77.80899047851562%\n",
      "[Validation set] Loss: 0.5522329757610956, Accuracy: 75.97765350341797%\n",
      "[TRAIN] Epoch: 42 Loss:0.5428276722845824, Accuracy: 78.93258666992188%\n",
      "[Validation set] Loss: 0.5400709410508474, Accuracy: 75.97765350341797%\n",
      "[TRAIN] Epoch: 43 Loss:0.5346609354019165, Accuracy: 78.65168762207031%\n",
      "[Validation set] Loss: 0.5404861072699229, Accuracy: 75.97765350341797%\n",
      "[TRAIN] Epoch: 44 Loss:0.5318295968615491, Accuracy: 78.23033905029297%\n",
      "[Validation set] Loss: 0.5308548410733541, Accuracy: 75.97765350341797%\n",
      "[TRAIN] Epoch: 45 Loss:0.5357958337535029, Accuracy: 78.93258666992188%\n",
      "[Validation set] Loss: 0.5210531949996948, Accuracy: 77.65363311767578%\n",
      "[TRAIN] Epoch: 46 Loss:0.523352677407472, Accuracy: 77.52809143066406%\n",
      "[Validation set] Loss: 0.5254352788130442, Accuracy: 79.3296127319336%\n",
      "[TRAIN] Epoch: 47 Loss:0.5216091171554897, Accuracy: 78.51123809814453%\n",
      "[Validation set] Loss: 0.5123477478822073, Accuracy: 77.65363311767578%\n",
      "[TRAIN] Epoch: 48 Loss:0.5049671051294907, Accuracy: 78.08988952636719%\n",
      "[Validation set] Loss: 0.5096105138460795, Accuracy: 76.53631591796875%\n",
      "[TRAIN] Epoch: 49 Loss:0.5064656021802322, Accuracy: 77.66854095458984%\n",
      "[Validation set] Loss: 0.49968575437863666, Accuracy: 79.3296127319336%\n",
      "[TRAIN] Epoch: 50 Loss:0.5015838366487751, Accuracy: 78.51123809814453%\n",
      "[Validation set] Loss: 0.5009511510531107, Accuracy: 79.3296127319336%\n",
      "[TRAIN] Epoch: 51 Loss:0.49521394527476764, Accuracy: 78.65168762207031%\n",
      "[Validation set] Loss: 0.5033253033955892, Accuracy: 79.3296127319336%\n",
      "[TRAIN] Epoch: 52 Loss:0.4937510866185893, Accuracy: 78.65168762207031%\n",
      "[Validation set] Loss: 0.4908800423145294, Accuracy: 79.3296127319336%\n",
      "[TRAIN] Epoch: 53 Loss:0.4896431422751883, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.48790575563907623, Accuracy: 79.3296127319336%\n",
      "[TRAIN] Epoch: 54 Loss:0.48583974397700763, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4793763756752014, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 55 Loss:0.48293659609297046, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4814961900313695, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 56 Loss:0.48584467043047364, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4746854950984319, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 57 Loss:0.4810152014960413, Accuracy: 78.65168762207031%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation set] Loss: 0.47153379519780475, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 58 Loss:0.48406672866448114, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4773877263069153, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 59 Loss:0.4774007032746854, Accuracy: 78.65168762207031%\n",
      "[Validation set] Loss: 0.4625966747601827, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 60 Loss:0.4800188204516535, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4729154109954834, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 61 Loss:0.4748817345370417, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4727350324392319, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 62 Loss:0.47556693528009497, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4534069001674652, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 63 Loss:0.46445018441780755, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4741856058438619, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 64 Loss:0.46344498966051184, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.46931441128253937, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 65 Loss:0.4687190211337546, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4593964417775472, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 66 Loss:0.47069021670714667, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45314334829648334, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 67 Loss:0.46790671866873035, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4594726463158925, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 68 Loss:0.4643853591836017, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4498925358057022, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 69 Loss:0.4732553842275039, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44743215541044873, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 70 Loss:0.46249491105908935, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44980834424495697, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 71 Loss:0.45942683453145233, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44009190797805786, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 72 Loss:0.45583795075831207, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4520231435696284, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 73 Loss:0.460163537574851, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44170962770779926, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 74 Loss:0.45901380932849384, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4532380700111389, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 75 Loss:0.44795309331106103, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44234193364779156, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 76 Loss:0.4566445143326469, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4582266757885615, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 77 Loss:0.4558604631734931, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43638970454533893, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 78 Loss:0.454541890517525, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45600054661432904, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 79 Loss:0.4599016606807709, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4368713398774465, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 80 Loss:0.4529575614825539, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4577135294675827, Accuracy: 79.88826751708984%\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-03.\n",
      "[TRAIN] Epoch: 81 Loss:0.4519079586733942, Accuracy: 78.93258666992188%\n",
      "[Validation set] Loss: 0.44501634935537976, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 82 Loss:0.4461611755516218, Accuracy: 78.93258666992188%\n",
      "[Validation set] Loss: 0.44258210559686023, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 83 Loss:0.45602851199067157, Accuracy: 78.93258666992188%\n",
      "[Validation set] Loss: 0.4621955355008443, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 84 Loss:0.44157465385354083, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4452241162459056, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 85 Loss:0.45117449371711066, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43314673006534576, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 86 Loss:0.4555433327737062, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44055041174093884, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 87 Loss:0.4607034753198209, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.451940397421519, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 88 Loss:0.4484844751979994, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4406392027934392, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 89 Loss:0.4480420091877813, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45343271394570667, Accuracy: 79.88826751708984%\n",
      "Epoch    91: reducing learning rate of group 0 to 1.0000e-04.\n",
      "[TRAIN] Epoch: 90 Loss:0.4503350024637969, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44044239322344464, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 91 Loss:0.4475852264010388, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44342206915219623, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 92 Loss:0.4478328007718791, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4573282798131307, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 93 Loss:0.4542841794698135, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4441393315792084, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 94 Loss:0.44465914627780084, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4598001142342885, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 95 Loss:0.44398716999136884, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4619886428117752, Accuracy: 79.88826751708984%\n",
      "Epoch    97: reducing learning rate of group 0 to 1.0000e-05.\n",
      "[TRAIN] Epoch: 96 Loss:0.4516417798788651, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43010883529980976, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 97 Loss:0.44640677778617194, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45524123311042786, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 98 Loss:0.4411391298408094, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45217420160770416, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 99 Loss:0.4468323406965836, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4367907792329788, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 100 Loss:0.4478524728961613, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44973236819108325, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 101 Loss:0.4458271835161292, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4341661681731542, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 102 Loss:0.46298717934152356, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4546772042910258, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 103 Loss:0.4644949151122052, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44045354425907135, Accuracy: 79.88826751708984%\n",
      "Epoch   105: reducing learning rate of group 0 to 1.0000e-06.\n",
      "[TRAIN] Epoch: 104 Loss:0.4508374437041905, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4387170076370239, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 105 Loss:0.4506772912066916, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43416841328144073, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 106 Loss:0.45324932362722314, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4475081463654836, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 107 Loss:0.45456069707870483, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44441143174966175, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 108 Loss:0.4530440206113069, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43859972556432086, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 109 Loss:0.4441054722537165, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4495118012030919, Accuracy: 79.88826751708984%\n",
      "Epoch   111: reducing learning rate of group 0 to 1.0000e-07.\n",
      "[TRAIN] Epoch: 110 Loss:0.4582797664663066, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45263571043809253, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 111 Loss:0.446344511664432, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4466539869705836, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 112 Loss:0.4486169737318288, Accuracy: 78.7921371459961%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation set] Loss: 0.4380175272623698, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 113 Loss:0.45050726118295087, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4416883885860443, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 114 Loss:0.4718762545481972, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44837385416030884, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 115 Loss:0.4461971171524214, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4405052661895752, Accuracy: 79.88826751708984%\n",
      "Epoch   117: reducing learning rate of group 0 to 1.0000e-08.\n",
      "[TRAIN] Epoch: 116 Loss:0.4583080151806707, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4460664838552475, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 117 Loss:0.447433326555335, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44657300412654877, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 118 Loss:0.44618387196374976, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4564974407354991, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 119 Loss:0.444792777299881, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4303930352131526, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 120 Loss:0.4459696010403011, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4434904307126999, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 121 Loss:0.45140178825544275, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4454174389441808, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 122 Loss:0.44155140221118927, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4556160867214203, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 123 Loss:0.44961531913798786, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4538157234589259, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 124 Loss:0.44877060729524365, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45249080161253613, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 125 Loss:0.4458499550819397, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4433143089214961, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 126 Loss:0.44766034898550616, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43601183593273163, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 127 Loss:0.45749247592428455, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4381165603796641, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 128 Loss:0.44518865126630536, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4371885855992635, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 129 Loss:0.44314128290052, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44012076159318286, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 130 Loss:0.445099756769512, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4395521928866704, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 131 Loss:0.44535035024518554, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.444254070520401, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 132 Loss:0.46403733155001764, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44793157279491425, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 133 Loss:0.4500584433908048, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4332941919565201, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 134 Loss:0.44674688966377923, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4356166025002797, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 135 Loss:0.4431662468806557, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4458272655804952, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 136 Loss:0.44560378530751105, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44811757405598956, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 137 Loss:0.46104159821634705, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43596283594767254, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 138 Loss:0.450223199699236, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44245995581150055, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 139 Loss:0.4618160983790522, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4411235998074214, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 140 Loss:0.4455764462118563, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44123486677805585, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 141 Loss:0.45285870070042816, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4587237636248271, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 142 Loss:0.4502195443796075, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45302621523539227, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 143 Loss:0.45889813355777576, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4515720357497533, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 144 Loss:0.44914224873418396, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43483301003774005, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 145 Loss:0.45882240067357605, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4385225474834442, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 146 Loss:0.4545929185722185, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.46276605625947315, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 147 Loss:0.4442365545293559, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4333809018135071, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 148 Loss:0.4461227461047795, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4446794092655182, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 149 Loss:0.4499614057333573, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4492502311865489, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 150 Loss:0.45040356335432635, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45186594625314075, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 151 Loss:0.4485341155010721, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44161224365234375, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 152 Loss:0.454086290753406, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44933579365412396, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 153 Loss:0.4554552435874939, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45309963325659436, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 154 Loss:0.45413061069405597, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4574894309043884, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 155 Loss:0.441996306180954, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43808965384960175, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 156 Loss:0.4502673097278761, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.450759490331014, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 157 Loss:0.45476682030636334, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44471704959869385, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 158 Loss:0.44586395439894305, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4526544362306595, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 159 Loss:0.44392754850180255, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4419548561175664, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 160 Loss:0.4530234531215999, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44429156680901843, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 161 Loss:0.4476362039213595, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4599599788586299, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 162 Loss:0.45716389884119446, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44478659828503925, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 163 Loss:0.46068472188452014, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45133088529109955, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 164 Loss:0.45210552863452746, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43929319580396015, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 165 Loss:0.4408636203278666, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43214796483516693, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 166 Loss:0.45809632280598517, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4477366805076599, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 167 Loss:0.45521524937256524, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4373217821121216, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 168 Loss:0.45081501162570453, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45050181448459625, Accuracy: 79.88826751708984%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Epoch: 169 Loss:0.4580188056697016, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4547663430372874, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 170 Loss:0.46377499595932337, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4404030342896779, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 171 Loss:0.45489376718583313, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43752218782901764, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 172 Loss:0.4447334680868232, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4461234410603841, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 173 Loss:0.44392725047857867, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4360332190990448, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 174 Loss:0.45008220491201983, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44154152274131775, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 175 Loss:0.44710654409035394, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44296639164288837, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 176 Loss:0.4594736526841703, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43327006200949353, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 177 Loss:0.45168439331262006, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.42974016070365906, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 178 Loss:0.4514852956585262, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4359209140141805, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 179 Loss:0.4535898656948753, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4463893423477809, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 180 Loss:0.4470725681470788, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4374104191859563, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 181 Loss:0.451953517354053, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4444618970155716, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 182 Loss:0.452332044425218, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44522807995478314, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 183 Loss:0.44680570649064105, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44299691418806714, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 184 Loss:0.448362419138784, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4371674607197444, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 185 Loss:0.4505519491174947, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4525495171546936, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 186 Loss:0.45449142093243805, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45382581651210785, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 187 Loss:0.4487571547860685, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4468821535507838, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 188 Loss:0.44614230420278467, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.437347744901975, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 189 Loss:0.4445903288281482, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44876158734162647, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 190 Loss:0.4522608168747114, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4452216327190399, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 191 Loss:0.4504453747168831, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4552897959947586, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 192 Loss:0.4400046176236609, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4516577621301015, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 193 Loss:0.44739242092422815, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44403526186943054, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 194 Loss:0.44225542830384296, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44811070958773297, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 195 Loss:0.4433313310146332, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.46706197162469226, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 196 Loss:0.4516366914562557, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44021840393543243, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 197 Loss:0.44885376484497735, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4396696984767914, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 198 Loss:0.44496240693589917, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4513806899388631, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 199 Loss:0.4564834599909575, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4398709336916606, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 200 Loss:0.45431488363639166, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4418488045533498, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 201 Loss:0.4515982257283252, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4484172413746516, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 202 Loss:0.4473847448825836, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4473217229048411, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 203 Loss:0.4490948876608973, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4515341967344284, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 204 Loss:0.4506920964821525, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4362727850675583, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 205 Loss:0.4496872107619825, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45418458183606464, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 206 Loss:0.4628609483656676, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4342345992724101, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 207 Loss:0.4536254548508188, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43385272721449536, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 208 Loss:0.4533375592335411, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44975713392098743, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 209 Loss:0.4535046064335367, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44983623425165814, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 210 Loss:0.44591990761134936, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4583949049313863, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 211 Loss:0.4460810371067213, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45802634954452515, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 212 Loss:0.45603633445242175, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4322059551874797, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 213 Loss:0.44708732936693274, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44559331734975177, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 214 Loss:0.4498249421948972, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4461725850900014, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 215 Loss:0.4423479191634966, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4459715932607651, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 216 Loss:0.4504909450593202, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4323791563510895, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 217 Loss:0.4471138365890669, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4427042255798976, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 218 Loss:0.4447184839974279, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45332639912764233, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 219 Loss:0.4463615974654322, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4412415474653244, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 220 Loss:0.45244817889255023, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4348802864551544, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 221 Loss:0.4454344692437545, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4436185409625371, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 222 Loss:0.45855358761289844, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.470256711045901, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 223 Loss:0.4496171228263689, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4542984664440155, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 224 Loss:0.4622641192830127, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44477970401446026, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 225 Loss:0.4451954248158828, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4464830259482066, Accuracy: 79.88826751708984%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Epoch: 226 Loss:0.44652991450351215, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44769838949044544, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 227 Loss:0.4556296117927717, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4425529142220815, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 228 Loss:0.44717756432035694, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44172706206639606, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 229 Loss:0.4452111163864965, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4429500997066498, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 230 Loss:0.4641534660173499, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4419921437899272, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 231 Loss:0.44722990497298865, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4488018850485484, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 232 Loss:0.44719274925149005, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44166308641433716, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 233 Loss:0.4502173260502193, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4498081753651301, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 234 Loss:0.44725440118623816, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4563594063123067, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 235 Loss:0.45517013643098914, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4401283611853917, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 236 Loss:0.4549871747908385, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45911602179209393, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 237 Loss:0.45742204526196356, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44295216103394824, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 238 Loss:0.4488792691541755, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4443107595046361, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 239 Loss:0.459295647299808, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44702601929505664, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 240 Loss:0.4491662110971368, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44430982569853467, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 241 Loss:0.45361684197964874, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45781926810741425, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 242 Loss:0.4590576926003332, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4314422657092412, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 243 Loss:0.4410054605940114, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4530487507581711, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 244 Loss:0.44495008561922156, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44098056356112164, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 245 Loss:0.44472945773083233, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4513708104689916, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 246 Loss:0.44531148412953253, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4449331611394882, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 247 Loss:0.4585413090560747, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43823906779289246, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 248 Loss:0.46825178809787915, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45813092589378357, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 249 Loss:0.4478018024693365, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4456522713104884, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 250 Loss:0.4482275636299797, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4406940092643102, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 251 Loss:0.4576339047888051, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.445774386326472, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 252 Loss:0.44892248381739075, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4394901990890503, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 253 Loss:0.44822040718534717, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43568889300028485, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 254 Loss:0.4477054373077724, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43601370354493457, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 255 Loss:0.447452530912731, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4473218222459157, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 256 Loss:0.44394992875016254, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44085929294427234, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 257 Loss:0.44205034167870233, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.447209894657135, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 258 Loss:0.4495041331519251, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43970470627148944, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 259 Loss:0.4452446129011071, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44072428345680237, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 260 Loss:0.45209792774656543, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45322872201601666, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 261 Loss:0.4438934863909431, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44144681096076965, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 262 Loss:0.4526275124238885, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4418329894542694, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 263 Loss:0.45966122590977215, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4492588937282562, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 264 Loss:0.4532569102619005, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44425296286741894, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 265 Loss:0.44617645766424097, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4464685469865799, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 266 Loss:0.4669857491617617, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4432709614435832, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 267 Loss:0.4424504922783893, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4418039172887802, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 268 Loss:0.4417931223693101, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4485430916150411, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 269 Loss:0.4550547677537669, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44610268870989483, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 270 Loss:0.45554114295088727, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45193026463190716, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 271 Loss:0.45186472392600513, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4430395265420278, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 272 Loss:0.45523401317389117, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4465477565924327, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 273 Loss:0.44872583124948584, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4315473015109698, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 274 Loss:0.4434404237114865, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4674975872039795, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 275 Loss:0.45190961075865704, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45043957730134326, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 276 Loss:0.46338755928951764, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4414377609888713, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 277 Loss:0.45141788021377893, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4481593022743861, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 278 Loss:0.4450347112572711, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44872575501600903, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 279 Loss:0.4466178203406541, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4360426515340805, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 280 Loss:0.4525787415711776, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44961946705977124, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 281 Loss:0.4525314362152763, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44241683185100555, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 282 Loss:0.44836986194486206, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4554775208234787, Accuracy: 79.88826751708984%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Epoch: 283 Loss:0.44394648463829706, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4387176185846329, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 284 Loss:0.45095314927723096, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4465423971414566, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 285 Loss:0.44578735336013464, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4345732827981313, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 286 Loss:0.4534039432587831, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45060884455839795, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 287 Loss:0.4474842237389606, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4467163532972336, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 288 Loss:0.44504177829493646, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43392906586329144, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 289 Loss:0.4533661655757738, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4471917500098546, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 290 Loss:0.44544339698293933, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45697906116644543, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 291 Loss:0.45609731259553327, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45872455338637036, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 292 Loss:0.44895866642827575, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4388745774825414, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 293 Loss:0.4450827325167863, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43324342370033264, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 294 Loss:0.4563755716966546, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4535922904809316, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 295 Loss:0.45067043019377667, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4465889086325963, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 296 Loss:0.4616183923638385, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45014085123936337, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 297 Loss:0.4503069921680119, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4477016081412633, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 298 Loss:0.4589181503523951, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45865944027900696, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 299 Loss:0.44242263099421625, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45639654497305554, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 300 Loss:0.46654514514881634, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4475770692030589, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 301 Loss:0.4485856566740119, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45369014143943787, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 302 Loss:0.446763743524966, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4495875835418701, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 303 Loss:0.4459002925002057, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4332649161418279, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 304 Loss:0.4528586786726247, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44113631546497345, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 305 Loss:0.45005828660467395, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.457797110080719, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 306 Loss:0.44851394127244537, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44676751891771954, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 307 Loss:0.4445891898611318, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4315672939022382, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 308 Loss:0.4478621729042219, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4576386362314224, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 309 Loss:0.45026697607144067, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4361523737510045, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 310 Loss:0.45958248817402386, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4460572600364685, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 311 Loss:0.4636626088100931, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4450545360644658, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 312 Loss:0.4441944736501445, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44462110102176666, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 313 Loss:0.45239828332610754, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44523011644681293, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 314 Loss:0.4451460883669231, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4553971787293752, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 315 Loss:0.4414437976868256, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.443857138355573, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 316 Loss:0.4472030440102453, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4546903868516286, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 317 Loss:0.45103105643521185, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45282868047555286, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 318 Loss:0.4602619163368059, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44744225839773816, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 319 Loss:0.44753951482150867, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45934148629506427, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 320 Loss:0.4517360692438872, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43916932741800946, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 321 Loss:0.44191904689954675, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43434518575668335, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 322 Loss:0.45289981883505115, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4436383545398712, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 323 Loss:0.45224063292793604, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44098247587680817, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 324 Loss:0.4551738098911617, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43931960066159564, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 325 Loss:0.4535557575847792, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44795385003089905, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 326 Loss:0.44816691201666126, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4468361089626948, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 327 Loss:0.4465111358010251, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4508008410533269, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 328 Loss:0.4482041169767794, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.46458787719408673, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 329 Loss:0.4491737549719603, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44379522403081256, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 330 Loss:0.45219950313153473, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.445371334751447, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 331 Loss:0.45390070197374927, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4418443789084752, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 332 Loss:0.4481828225695569, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44687240819136304, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 333 Loss:0.44532282326532446, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44036436080932617, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 334 Loss:0.4439163784617963, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4508930891752243, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 335 Loss:0.4471058638199516, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.441742663582166, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 336 Loss:0.45432831670926965, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4508247474829356, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 337 Loss:0.4548147385535033, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4390196005503337, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 338 Loss:0.44440391141435376, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4563403030236562, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 339 Loss:0.45627120849878894, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4495558838049571, Accuracy: 79.88826751708984%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Epoch: 340 Loss:0.4456472617128621, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44280313948790234, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 341 Loss:0.44160926601161127, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45821885764598846, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 342 Loss:0.4415616995614508, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44406702121098834, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 343 Loss:0.44965743111527484, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44258396327495575, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 344 Loss:0.44268093420111615, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4527693639198939, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 345 Loss:0.449499794970388, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43625134726365405, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 346 Loss:0.44617409809775976, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4447064052025477, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 347 Loss:0.4553293844927912, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45075560609499615, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 348 Loss:0.4532884514850119, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4532360682884852, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 349 Loss:0.4565578919389974, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44672947625319165, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 350 Loss:0.4491121302480283, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44467246035734814, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 351 Loss:0.45720001796017523, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43737634023030597, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 352 Loss:0.4531483617813691, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43981902301311493, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 353 Loss:0.44802649772685504, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43341586490472156, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 354 Loss:0.44449654869411304, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43055598189433414, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 355 Loss:0.46902698148851807, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44522028664747876, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 356 Loss:0.4543356105037358, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4368313103914261, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 357 Loss:0.4444560304931972, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4398644119501114, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 358 Loss:0.4511093082635299, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43560392161210376, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 359 Loss:0.45380411718202673, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44776282211144763, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 360 Loss:0.44359772749569104, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4619775166114171, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 361 Loss:0.4465408260407655, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4572475254535675, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 362 Loss:0.45558240232260333, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43536153932412464, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 363 Loss:0.4442619020524232, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45562749604384106, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 364 Loss:0.4514374823673912, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.442052220304807, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 365 Loss:0.44572298163953034, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4428998778263728, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 366 Loss:0.45634592097738513, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4515761236349742, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 367 Loss:0.4548756259938945, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4418823669354121, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 368 Loss:0.4468514452809873, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44253145158290863, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 369 Loss:0.4433966188327126, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45019463698069256, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 370 Loss:0.4436963051557541, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.443808654944102, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 371 Loss:0.4472428508426832, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45669594407081604, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 372 Loss:0.4593588632086049, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4461187422275543, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 373 Loss:0.45180811830188916, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44145500659942627, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 374 Loss:0.4467354261356851, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4488750646511714, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 375 Loss:0.44402098137399426, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.46075818439324695, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 376 Loss:0.44800871999367425, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4426471988360087, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 377 Loss:0.45451559320740076, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45673369864622754, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 378 Loss:0.45138470763745514, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4407259275515874, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 379 Loss:0.4533922996210015, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4388262430826823, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 380 Loss:0.4450257122516632, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43694039185841876, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 381 Loss:0.44502634846645855, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4485214004913966, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 382 Loss:0.44843479083931964, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45050064225991565, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 383 Loss:0.4547226105047309, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43968843420346576, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 384 Loss:0.4447280064873073, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4522374818722407, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 385 Loss:0.4463219396446062, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44475620488325757, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 386 Loss:0.45448406623757404, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4436350216468175, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 387 Loss:0.449836324090543, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4391777366399765, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 388 Loss:0.4535180109998454, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44493051369984943, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 389 Loss:0.45124723600304645, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4437570621569951, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 390 Loss:0.4442062118779058, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4517766584952672, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 391 Loss:0.457434813613477, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43750397364298504, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 392 Loss:0.45328948549602344, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4465256830056508, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 393 Loss:0.44501673786536505, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44001390039920807, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 394 Loss:0.44523100490155426, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44750045239925385, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 395 Loss:0.45916670172110846, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44301454226175946, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 396 Loss:0.4528080898782481, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4350932836532593, Accuracy: 79.88826751708984%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Epoch: 397 Loss:0.44445131913475366, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4473561644554138, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 398 Loss:0.45108396592347516, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4392619381348292, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 399 Loss:0.44326765900072845, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43961083392302197, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 400 Loss:0.4469813393509906, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4343900680541992, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 401 Loss:0.4501797038575877, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45511270066102344, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 402 Loss:0.44717319763225055, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43335547546545666, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 403 Loss:0.4507357154203498, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4489514281352361, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 404 Loss:0.44600163983262103, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44832488894462585, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 405 Loss:0.46232121534969495, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4469190090894699, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 406 Loss:0.45036642447761865, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45821358760197956, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 407 Loss:0.45174438797909283, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44608792165915173, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 408 Loss:0.4492451302383257, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4492067148288091, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 409 Loss:0.4583491721878881, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.443044051527977, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 410 Loss:0.4510147675223973, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44017403324445087, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 411 Loss:0.44829303674075915, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43668508032957715, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 412 Loss:0.4600310960541601, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4421779861052831, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 413 Loss:0.4502623858659164, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44331452747186023, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 414 Loss:0.4473227158836696, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4494764854510625, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 415 Loss:0.4463484209516774, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.441710631052653, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 416 Loss:0.44782221965167834, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43782903750737506, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 417 Loss:0.44516940479693207, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44781660536925, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 418 Loss:0.46416809895764227, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4578133473793666, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 419 Loss:0.45745893664982007, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44039080043633777, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 420 Loss:0.45521973008694855, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45726344982783, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 421 Loss:0.457180380821228, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4330394963423411, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 422 Loss:0.45172481821930927, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45117318133513135, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 423 Loss:0.44255318265894183, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.46080290774504346, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 424 Loss:0.4521468838919764, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43653519451618195, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 425 Loss:0.4642958278241365, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45363757014274597, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 426 Loss:0.4562317029289577, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4620314488808314, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 427 Loss:0.45008275042409485, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4548899134000142, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 428 Loss:0.4568164646625519, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4470164030790329, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 429 Loss:0.45993210051370703, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44456758598486584, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 430 Loss:0.449481593525928, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.443483238418897, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 431 Loss:0.4491510015466939, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4418165385723114, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 432 Loss:0.44652489734732587, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4368926336367925, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 433 Loss:0.4522962168506954, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4341057737668355, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 434 Loss:0.44887678908265155, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43586864570776623, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 435 Loss:0.4526080644649008, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4531206438938777, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 436 Loss:0.4660837831704513, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44468937317530316, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 437 Loss:0.4439281050277793, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4481353263060252, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 438 Loss:0.44130308213441266, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43979934354623157, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 439 Loss:0.45231696185858355, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44987668097019196, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 440 Loss:0.4422512268242629, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4460996389389038, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 441 Loss:0.44518262795780017, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4539824624856313, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 442 Loss:0.46303006747494574, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4372132221857707, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 443 Loss:0.4585559951222461, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4483585059642792, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 444 Loss:0.4477535447348719, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.46254002054532367, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 445 Loss:0.4400999196197676, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44497184455394745, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 446 Loss:0.44700117992318195, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4560600817203522, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 447 Loss:0.4487843150677888, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.46429288387298584, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 448 Loss:0.4447751602400904, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4322156508763631, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 449 Loss:0.4647619296675143, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4497467080752055, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 450 Loss:0.4466721052708833, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4452092746893565, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 451 Loss:0.4482252014719922, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.46187012394269306, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 452 Loss:0.45100411902303283, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44467786451180774, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 453 Loss:0.4411937633286352, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45130572219689685, Accuracy: 79.88826751708984%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Epoch: 454 Loss:0.4467777104481407, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44973621765772503, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 455 Loss:0.44681822735330334, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4491588721672694, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 456 Loss:0.4462973397711049, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4432832847038905, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 457 Loss:0.45333517634350323, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4367534468571345, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 458 Loss:0.4434597991082979, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44662924607594806, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 459 Loss:0.44450918358305225, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43202877541383106, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 460 Loss:0.45030524808427563, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44995400806268054, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 461 Loss:0.44534994208294415, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43686656653881073, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 462 Loss:0.4571673546148383, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45242907603581745, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 463 Loss:0.4583980013494906, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4443196803331375, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 464 Loss:0.45750082964482514, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4637511571248372, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 465 Loss:0.4453691593978716, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4464285572369893, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 466 Loss:0.4538670091525368, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.453004186352094, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 467 Loss:0.45258311862530914, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44017081956068677, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 468 Loss:0.44786006212234497, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4369286298751831, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 469 Loss:0.4647842956625897, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43956636389096576, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 470 Loss:0.4488941534705784, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.446316530307134, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 471 Loss:0.4541194218656291, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.46158228317896527, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 472 Loss:0.4544234029624773, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44448941946029663, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 473 Loss:0.4529608021611753, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4438644101222356, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 474 Loss:0.45498707242633984, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4535515606403351, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 475 Loss:0.4674289991026339, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4515721450249354, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 476 Loss:0.4473708518173384, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45278051495552063, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 477 Loss:0.4514413851758708, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4464041292667389, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 478 Loss:0.4450362830058388, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4440770447254181, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 479 Loss:0.472449805425561, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4442933648824692, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 480 Loss:0.4449119658573814, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4364070345958074, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 481 Loss:0.44571333475734876, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4396176387866338, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 482 Loss:0.4493167899225069, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43151481449604034, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 483 Loss:0.45131009298822156, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4364923636118571, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 484 Loss:0.45513735387636267, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4495519498984019, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 485 Loss:0.44674246920191724, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4536360253890355, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 486 Loss:0.45022179380707117, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4630943338076274, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 487 Loss:0.4683299492234769, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4588651905457179, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 488 Loss:0.4492375345333763, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45700060824553174, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 489 Loss:0.45732104519139166, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.4535643607378006, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 490 Loss:0.44766563436259393, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45259888966878253, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 491 Loss:0.4407748567021411, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45817076911528903, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 492 Loss:0.4472027641275655, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43178923924763996, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 493 Loss:0.44562939845997357, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44681045909722644, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 494 Loss:0.4479835901571357, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45041459798812866, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 495 Loss:0.4494641021541927, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44986769060293835, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 496 Loss:0.4508583286534185, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44948405027389526, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 497 Loss:0.4439412990342016, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.43969906866550446, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 498 Loss:0.447628000508184, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.45140375196933746, Accuracy: 79.88826751708984%\n",
      "[TRAIN] Epoch: 499 Loss:0.4515246956244759, Accuracy: 78.7921371459961%\n",
      "[Validation set] Loss: 0.44316211839516956, Accuracy: 79.88826751708984%\n"
     ]
    }
   ],
   "source": [
    "#după fiecare epocă de train() verificăm rezultatele pe setul de validare\n",
    "for epoch in range(500):\n",
    "  train(epoch)\n",
    "  validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validare\n",
    "\n",
    "La final, dupa ce reteaua neuronala a fost antrenata, se face plot la loss-ul de pe datele de train si de validare. Acest grafic ajuta la depistarea problemelor cu privire la convergenta algoritmului, overfit si underfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CNO72JVEjVy1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4200f84190>,\n",
       " <matplotlib.lines.Line2D at 0x7f4200f84390>]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Printăm comparativ cu roșu loss-ul de pe setul de validare și cu albastru loss-ul de pe setul de validare.\n",
    "plt.plot(train_losses, \"r-\", test_losses, \"b-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se mai poate face un grafic cu evolutia accuracy-ului pe cele 2 seturi de date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJk5Y0GatLMr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4200edffd0>,\n",
       " <matplotlib.lines.Line2D at 0x7f4200eeb250>]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAZkUlEQVR4nO3df3BcZ33v8fdKq5/+ITneYCQ7YJeEMMSdGOqmySQDIcY0EJofM5nvTXoJJgmIy9SU3uFSHGYutNfcmdAmpG7vHaZqgJgQSL51yQ2k4BvGwJTSNjQJvhdfaH75GrAdW5YjyfIPSSvt9o9zZClaBe2ud3X06HxeM5rdc/acs99no3z86DnP2ZMpFouIiEh4GpIuQEREqqMAFxEJlAJcRCRQCnARkUApwEVEApWd5/fTlBcRkepkZq6Y7wDn8OHDVe2Xy+Xo7++vcTULm9qcDmpzOpxLm7u7u2ddryEUEZFAKcBFRAKlABcRCZQCXEQkUGWdxDSz/wx8kGgWyU+B24Eu4GFgJfA0cJu7j9WpThERmWHOHriZrQb+ENjo7uuBRuAW4HPAfe5+ITAA3FnPQkVE5JXKHULJAm1mlgXagZeAa4Bd8es7gRtrX56IiLyaOYdQ3P2Qmd0D/BI4AzxBNGQy6O7j8WYHgdV1q3IByO7bR9t3vjOv79nY3s6y06fn9T2TpjanQxrbzAc/CCtW1PSQcwa4ma0AbgDWAYPA3wLXlvsGZtYD9AC4O7lcrrpCs9mq962F7L330vDEExQzJRdD1dXSeX23hUFtToe0tbm4eTO5iy6q6THLOYn5TuD/u/sxADP7BnAl0Glm2bgXvgY4NNvO7t4L9MaLxWqvREr0yq1ikdf++MecuvVWhu65Z97eVlerpYPanA71uBKznAD/JXC5mbUTDaFsAp4Cvg/cTDQTZQvwWFWVLUCN+/fT/MwzTHR30/b44zA6SsPgIPlLL026NBGRs8oZA3/SzHYBzwDjwE+IetR/DzxsZp+N132xnoXOp/Ovu46GEycYufpqWn74QwodHYyvWcPo29+edGkiImeVNQ/c3T8DfGbG6v3AZTWvKEG7d7dyySV59py4nhv5X7QePcrYZZdxfNeuuXeew9gYPPjgEoaGMtx00xnWrZuoQcUikmbz/m2EC9XEBNx553nx0oNs5a/Ycfi/MtHVxXPPZdm+fTnDwxkmJjK0thY5cyZDNlv+t+MODjbw/PNNAHzpS0u48MLxOfaApqYs+fzKapoTLLU5HdLY5nvvzfCGN9T2mArw2LFjr5wS/xPewsmhAn9+8A4++47X0NlZ4JJL8vzoR81nt7niilEaG8s7/qpVBW6/fZDLLhvjc59bzpkzc89maWqChpR92YHanA5pbHO5WVEJBXjs6NFXfro/4irex1f51nPXc8UVo9x99xAXXjjOk082c/x4A+3tRa6+erSq93rggZfL2i46a328qvcIldqcDultc22PqQCPHTlS+s/jt7gegK1bT54d8vid39HXvYjIwpCyP2Je3ZEj0Ufxp38yWPLa2rVzj1eLiMw3BXjsyJFGGhuL3HHrcRqYmiHSmJlg9WrNGBGRhUcBHjt+vIGVKwtkTwxQIBpOOZ8+LugYoqkp4eJERGahAI8NDDTQ2VmgYXCQq/ghAA9yG9vf872EKxMRmZ1OYsYGB+MAHxrice7gl7yO32QfAxuv5UzSxYmIzEI98NjZAB8cpIMT/Cb7ACi2tydcmYjI7BTgsaGhDJ2dRRoGo1ko46ujrzcvtrUlWZaIyKtSgMcGBxvo6CiQiQN84nWvAyAzrimEIrIwKcCBw4cbOHVqagilmM2Sv+SS6MV5voGDiEi5dBIT+O3ffi1AFOAvDVLo7OTEtm2Mr1vHyObNCVcnIjI7Bfg04yMTNLz8MoXOTmhr4/QHPpB0SSIir0oBDrS2FhgZaeDD299MG32MXraovuZcRBYpBTgAGf7ojY9xft8YJz5yF6NXXZV0QSIic0p9gI+NwchIhs6Jlxm/4AJObt2adEkiImWZM8DN7GLgkWmrfgP4NNAJfAg4Fq//lLt/u+YV1tnJk9Esk+Vj/RS7OhKuRkSkfOXc1PhZYAOAmTUCh4BHgduB+9z9nrpWWGfDw9FMyuWjx6KTlyIigah0Hvgm4EV3/0U9iknC8HDUA+88c1QBLiJBqXQM/Bbg69OWt5rZ+4GngI+7+8DMHcysB+gBcHdyuVx1hWazVe/76zQ2RgHeceolWrvfQlMd3qNa9WrzQqY2p4PaXKNjlruhmTUD1wN3xau+AGwHivHjvcAdM/dz916gN14s9ld5U7jofnI1vqEccPBgC7CSjsIAJ5ubOVWH96hWvdq8kKnN6aA2V6a7u3vW9ZX0wN8NPOPuRwEmHwHM7G+Ax6uqLGEnT0ajSB0MUdQQiogEpJIx8FuZNnxiZl3TXrsJ4u9fDczAwFSAFzo0C0VEwlFWD9zMlgCbgQ9PW/1nZraBaAjlwIzXgvHCC1mWted5zek+ji9dmnQ5IiJlKyvA3f0UsHLGutvqUtE8e+65LBd3DZJ5EYoKcBEJSOq/Tva557K8aVV0YqG4bFnC1YiIlC/VAT46CsePN/L6ZdHFpIUlSxKuSESkfKkO8JGRaA54e+EUoCEUEQmLApxpAa4euIgERAEOtE0MU2hvh8bGhCsSESmfAhxoyw9r+EREgqMAB9rzJxTgIhIcBTjQPjZEQQEuIoFRgANLf/WcTmCKSHDSHeADIwAsOXGUMd3IWEQCk+p7Yo4dPQGsJv/JP2T4I1cnXY6ISEVS3QMf7RsGoGX1ioQrERGpXKoDfKw/uoCnafV5CVciIlK5VAf46MtnAGi5IF23dhKRxSHdAR6fxGw5ry3hSkREKpfqAB85XSRLnmyqT+WKSKhSHeBnxrK0NYwmXYaISFXm7Hua2cXAI9NW/QbwaeAr8fq1RLdUM3cfqH2J9XMmn6UtM5J0GSIiVZmzB+7uz7r7BnffAPwWcBp4FNgG7HH3i4A98XJQRsaztDWOJV2GiEhVKh1C2QS86O6/AG4AdsbrdwI31rKw+TCSz9LaoAAXkTBVevruFuDr8fNV7v5S/PwIsGq2HcysB+gBcHdyueqm7GWz2ar3fTUjhRbas3lyue6aHrdW6tHmhU5tTge1uUbHLHdDM2sGrgfumvmauxfNrDjbfu7eC/TGi8X+/v5q6iSXy1Htvq/mdD5LS+NozY9bK/Vo80KnNqeD2lyZ7u7ZO5mVDKG8G3jG3Y/Gy0fNrAsgfuyrqrIEjUw005bNJ12GiEhVKgnwW5kaPgH4JrAlfr4FeKxWRc2XMxPNtCrARSRQZQW4mS0BNgPfmLb6bmCzmT0PvDNeDspIoYW2pvGkyxARqUpZY+DufgpYOWPdcaJZKcE6U2iltWki6TJERKqS7isxiy20qgcuIoFKeYC30tasHriIhCndAU4brS2FpMsQEalKagN8fKzAOE20tqgHLiJhSm2Aj5yIpg+2tsx6/ZGIyIKX2gAfHY5OXra2KsBFJEypDfCRk9HQiQJcREKV3gBXD1xEApfaAB+d7IHrdpgiEqjUBvhQfO+gtvZk6xARqVZqA/zRJ1bSxmne+sbBpEsREalKagP8n/9vJ9eym+WdSVciIlKd1Ab4yGgDnQxSbGlJuhQRkaqkNsBHRzO0MkJh1ax3ghMRWfBSG+BjY1GAT7z2tUmXIiJSldQG+Mh4lpbsBMWlS5MuRUSkKqkM8IkJyBeytCzLQiaTdDkiIlUp6448ZtYJ3A+sB4rAHcDvAh8CjsWbfcrdv12PImttbCwK7aZlOoEpIuEqK8CBHcBud7/ZzJqBdqIAv8/d76lbdXUycia6fL75PF3FIyLhmnMIxcw6gLcBXwRw9zF3D/rql/wv+wDIvl4zUEQkXOX0wNcRDZN82cwuBZ4GPha/ttXM3g88BXzc3Qdm7mxmPUAPgLuTy+WqKzSbrXrfmYZfeAbYwPL1b6jZMeuhlm0OhdqcDmpzjY5Z5jZvBT7q7k+a2Q5gG/A/gO1EY+LbgXuJxsZfwd17gd54sdjf319Voblcjmr3nenEz14EIJ9bWrNj1kMt2xwKtTkd1ObKdHd3z7q+nAA/CBx09yfj5V3ANnc/OrmBmf0N8HhVlSVgcgy8ZWkTkE+2GBGRKs05Bu7uR4BfmdnF8apNwM/MrGvaZjcB++pQX12MjkazUFraNIVQRMJV7iyUjwIPxTNQ9gO3A39pZhuIhlAOAB+uS4V1cDbAWxMuRETkHJQV4O6+F9g4Y/VttS9nfoyMRn94NDfrbjwiEq7UXYl5+nSGm7+zFYAW3ZFeRAKWugDv65tqcquGUEQkYKkL8MHBqSarBy4iIUtdgA8MKMBFZHFIXYC//PJUk5uaFOAiEq7UBfhkD3xT+4/o6FCAi0i4UhngDUzw+NqP0JC61ovIYpK6CBsYaGBF00kamhuTLkVE5JyUeyVm4i6/PMvevV1zbziHYjHDxW0HIBtM00VEZhVMiv30pxk2bhzjyivHzvlY79z9eYpNTTWoSkQkOcEEeLEIV1wxxic+MXzOx8r98J8oNC2pQVUiIskJZgy8UKjh/YfzeQ2hiEjwggnwYjFTs1kjmXxeQygiErwgArwYT9euWQ98fBwU4CISuKACvKGhNhfeqAcuIotBEAFeKNT4gBoDF5FFIIgAn+qB1+Z4mfFxis3NtTmYiEhCyuqGmlkncD+wnugWancAzwKPAGuJbqlm7j5QjyIne+A1GwMfG1MPXESCV26fdgew293fBFwK/BzYBuxx94uAPfFyXdSlB64AF5HAzRmJZtYBvA34IoC7j7n7IHADsDPebCdwY72KLBajrndN54FrCEVEAldON3QdcAz4spldCjwNfAxY5e4vxdscAVbNtrOZ9QA9AO5OLperuMhTp6LHpUvbyeXO/T5omXyetuXLaa6ilvmUzWar+rxCpjang9pco2OWuc1bgY+6+5NmtoMZwyXuXjSzWef4uXsv0BsvFvv7+ysu8uTJDNDFmTOn6O8/VfH+r1As0j0xwel8nuEqaplPuVyOaj6vkKnN6aA2V6a7u3vW9eWMKh8EDrr7k/HyLqJAP2pmXQDxY19VlZWhphfy5PPRMTUPXEQCN2cP3N2PmNmvzOxid38W2AT8LP7ZAtwdPz5WryIbnn8R6KJp/wu0/ODZczpWZnQUUICLSPjKnYrxUeAhM2sG9gO3E/Xe3czuBH4BWH1KhLaHvgZcxdKvPsjKr+6oyTELK1bU5DgiIkkpK8DdfS+wcZaXNtW2nNkNb7kdHobTH7yTY793zbkfsKmJ/Pr1534cEZEEBTEZemLNBQAUXv868htXJlyNiMjCEMSl9JNXYtbqy6xERBaDIAK8qNwWESkRRIBP9cCTrUNEZCEJIhJrfkMHEZFFIKgAVw9cRGRKEJFY86+TFRFZBIIIcIiSW7NQRESmBBHg6oGLiJQKIsB1ElNEpFQQAa4euIhIqSACXLNQRERKBRGJ6oGLiJQKIsDVAxcRKRVEJE7d1FjTCEVEJgUS4NGjhlBERKYEEeAaAxcRKVXWDR3M7AAwDEwA4+6+0cz+BPgQcCze7FPu/u16FKkxcBGRUpXckecd7t4/Y9197n5PLQuajXrgIiKlgujTqgcuIlKq3B54EXjCzIrAX7t7b7x+q5m9H3gK+Li7D9SjyKlZKPU4uohImMoN8Kvc/ZCZvQb4rpn9G/AFYDtRuG8H7gXumLmjmfUAPQDuTi6Xq7jIjo5M/LiMXG5pxfuHKpvNVvV5hUxtTge1uTYyxQpvOBmfvDw5fezbzNYCj7v7+jl2Lx4+fLjSGtm7t4nrrjufBx44zubNoxXvH6pcLkd//8zTDoub2pwOanNluru7YfJ7taeZc1TZzJaY2bLJ58C7gH1m1jVts5uAfVVVVgaNgYuIlCpnCGUV8KiZTW7/NXffbWYPmtkGoiGUA8CH61WkZqGIiJSaM8DdfT9w6Szrb6tLRbNQD1xEpFQQkahZKCIipQIJ8OhRAS4iMiWIAJ8aA9e3EYqITAoiwNUDFxEpFUSAT/bAdRJTRGRKEJGoHriISKkgAlw9cBGRUoFEoqYRiojMFESATw2haBaKiMikIAJcl9KLiJQKIsB1Kb2ISKkgIlE9cBGRUkEEuHrgIiKlgohEzQMXESkVSIBHyd3QoFkoIiKTggjwyTFwERGZEkSAawxcRKRUEJGoWSgiIqXKuScmZnYAGAYmgHF332hm5wGPAGuJ7olp7j5QjyLVAxcRKVVJJL7D3Te4+8Z4eRuwx90vAvbEy3WhHriISKlz6dPeAOyMn+8Ebjz3cn499cBFRKaUNYQCFIEnzKwI/LW79wKr3P2l+PUjwKrZdjSzHqAHwN3J5XIVF7lkSZTcK1Z0UsXuwcpms1V9XiFTm9NBba7RMcvc7ip3P2RmrwG+a2b/Nv1Fdy/G4V4iDvveeLHY399fcZEnTrQBKxgaGqC/f6Li/UOVy+Wo5vMKmdqcDmpzZbq7u2ddX9aghLsfih/7gEeBy4CjZtYFED/2VVVZGTQGLiJSas4AN7MlZrZs8jnwLmAf8E1gS7zZFuCxehWpWSgiIqXKicRVwD+a2f8Bfgz8vbvvBu4GNpvZ88A74+W6UA9cRKTUnGPg7r4fuHSW9ceBTfUo6tWoBy4iMiWISCwUdE9MEZGZgghw3RNTRKRUEAGuMXARkVJBBLhmoYiIlAoiEtUDFxEpFUSA65ZqIiKlggpwDaGIiEwJIhIn74mpHriIyJRAAjx61DRCEZEpQQS4TmKKiJQKIsA1Bi4iUiqISFQPXESkVBABPkk9cBGRKUFEor7MSkSkVBABrlkoIiKlgghwjYGLiJQKIsA1C0VEpFS5d6XHzBqBp4BD7v5eM3sAeDswFG/yAXffW/sS1QMXEZlN2QEOfAz4ObB82rpPuPuu2pZUSj1wEZFSZUWima0BrgPur285s9O3EYqIlCq3B/4XwB8Dy2as/+9m9mlgD7DN3Udn7mhmPUAPgLuTy+UqLrKtrRGA88/PpSrEs9lsVZ9XyNTmdFCba3TMuTYws/cCfe7+tJldPe2lu4AjQDPQC3wS+G8z93f33vh1gGJ/f3/FRZ48uQxYxvHjle8bslwuRzWfV8jU5nRQmyvT3d096/pyeuBXAteb2XuAVmC5mX3V3d8Xvz5qZl8G/ktVlZWhWISGBs0BFxGZbs4xcHe/y93XuPta4Bbge+7+PjPrAjCzDHAjsK9eRRYKGv8WEZmpklkoMz1kZucDGWAv8J9qU1KpqAder6OLiISpogB39x8AP4ifX1OHemZVLKoHLiIyUxD9WvXARURKBRGLhUJGPXARkRmCCHD1wEVESgURi5qFIiJSKogAVw9cRKRUELGoWSgiIqWCCXD1wEVEXimIWNQsFBGRUkEEuHrgIiKlzuVS+nmzfn2eTKaQdBkiIgtKEAH++79/mlyunZR9+6SIyK+lgQkRkUApwEVEAqUAFxEJlAJcRCRQCnARkUApwEVEAqUAFxEJlAJcRCRQmWKxOJ/vN69vJiKyiJR8I9R898Az1f6Y2dPnsn+IP2pzOn7U5nT81KDNJTSEIiISKAW4iEigQgrw3qQLSIDanA5qczrUvM3zfRJTRERqJKQeuIiITKMAFxEJVBA3dDCza4EdQCNwv7vfnXBJNWFmXwLeC/S5+/p43XnAI8Ba4ABg7j5gZhmiz+A9wGngA+7+TBJ1V8vMLgC+Aqwiuiag1913LOY2A5hZK/APQAvR/3O73P0zZrYOeBhYCTwN3ObuY2bWQvQ5/RZwHPgP7n4gkeLPgZk1Ak8Bh9z9vYu9vQBmdgAYBiaAcXffWM/f7wXfA49/Cf4n8G7gzcCtZvbmZKuqmQeAa2es2wbscfeLgD3xMkTtvyj+6QG+ME811tI48HF3fzNwOfAH8X/LxdxmgFHgGne/FNgAXGtmlwOfA+5z9wuBAeDOePs7gYF4/X3xdiH6GPDzacuLvb2T3uHuG9x9Y7xct9/vBR/gwGXAC+6+393HiP4FvyHhmmrC3f8BeHnG6huAnfHzncCN09Z/xd2L7v4vQKeZdc1PpbXh7i9N9jDcfZjof+7VLOI2A8T1n4wXm+KfInANsCteP7Pdk5/HLmBT3FsLhpmtAa4D7o+XMyzi9s6hbr/fIQT4auBX05YPxusWq1Xu/lL8/AjRcAMsss/BzNYCbwGeJAVtNrNGM9sL9AHfBV4EBt19PN5ketvOtjt+fYho2CEkfwH8MTB5N/KVLO72TioCT5jZ02bWE6+r2+93CAGeWu5eZBF+f4yZLQX+Dvgjdz8x/bXF2mZ3n3D3DcAaor8q35RwSXVjZpPndZ5OupYEXOXubyUaHvkDM3vb9Bdr/fsdQoAfAi6YtrwmXrdYHZ38Myp+7IvXL4rPwcyaiML7IXf/Rrx6Ubd5OncfBL4PXEH0J/PkRILpbTvb7vj1DqKTe6G4Erg+PqH3MNHQyQ4Wb3vPcvdD8WMf8CjRP9Z1+/0OIcD/FbjIzNaZWTNwC/DNhGuqp28CW+LnW4DHpq1/v5ll4hNgQ9P+LAtCPK75ReDn7v75aS8t2jYDmNn5ZtYZP28DNhON/38fuDnebGa7Jz+Pm4HvxT23ILj7Xe6+xt3XEv3/+j13/48s0vZOMrMlZrZs8jnwLmAfdfz9XvDTCN193My2Av+baBrhl9z9/yVcVk2Y2deBq4GcmR0EPgPcDbiZ3Qn8ArB4828TTTd6gWjK0e3zXvC5uxK4DfhpPB4M8CkWd5sBuoCd8YyqBsDd/XEz+xnwsJl9FvgJ0T9uxI8PmtkLRCe5b0mi6Dr4JIu7vauAR80Momz9mrvvNrN/pU6/37qUXkQkUCEMoYiIyCwU4CIigVKAi4gESgEuIhIoBbiISKAU4CIigVKAi4gE6t8B6eyCtlBM0ogAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(accuracies, \"r-\", acc_train, \"b-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "SimpleHeartDiseasePytorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
